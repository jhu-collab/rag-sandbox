// Responds using the embeddings generated by openai's assistants api

import OpenAI from "openai";
import * as dotenv from "dotenv";
dotenv.config();
import { MessageContentText } from "openai/resources/beta/threads/index.mjs";
import { questions } from "../questions.js";
import { customInstructions } from "../model_custom_instructions.js";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function waitForRunToComplete(threadId, runId) {
  // Currently, openai wants us to retrieve the status repeatedly until the run is 'completed'
  let runStatus = (await openai.beta.threads.runs.retrieve(threadId, runId))
    .status;
  while (runStatus !== "completed") {
    await new Promise((resolve) => setTimeout(resolve, 500));
    runStatus = (await openai.beta.threads.runs.retrieve(threadId, runId))
      .status;
  }
}

async function getModelResponseFromThread(threadId) {
  const messagesInThread = await openai.beta.threads.messages.list(threadId);
  const responseTextObject = messagesInThread.data[0]
    .content[0] as MessageContentText;
  return responseTextObject.text.value;
}

async function outputModelResponse(question, thread, assistantId) {
  await openai.beta.threads.messages.create(thread.id, {
    role: "user",
    content: question,
  });
  const run = await openai.beta.threads.runs.create(thread.id, {
    assistant_id: assistantId,
    instructions: customInstructions,
  });

  await waitForRunToComplete(thread.id, run.id);
  const modelResponse = await getModelResponseFromThread(thread.id);
  console.log(`Question: ${question}`);
  console.log(`Answer: ${modelResponse}\n`);
}

async function main() {
  // Fetch previously created assistant's id
  const assistantId = process.env.OPENAI_ASSISTANT_ID;
  // Create a single thread for all LLM responses
  const thread = await openai.beta.threads.create();

  for (const question of questions) {
    await outputModelResponse(question, thread, assistantId);
  }
}
main();
