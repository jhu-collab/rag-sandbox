# Course Overview & Java Review!

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* outcome 1
* outcome 2


> [Starter code](../../zip/chap00-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap00-solution.zip) for this chapter.



 
# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Title




Highlight something in here!







Exercise Description of exercise.


Solution

Description of solution.





Resources

* Add resources here.



 

# Course Overview & Java Review!

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Define **algorithm**, define **data structure**.
* Distinguish between **data type** and **data structure**.
* Identify parts of a Java class.
* Identify selected object-oriented concepts in action: *Class*, *Object*, *Encapsulation*, *Abstraction*, etc.
* Understand **linear and binary search** well enough to implement them.
* Differentiate between the **best-case** and the **worst-case** scenarios for linear and binary search **runtime**.
* Recognize the speed difference between linear search and binary search.
* Differentiate when binary search can be used vs. when linear search can be used.
* Recognize the overall goals/content of the course and associated logistics.


> [Starter code](../../zip/chap01-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap01-solution.zip) for this chapter.



# Parts of a Java Class



* Identify parts of a Java class




Suppose we open the file `Student.java` to find the following code snippet: 

```java
public class Student {
  private String name;
  private String email;

  public Student(String name, String email) {
    this.name = name;
    this.email = email;
  }

  public String getName() {
    return name;
  }

  public String getEmail() {
    return email;
  }
}
```

Exercise Identify the parts of the `Student` class: fields, constructor, methods.



Solution

Parts of the `Student` class:

* Fields (instance variables)
  ```java
  private String name;
  private String email;
  ```
* Constructor
  ```java
  public Student(String name, String email) {
    this.name = name;
    this.email = email;
  }
  ```
* Methods
  ```java
  public String getName() {
    return name;
  }

  public String getEmail() {
    return email;
  }
  ```




Resources

* For a quick refresher on Java's syntax, refer to this awesome [Java Programming Cheatsheet](https://introcs.cs.princeton.edu/java/11cheatsheet/).



# Object-Oriented Terminology



* Identify selected object-oriented concepts in action




Exercise Use the `Student` class to describe the following terms: Class, Object, Encapsulation, Abstraction, Data type.

```java
public class Student {
  private String name;
  private String email;

  public Student(String name, String email) {
    this.name = name;
    this.email = email;
  }

  public String getName() {
    return name;
  }

  public String getEmail() {
    return email;
  }
}
```



Solution

* `Student` is a **class** (declared with the keyword `class`), and we can use it to make (instantiate) **objects** (using the `new` operator)

  ```java
  Student john = new Student("John Doe", "john@email.com");
  Student jane = new Student("Jane Doe", "jane@email.com");
  ```

* An **object** represents an individual, identifiable item, unit, or entity, either real or abstract, with a well-defined role in the problem domain. 

* A **class** defines the attributes, structure, and operations that are common to a set of objects, including how the objects are created.
    

* A class provides an **encapsulation** by bundling related _data_ (fields) and _behaviors_ (methods) into one cohesive unit. 


* `Student` is an **abstraction**; there is so much information we could capture to represent (model) a student, but we only store what matters to us (to the problem we are trying to solve). In our case, the information of interest are `name` and `email`.

* Classes allow us to define our own data types. A **data type** consists of a _type_ (a collection of values) together with a collection of _operations_ to manipulate the type. 
  * For example, an _integer variable_ is a member of the _integer data type_. Integer arithmetic is an example of _operations_ allowed on the integer data type. Boolean arithmetic is an example of _operations_ not allowed on the integer data type.




Resources

* Oracle's Java Tutorial: [What is a class?](https://docs.oracle.com/javase/tutorial/java/concepts/class.html)
* Oracle's Java Tutorial: [What is an object?](https://docs.oracle.com/javase/tutorial/java/concepts/object.html)
* OOPortal has a [glossary of object-oriented terminology](https://www.ooportal.com/object-oriented-analysis/object-oriented-analysis-glossary.php).
* 101computing.net has an [interactive Domino Activity on Object-Oriented Programming Terminology](https://www.101computing.net/object-oriented-programming-terminology/).
* Quizlet has a [deck of public flashcards on Object-Oriented Glossary](https://quizlet.com/67144448/oop-glossary-flash-cards/).



# Composite Data Types



* Identify selected object-oriented concepts in action




A composite data type is a type that is made up of other types. For example, here is a `Roster` class that holds an array of `Student` objects:

```java
public class Roster {
  private Student[] students;
  private int numStudents;

  public Roster(int size) {
    students = new Student[size];
    numStudents = 0;
  }

  public void add(Student s) {
    // stub
  }

  public void remove(Student s) {
    // stub
  }

  // Assumption: students' emails are unique.
  public Student find(String email) {
    return null; // stub
  }
}
```


What is a method stub?

A [stub](https://en.wikipedia.org/wiki/Method_stub) is a method that doesn't actually do anything other than declaring itself and the parameters it accepts (and returning something _valid_, so the compiler is happy). Stubs are commonly used as placeholders for implementation.




`Roster` is a class, so it too is an _abstraction_ and a _data type_. However, it is different from the `Student` class; the latter is to model a student in our problem domain, the former represents a **collection** of students. Sometimes this distinction is interesting, so to highlight it, we call a class like `Roster` a **container** class or an **aggregate** class.



Object-Oriented programming paradigm is a powerful tool for modeling because it allows you to create **composite** data types (as well as a hierarchy of data types but more on these later).


# Linear Search



* Understand linear search well enough to implement it




Here is a simple strategy to implement the `find` method: go over elements of the `students` array in sequence till the desired element (student with the given email) is found, or _search space_ is exhausted (that is, we checked all the students and we have not found the target). This strategy is called **linear** search.

Exercise Implement `find` according to the linear search strategy.



Solution

```java
// Assumption: students' emails are unique.
public Student find(String email) {
    for (int i = 0; i < numStudents; i++) {
        if (email.equals(students[i].getEmail())) {
            return students[i];
        }
    }
    return null;
}
```

Notice I've used `.equals` to compare two strings (and not `==`). Generally speaking, you should not use `==` for non-primitive types because with `==`, you are comparing memory locations on objects, not their "values."

**Aside:** The code would have worked with `==` too! It is unfortunate that `==` works for Strings and primitive wrappers (see [Equality and Comparison in Java: Pitfalls and Best Practices](https://medium.com/better-programming/equality-and-comparison-in-java-pitfalls-and-best-practices-96b713e7009)). 




Resources

* Daniel Liang has an [interactive demo](http://www.cs.armstrong.edu/liang/animation/web/LinearSearch.html) for linear search.
* [Equality and Comparison in Java: Pitfalls and Best Practices](https://medium.com/better-programming/equality-and-comparison-in-java-pitfalls-and-best-practices-96b713e7009).



# An Algorithm!



- Define algorithm
- Differentiate between the best-case and the worst-case scenarios for linear search runtime




Linear search is an algorithm!



The Oxford dictionary defines *algorithm* as "a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer."



What can we say about the performance of the linear search? Well, let's consider different cases:

* If we are lucky, and the target student which we are looking for is the first element in the `students` array, then we will find it immediately. That's the **best-case** scenario. 

* The **worst-case** is when the target student is not on the roster. Then, the linear search has to go over all the students (their emails) to report the array does not contain what we are looking for. 



The difference between the _best_ and _worst_ case scenarios would be more prominent when the `students` array is large and has many elements. 



Exercise How long will it take to find a student in each case? (Assume each iteration of the for loop takes $0.004$ milliseconds.) 


1. `Roster` is used for a required freshman science class at JHU that typically has a few hundred of students (all sections combined); let's round that up to 1000 students!

| Best-case      | Worst-case |
| -------------- | ---------- |
|    |            |


2. `Roster` is used for a JHU Engineering for Professional MOOC (Massive Open Online Course) that has a few hundred thousand students (all cohorts combined); let's round that up to a million!

| Best-case      | Worst-case |
| -------------- | ---------- |
|    |            |



Solution

The best-case scenario for both cases is the same: it takes one loop iteration to find the student we are looking for. Therefore, it takes $0.004$ milliseconds.

The worst-case scenario will be different: 
* In the first case, we must search $n = 1000$ elements. 

It takes $10^3 \times (4 \times 10^{-3}) = 4$ milliseconds. 

* In the second case, we must search $n = 1000000$ elements. 

It takes $10^6 \times (4 \times 10^{-3}) = 4000$ milliseconds or $4$ seconds.





Resources

Wikipedia's entry on [Linear Search](https://en.wikipedia.org/wiki/Linear_search) is a good resource for a more in-depth analysis.


# Binary Search



* Understand binary search well enough to implement it




If we sort the `students` array (or keep it sorted as we `add`/`remove` students), we can apply a more _efficient_ searching algorithm called Binary Search. 



Binary search makes clever use of the knowledge that the data is sorted to consider a (much smaller) subset of elements in finding the target. 



Here is how it works:

* Compare the target value to the middle element of the array. 
* If they are equal, you found the target!
* If the target is **larger** than the middle element, you know the target is not in the first half of the array (because the array is sorted), so you can ignore the first half and repeat the search process for the second half. 
* If the target is **smaller** than the middle element, ignore the second half and repeat the search process for the first half.

By following the process described above, you are (approximately) halving the search space at every step. Eventually, you either find the target or run out of elements (in which case you would report "target not found!")

 

Binary search is also called "half-interval search" or "binary chop," both are more descriptive names for what it does.




Demo

The following demo shows a schematic representation of the Binary Search process.  



Notice, in the demo, the target is found after three steps (comparisons); Linear search would have needed six comparisons to find the target (assuming we start on the left side of the array with the smallest element). 




Resources

* [Watch David Malan](https://www.youtube.com/watch?v=YzT8zDPihmc) doing the classic phonebook binary search demo!
* Khan Academy has [a lesson on Binary Search](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search) which is very helpful.


# A recursive implementation! 



- Understand binary search well enough to implement it.
- Identify selected object-oriented concepts in action.




Exercise Implement `find` according to the binary search algorithm. Assume the `students` array is sorted.


Solution

Here is a _recursive_ version of `find` that implements Binary Search:

```java
// Binary Search
// Assumption:
// - students' emails are unique
// - students array is sorted (based on emails)
public Student find(String email) {
  // delegate to the helper find method!
  return find(email, 0, numStudents - 1);
}

// helper: recursive binary search
private Student find(String email, int first, int last) {
  // we use first and last indicies to narrow the attention
  // on a portion of the students array. For example:
  // to search only in the first 5 elements: find(email, 0, 6);
  // to search the entire array: find(email, 0, numStudents - 1); 

  // base case
  if (last < first) { // no more element to search
    return null; // if we are here, the target is not in this roster
  }

  int mid = (first + last) / 2;

  if (email.compareTo(students[mid].getEmail()) == 0) {
    // found it!
    return students[mid];
  } else if (email.compareTo(students[mid].getEmail()) > 0) {
    // ignore the first half
    // repeat the search for the second half
    // look in the students array but start from mid point
    return find(email, mid + 1, last);
  } else  {
    // ignore the second half
    // repeat the search for the first half
    // look in the students array from start up to mid point
    return find(email, first, mid - 1);
  }
}
```

Note that `find` makes use of a **private** helper method (also called) `find`:

* The two `find` methods share a name but have a different set of parameters; this is called method overloading, which allows you to reuse a method name.
  
* The _helper_ find is declared as a `private` method; this means any _client_ of `Roster` (other classes/code/program that use `Roster`) cannot directly access it. 



**Information Hiding Principle**: To prevent certain aspects of a class (or software component) from being accessible to its clients. 



Information hiding shifts the code's dependency onto a well-defined interface. 
For example, clients of the `Roster` class would use `find` with one parameter `email` irrespective of whether `find` implements the linear or binary search.

Another common way where information hiding manifests itself is by making your class attributes (fields) inaccessible from the outside while providing getter and setter methods for attributes that shall be readable/updatable by other classes.



Java supports [access modifiers](https://docs.oracle.com/javase/tutorial/java/javaOO/accesscontrol.html) (like `private`) that you can use to observe information hiding principle.




Resources

* A good tutorial on how `compareTo` works: [Java String compareTo()](https://www.javatpoint.com/java-string-compareto)
* Techie Delight has a [detailed article](https://www.techiedelight.com/binary-search/) on iterative and recursive implementation of Binary search in C, Java, and Python.
* OpenGenus article on [Iterative and Recursive Binary Search Algorithm](https://iq.opengenus.org/binary-search-iterative-recursive/#:~:text=The%20major%20difference%20between%20the,the%20iterative%20version%20is%20efficient) is also a good resource.




# Running time



* Recognize the speed difference between linear search and binary search




For binary search, like linear search, the best case is when we get lucky, and the first value we look at is the target value. But let's consider the worst-case: the target element is not to be found. 

Assume the array has $n$ elements. For some random target value (which is not in the array), we look at the value in the middle and eliminate about half of the numbers. We repeat this process again and again until there is one element left. The question is, how many steps it will take us to get to that last element. 


Demo

The following slides assist in building an intuition for the answer:





At each _step_, as can be seen from the demo, we reduce the search space by a factor proportional to the inverse of 2 raised to the power of step. So, the total number of steps Binary search takes to finish its search, $x$, is proportional to the $\log_2 n$.

> **Note on notation**
> 
> In this class I use the notation $\lg x$ for _binary_ logarithm $\log_2 x$.


Resources

* Khan Academy's lesson on [running time of binary search](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/running-time-of-binary-search) is a delight to read.



# Best vs. Worst Case



- Differentiate between the best-case and the worst-case scenarios for binary search runtime
- Recognize the speed difference between linear search and binary search




Let's rework the last exercise for Binary search.

Exercise How long will it take to find a student in each case? (Assume the `students` array is sorted and each step through the search process takes $0.004$ milliseconds.) 


1. `Roster` is used for a required freshman science class at JHU that typically has a few hundred students (all sections combined); let's round that up to 1000 students!

| Best-case      | Worst-case |
| -------------- | ---------- |
|    |            |


2. `Roster` is used for a JHU Engineering for Professional MOOC (Massive Open Online Course) that has a few hundred thousand students (all cohorts combined); let's round that up to a million!

| Best-case      | Worst-case |
| -------------- | ---------- |
|    |            |



Solution

The best-case scenario for both cases is the same: it takes $0.004$ milliseconds to find the student we are looking for. 

The worst-case scenario:

* In the first case, for $N = 1000$ it take approximately $\log_2 (1000) \approx 10$ steps. 

It takes $10 \times (4 \times 10^{-3}) = 0.04$ milliseconds. 

* In the second case, for $N = 10^6$ it take approximately $\log_2 (10^6) \approx 20$ steps. 

It takes $20 \times (4 \times 10^{-3}) = 0.08$ milliseconds.


 

For linear search, when the size of the array increased by a factor of $1000$, the time (for worst-case) increased by the same factor of $1000$ (a linear scale, hence the name _linear_ search). However, for binary search, the same increase in size only doubled the runtime.






Resources

* Wikipedia entry on [Binary Search Performance](https://en.wikipedia.org/wiki/Binary_search_algorithm#Performance) provides an in-depth analysis.



# How to organize data?



* Differentiate when binary search can be used vs. when to use linear search.




Exercise When would you use linear/binary search to implement `Roster.find`? Does implementation of `find` affect the implementation of other operations?


Solution

To have different implementations for `find`, we must _organize_ the data in different ways. For binary search, the `students` array must be kept sorted as we `add` or `remove` students. 

Keeping the `students` array sorted imposes an extra cost: while a naive `add` would append the `students` array with a new student, the "diligent" `add` must add a new `student` in a slot where `students` array remains sorted. That means `add` must make due diligence to find the right spot, shift elements around to make a gap, and only then insert a new student. This process would be, at least, as costly as performing a linear search.



The _choice_ of implementing binary vs. linear search must be made based on the specification of the problem. 



For example: If we happen to have a roster where we add and remove more frequently than search, then we must not impose extra work on those operations. It would be advisable to leave the `students` array unsorted and have `find` implement the linear search.




Resources

* There is an [article on Medium](https://medium.com/@timothy.kaing/searching-algorithms-for-dummies-binary-vs-linear-87711ed2d84c) that reviews Linear vs. Binary search. 
* [Here](https://medium.com/better-programming/three-smart-ways-to-use-binary-search-in-coding-interviews-250ba296cb82) is another Medium article that may be of interest to some of you who want to go beyond the scope of this course.



# A data structure!



- Define "data structure."
- Distinguish between data type and data structure.




We've established that based on the specification of a problem, we choose whether `find` implements the linear or binary search. That choice would require organizing the (`students` array) data in different ways (sorted vs. unsorted). To establish the desired organization of data, we would have to implement `add` and `remove` methods differently. 

There are many cases where efficiently solving a problem requires such due diligence in the organization of data and implementation of operations that manipulate the data, just as in the `Roster` class.  



A Data Structure encapsulates organized mechanisms for efficiently storing, accessing & manipulating data.




Thus `Roster` is indeed a data structure.




In the general sense, any data type (even as simple as an integer) can be viewed as a simple data structure. More commonly, a distinction is made so the term "data structure" means an organization or structuring for a _collection_ of data types. 



Don't fixate too much on what is a type and what is a structure; in future chapters, we will make a clear distinction by using Java Interfaces to define (abstract) data types. Data structures would be fully specified, non-abstract (concrete) classes that _implement_ those interfaces.


Resources

* CrashCourse has a fun [video on Data Structures](https://www.youtube.com/watch?v=DuDz6B4cqVc); a bird's-eye overview of many of the data structures we will be looking at in this course. 



# What we will learn in this course!



* Recognize the overall goals/content of the course and associated logistics




This course is about the study of basic data structures. It concentrates on developing implementations, understanding their performance characteristics, and estimating their potential effectiveness in applications. All implementations are done in Java programming language and according to the Object-Oriented programming paradigm. In the process, you will learn more about basic algorithms as well. Particular emphasis is given to algorithms for sorting, searching (including searching on Graphs). 

### Topics

* Data Structures
    * Lists, Stack, Queue, Sets, Trees, Heaps, Hash Tables, Graphs, $\dots$
* Algorithms
    * Sorting, Balancing Search Trees, Graph Search Algorithms, $\dots$
* Advanced Java
    * Iterators, Generics, Inner/Nested classes, Comparators, $\dots$
* Concepts
    * Unit Testing, Test First Development, Asymptotic Analysis, $\dots$





Please refer to the [syllabus](https://www.notion.so/madooei/Data-Structures-Syllabus-c29a03b6b2a24a158bb9530b45bba831) for more information.




# Inheritance & Polymorphism

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Define **inheritance** and **polymorphism** and identify them in Java source code.
* Use inheritance to design simple **class hierarchies** that allows code to be reused for distinct sub-classes.
* Explain the relationship between inheritance and **type substitution** (the idea of a subtype being usable in a context that expects the super-type).
* Identify **apparent and actual types** of objects and explain the relationship between them.
* Express how **type casting** works for objects, and differentiate between "Upcasting" and "Downcasting." 
* Distinguish between *single*, *multiple*, and *multi-level* inheritance. 
* Explain the relationship between polymorphism and type substitution.
* Distinguish between *dynamic* (runtime) and *static* (compile-time) polymorphism.
* Describe how Java's **method dispatch** process works.
* Develop examples of compile-time and runtime polymorphism.
* Recognize inheritance must be used to model **"is-a" relationship**.
* Contrast "is-a" to "has-a" relationships. 
* Arrange classes in a simple class diagram that exhibits "is-a"/"has-a" relationships.
* Distinguish **method overloading** from **method overriding**.
* Explain how every object in Java has a `toString` method.


> [Starter code](../../zip/chap02-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap02-solution.zip) for this chapter.


# Code duplication is undesirable!



* Recognize inheritance can provide code reuse.



Here is the `Student` class from the previous chapter:


Student class

```java
public class Student {
  private String name;
  private String email;

  public Student(String name, String email) {
    this.name = name;
    this.email = email;
  }

  public String getName() {
    return name;
  }

  public String getEmail() {
    return email;
  }
}
```



Let's define a new class `GradStudent`:

```java
public class GradStudent {
  private String name;
  private String email;
  private String advisor;

  public GradStudent(String name, String email) {
    this.name = name;
    this.email = email;
  }

  public void setAdvisor(String advisor) {
    this.advisor = advisor;
  }

  public String getName() {
    return name;
  }

  public String getEmail() {
    return email;
  }

  public String getAdvisor() {
      return advisor;
  }
}
```

Exercise what are the similarities and differences between the `GradStudent` and `Student` classes?


Solution

* Both have `name` and `email` attributes with corresponding getter methods.
* The `GradStudent` also has an `advisor` attribute with corresponding setter and getter methods.



Notice there is considerable overlap (code duplication) between `Student` and `GradStudent` classes (which is expected since a grad student is a [specific kind of] student). 
Duplicate code is generally considered undesirable.

How can we **reuse** code from `Student` class in `GradStudent`? One of the ways we can achieve code reuse is through the **inheritance** mechanism in object-oriented programming. We will explore inheritance in detail in this chapter. 


Resources

* Wikipedia entry on [duplicate code](https://en.wikipedia.org/wiki/Duplicate_code).
* Refactoring Guru article on [duplicate code](https://refactoring.guru/smells/duplicate-code)



# Code reuse through Inheritance



- Define inheritance.
- Detect inheritance in Java class declaration.



Here is the `GradStudent` class declared as a **sub-class** of `Student`:

```java
public class GradStudent extends Student {
  private String advisor;

  public GradStudent(String name, String email) {
    super(name, email);
  }

  public void setAdvisor(String advisor) {
      this.advisor = advisor;
  }

  public String getAdvisor() {
      return advisor;
  }
}
```

The declaration of `GradStudent` contains what was _specific_ to it; all the shared attributes and operations are now **inherited** from the `Student` class. 



Inheritance is a mechanism that allows classes to be _derived_ from other classes, with the _child_ classes inheriting fields and methods from the _parent_ classes.



Another name for "child class" is sub-class or derived class. The "parent class" is also called the super-class or base class.



In addition to the properties inherited from the super-class, the sub-class can define its fields and methods.



Note that in the example of the `GradStudent` and `Student` classes, the `Student` class is the parent class, while the `GradStudent` class is the child class since it inherits properties from the super-class and defines fields and methods of its own.



Inheritance is a one-way relationship! Therefore, the parent class does not have access to the (additional) attributes/methods declared in a child class.





Resources

* BeginnersBook [Inheritance in Java Programming with examples](https://beginnersbook.com/2013/03/inheritance-in-java/).
* LearnJavaOnline [Inheritance](https://www.learnjavaonline.org/en/Inheritance).



# Inheritance syntax in Java



- Detect inheritance in Java class declaration.
- Make use of the super keyword.



Here is the `GradStudent` class again:

```java
public class GradStudent extends Student {
  private String advisor;

  public GradStudent(String name, String email) {
    super(name, email);
  }

  public void setAdvisor(String advisor) {
      this.advisor = advisor;
  }

  public String getAdvisor() {
      return advisor;
  }
}
```

Make a note of the following:

* To inherit from a class, use the `extends` keyword as in `GradStudent extends Student { ... }`.
  
* The sub-class must not redefine the fields/methods of the super-class (unless for _overriding_, which will be discussed later).
  
* The constructors of a super-class, unlike fields and methods, are not inherited by a sub-class. You must define _non-default_ constructors of a sub-class. The constructors of the sub-class can invoke the constructors of the super-class using the `super` keyword. 
  
    ```java
    public GradStudent(String name, String email) {
      super(name, email);
    }
    ```
    
    The keyword `super` is similar to the `this` keyword, but it points to the parent class. Therefore, it can be used to access fields and methods of the parent class.

     
    
    Call to `super()` must be the first statement in the sub-class constructor.
    
     

    We could update (or overload) the constructor of `GradStudent` to take an initial value for advisor:

    ```java
    public GradStudent(String name, String email, String advisor) {
      super(name, email);
      this.advisor = advisor;
    }
    ```


Resources

* Oracle's Java Tutorial [Providing Constructors for Your Classes](https://docs.oracle.com/javase/tutorial/java/javaOO/constructors.html) - read on default vs. non-default constructors.
* Oracle's Java Tutorial [Using the Keyword super](https://docs.oracle.com/javase/tutorial/java/IandI/super.html).
* Oracle's Java Tutorial [Hiding Fields](https://docs.oracle.com/javase/tutorial/java/IandI/hidevariables.html).



# Inheritance Nuances



* Distinguish accessibility from the inheritance of private properties.



Consider the following code snippet
   
```java
Student john = new Student("John Doe", "john@email.com");
GradStudent jane = new GradStudent("Jane Doe", "jane@email.com");
System.out.println(jane.getName() + " and " + john.getName());
```

Exercise What is printed out?

(a) Jane Doe and John Doe \
(b) Jane Doe and Jane Doe \
(c) null and John Doe \
(d) and John Doe 


Solution

The correct answer is (a). Note that: 
* The _private_ fields/methods are inherited, so `jane` indeed has a `name` field with the value "Jane Doe."
* `john` and `jane` are two separate objects - the use of the `super` keyword to invoke the constructor of `Student` in `GradStudent`'s constructor does not link the objects of these two classes to one another. 



Consider adding the following method to the `GradStudent` class, which raises the compilation error "name has private access in Student."
   
```java
public String toString() {
    return name + " " + email;
}
```

Exercise How can we resolve the compilation error? (There may be more than one correct answer!)

(a) you need to use `super.name` and `super.email` instead \
(b) you need to use `super(name)` and `super(email)` instead \
(c) private fields/methods are not inherited so you cannot access them \
(d) you need to use `getName()` and `getEmail()` instead  \
(e) you must change the visibility of `name` and `email` from `private` to `protected` or `public` in `Student` class


Solution

The correct answers are (d) and (e), with a preference for doing (d) over (e).



**Are private fields/methods inherited?** Yes! A sub-class inherits everything from its super-class. However, the inherited fields (or methods) that are declared private in a super-class are not directly accessible in sub-classes.

 

Declare a property with a visibility modifier of `protected` if you want to access it directly in derived classes. The `protected` modifier indicates that the property is visible to sub-classes but hidden from clients. Please use the `protected` modifier sparingly and only when it is needed. For fields, particularly, you don't need direct access in most cases. Therefore, you are better off using getters/setters to access them. 




Resources

* Baeldung has a great article on [Java Access Modifiers](https://www.baeldung.com/java-access-modifiers).
* Oracles Java Tutorials on [Controlling Access to Members of a Class](https://docs.oracle.com/javase/tutorial/java/javaOO/accesscontrol.html)
* This [discussion on StackOverflow](https://stackoverflow.com/questions/2073159/is-it-better-use-getter-methods-or-to-access-private-fields-directly-when-overri) is interesting. 



# Symmetric Relation?



* Describe why inheritance is not a symmetric relationship. 



Exercise We could make `Student` a sub-class of `GradStudent`. Is there any reason to prefer this to the alternative or the other way around?


Solution

Inheritance is **not symmetric**. The `GradStudent` has properties not shared with `Student` (i.e. the  advisor). 
If we make `Student` a sub-class of the `GradStudent`, it will inherit the `advisor` field along with its setter and getter methods which are not meant to be part of a `Student` object. 

**PS.** I know undergraduate students may have advisors. Still, I've made the simplifying assumption that they don't, at least not in a sense a graduate student has a _thesis advisor_.



# Selective Inheritance?



* Discuss why the further down in the hierarchy a class appears, the more specialized its behavior is.





In Java, it is not possible to select a subset of fields/methods to be inherited. 



Moreover, it is not possible to change the visibility of inherited fields/methods from `public` to `private`.

There is the possibility to _override_ the unwanted operations and change their behavior to, e.g., display an error message or throw an exception to indicate that the operation is not supported. 

For example, following the previous exercise, we can define `Student` to extend `GradStudent` yet not support the operations related to the `advisor` attribute.

```java
public class Student extends GradStudent {
    public Student(String name, String email) {
        super(name, email);
    }

    @Override
    public void setAdvisor(String advisor) {
        throw new UnsupportedOperationException();
    }

    @Override
    public String getAdvisor() {
        throw new UnsupportedOperationException();
    }
}
```



This strategy is not considered a good practice and must be avoided when possible.



# Transitive Relation?



* Describe multi-level inheritance.



Exercise Would it be possible to create a _chain_ of inheritance relationships as in the following example?

(a) Yes \
(b) No






Solution

Yes!  Inheritance is **transitive**. If `GradStudent` is a sub-class of `Student` and `PhdStudent` is a sub-class of `GradStudent`, then `PhdStudent` is also a sub-class of `Student`. 



The chain of inheritance relationships is known as "**multi-level inheritance**" (where a sub-class is inherited from another sub-class). 



Multi-level inheritance is supported in Java.


# Multiple Inheritance?



- Differentiate between multiple vs. single inheritance.
- Differentiate between multiple vs. multi-level inheritance.



Multiple inheritance is where one class has more than one super-class and inherits features from all parent classes.





Some programming languages, like C++ and Ruby, support Multiple Inheritance. Java, however, does not support multiple inheritance. Java does not permit you to extend more than one class. Although, a super-class itself may be a sub-class (of another class higher up in the hierarchy), allowing multi-level inheritance.



The reasons for not permitting multiple inheritance are primarily technical (compiler design) and, at this point, historical artifacts. You can refer to [Oracle's Java Tutorial: Inheritance](https://docs.oracle.com/javase/tutorial/java/IandI/multipleinheritance.html) for _their_ explanation of why Multiple Inheritance is not permitted.



Note that it is perfectly okay for a class to have multiple sub-classes. 


Resources

* TutorialsPoint has a well-written entry on inheritance, with a nice diagram for types of it, [here](https://www.tutorialspoint.com/java/java_inheritance.htm).



# Type Hierarchy



- Express the power of constructing type hierarchies.
- Arrange simple types into a type hierarchy. 




Classes allow us to define our data types. Indeed, `Student` and `GradStudent` are data types. More specifically, `GradStudent` is a **subtype** of `Student`. And by the same token, `Student` is the **base type** of `GradStudent`.

Since inheritance is transitive, it allows us to create a hierarchy of types. This makes object-oriented programming an instrumental paradigm for real-world modeling because the entities in real-world problems often naturally exhibit such a hierarchy.  


Example: Fruits








Example: Animals







Exercise We are building a software solution for a banking system. We have the following classes so far: `Account`, `Checking`, `Saving`, `Debit`, `Credit`. Arrange these classes into the following type hierarchy.






Solution






# Type Substitution 



- Define type substitution. 
- Identify type substitution in code.



Assume `Student` and `GradStudent` were not linked through inheritance (i.e., they were two independent data types). To allow both grad and non-grad students to take the same course, we must update the `Roster` class as follows, essentially duplicating every field and method to accommodate _two types of_ students.

```java
public class Roster {
  private Student[] students;
  private int numStudents;
  private GradStudent[] gradStudents;
  private int numGradStudents;

  public Roster(int size) {
    students = new Student[size];
    gradStudents = new GradStudent[size];
    numStudents = 0;
    numGradStudents = 0;
  }

  public void add(Student s) { ... }
  public void remove(Student s) { ... }
  public Student find(String email) { ... }
  public void addGrad(GradStudent s) { ... }
  public void removeGrad(GradStudent s) { ... }
  public GradStudent findGrad(String email) { ... }
}
```

Suppose `GradStudent` is declared as a subtype of `Student`. In that case, the (original) `Roster` class (as defined in the previous chapter) needs no changes at all. You will be able to add/remove grad students by leveraging **type substitution**.



**Type substitution**

A given type variable may be assigned a value of any subtype, and a method with a parameter of a given type may be invoked with an argument of any subtype of that type.



So, since we can _substitute_ `GradStudent` for `Student`, we can
* have one student array of type `Student` and store both objects of `Student` and `GradStudent` in it;
* have one pair of `add`/`remove` methods that take a parameter of type `Student` and invoke it by passing either an object of `Student` or `GradStudent`;
* have one `find` method that returns an object of type `Student`, and use it to search both grad and non-grad students.

Exercise Can we redefine `Roster` and use `GradStudent` in place of `Student` (in declaration of fields/parameters data types) to achieve the same effect?


Solution

Since inheritance is not symmetric, we would not be able to achieve the same effect if we were to define `Roster` and use `GradStudent` in place of `Student` (in the declaration of fields/parameters data types).




 
# Casting of Types



- Choose available object behaviors based on declared type.
- Express how type casting works for objects.
- Differentiate between Upcasting and Downcasting. 





The advantage of building type hierarchies lies in the power of type substitution. 



Does that mean we should strive to declare objects using more generalized base types? In other words, is one of the following statements preferred to the other?

```java
Student jane = new GradStudent("Jane Doe", "jane@email.com");
GradStudent matt = new GradStudent("Matt Doe", "matt@email.com");
```

The answer is: it depends (as it usually does) on the problem you are trying to solve. 

Before I elaborate more on the answer, let's notice the declaration of `jane` is similar to the case where we have an array of students and add objects of type `GradeStudent` to it:

```java
Student[] students = new Student[10];
students[0] = new GradStudent("Jane Doe", "jane@email.com");
```

This example is a contrived one. However, using the base type `Student` as in the example above to declare an array could be advantageous, as you have seen in designing the `Roster` class. On the other hand, there could be disadvantageous to declare objects to their base type. For example, if we need to access the advisors of graduate students, we would have difficulty with `jane`:

```java
matt.getAdvisor(); // works fine
jane.getAdvisor(); // Compiler error: Cannot resolve method 'getAdvisor' in 'Student'
```

Since `jane` (the object) is _declared_ as a `Student` (and not a `GradStudent`), its behavior (i.e., what it does, what operations/methods can be invoked on it) is limited to those declared in the `Student` class. 

It is noteworthy to point out `jane` has `advisor` (since she is a `GradStudent`); you can get that information "out of `jane`" by *casting* types:

```java
// Assumption: s is instantiated as GradStudent
private static String greetAdvisor(Student s) {
  // Cast object of Student to GradStudent
  GradStudent gs = (GradStudent) s;
  // Call getAdvisor() on the casted object
  return "Hi, " + gs.getAdvisor();
}
```


Aside: Which class the greetAdvisor method is being defined in?

It doesn't matter! The `greetAdvisor` is not defined in `Student` nor `GradStudent`; it is defined in other classes. The point is that `greetAdvisor` uses `Student` (and `GradStudent`).




You can send `jane` or `matt` to the `greetAdvisor` method. However, suppose the argument provided to `greetAdvisor` was not instantiated as `GradStudent`. In that case, the method  will break during its runtime (it will throw `ClassCastException` to indicate that the code has attempted to cast an object to a sub-class of which it is not an instance of.) To guard against this scenario, you can rewrite `greetAdvisor` as follows:

```java
private static String greetAdvisor(Student s) {
  // Use instanceof operator to check 
  // if the object belongs to a specific type
  if (s instanceof GradStudent) {
    GradStudent gs = (GradStudent) s;
    return "Hi, " + gs.getAdvisor();
  } else {
    return "This student has no advisor!";
  }
}
```



Casting from `Student` to `GradStudent` is known as **Downcasting**, the typecasting of a parent object to a child object.



When an object of type `GradStudent` is passed as an argument to `greetAdvisor`, the compiler implicitly casts it to `Student`. The typecasting of a child object to a parent object is known as **Upcasting**.


Resources

* [Java `instanceof` Operator.](https://www.baeldung.com/java-instanceof)
* [Difference between `instanceof` and `getClass()`.](https://www.javapedia.net/Object,-Class-and-Package/2333)



# Apparent vs. Actual Type 



- Label apparent and actual types of objects.
- Explain the relationship between apparent and actual type.



Consider the following statement:

```java
Student jane = new GradStudent("Jane", "jan@e.mail");
```

Notice `jane` is _declared_ as `Student` but instantiated using `GradStudent`. We can break the statement into two statements for clarity:

```java
Student jane;  // Declare jane's type is "Student"
// Then, instantiate it as GradStudent
jane = new GradStudent("Jane", "jan@e.mail");
```

The disparity between the types is allowed (as long as `GradStudent` is a subtype of `Student`). However, how shall we answer the question: what is `jane`'s data type? 

To answer this question, we need to make a distinction between the _apparent_ vs. _actual_ type of an object: 

- When you **declare** a variable (`SomeClass obj`), you are setting its **apparent type** to be the declared type.

- When you **instantiate** an object (`obj = new SomeClass()`) you are setting its **actual type** to be the instantiated type.

Therefore, the apparent type of `jane` is `Student`, while the actual type of `jane` is `GradStudent`.



The type substitution allows the actual type of an object to be a subtype of its apparent type.




Exercise Suppose we declared `Student jane` and `GradStudent john`. Which of the following are not valid Java statements? (there may be more than one correct answer)


(a) `jane = new Student("Jane", "jan@e.mail");` \
(b) `jane = new GradStudent("Jane", "jan@e.mail");` \
(c) `john = new GradStudent("John", "john@e.mail");` \
(d) `john = new Student("John", "john@e.mail");` 



Solution

All statements are valid except for (d) which is invalid because in Java the apparent type of an object cannot be a subtype of its actual type.. 





**Note:** The apparent type determines **what** the object can do, while the actual type determines **how** the object will behave. 



To understand the above statement, we must explore another pillar of object-oriented programming: *polymorphism*. 


# Static (Compile-time) Polymorphism



- Define polymorphism. 
- Associate polymorphism with type substitution.
- Develop an example of static polymorphism.



Polymorphism (the idea of "having many forms") is closely related to type substitution. You have already seen a flavor of it in action: you can pass an argument of type `GradStudent` to a method like `add` that takes a parameter of type `Student` because the compiler honors type substitution and _implicitly_ casts from the subtype (`GradStudent`) to the base type (`Student`). This is called **static** or **compile-time** polymorphism. 

Exercise Make up an example to showcase compile-time polymorphism for the following type hierarchy.






Solution

Suppose we have the following class:

```java
public class Shelter {
  private Animal[] animals;
  private int numAnimals;

  // Constructor not shown to save space

  public add(Animal a) {
    animals[numAnimals++] = a;
  }
}
```

Objects of type `Dog` and `Cat` can be passed to the `add` method and be stored in the `animals` array. 


# Dynamic (Runtime) Polymorphism 



- Explain runtime polymorphism. 
- Identify instances of runtime polymorphism in code.
- Describe how Java's method dispatch process works.
- Distinguish between runtime vs. compile-time polymorphism.



There is another flavor of polymorphism called **dynamic** or **runtime** polymorphism where the Java Virtual Machine (JVM) decides at runtime which method to _dispatch_.

 

**Method Dispatch** 

The process to decide which implementation of a method to use where there are multiple implementations available (as a result of method overriding that will be discussed next).



The dynamic polymorphism relates to the ability of a child class to **override** the behavior of its parent class. 

Example: Consider the following classes

```java
public class CourseAssistant {

  public double getBaseSalary() {
    return 500.00; //dollars
  }

  public double getHourlyPayRate() {
    return 15.00; // dollars
  }
}
```

```java
public class ExperiencedCourseAssistant extends CourseAssistant {

  @Override
  public double getHourlyPayRate() {
    return 1.1 * super.getHourlyPayRate(); 
  }
}
```

The `ExperiencedCourseAssistant` inherits the methods `getBaseSalary()` and `getHourlyPayRate()` from `CourseAssistant` but it **overrides** the `getHourlyPayRate()` method to implement a 10% higher (than the base) pay rate.

Exercise Assume we have the following method

```java
public double calcPayment(CourseAssistant ca, int hoursWorked) {
  return ca.getBaseSalary() + 
    hoursWorked * ca.getHourlyPayRate();
}
```

What does `calcPayment` return in the following cases?

```java
CourseAssistant tom = new CourseAssistant();
CourseAssistant mona = new ExperiencedCourseAssistant();

calcPayment(tom, 10); 
calcPayment(mona, 10); 
```

(a) both call return $650$ \
(b) both call return $665$ \
(c) first call returns $650$ and second one returns $665$ \
(d) first call returns $665$ and second one returns $650$


Solution

* `calcPayment(tom, 10); //-> 650`

When `tom` is passed to `calcPayment`, the return statement evaluates to:

$$
500.00 + 10 \times 15.00
$$ 

* `calcPayment(mona, 10); //-> 665`

When `mona` is passed to `calcPayment`, the return statement evaluates to:

$$
500.00 + 10 \times (1.1 \times 15.00)
$$

So `mona` gets more because `ca.getHourlyPayRate()` resolves to the overridden implementation in `ExperiencedCourseAssistant` class. In other words, JVM looks at the **actual type** of the `ca` object to figure out **how** `getHourlyPayRate()` is implemented.



JVM method dispatch starts at the instantiated/actual type and then looks up the hierarchy until it finds an implementation.




# Exercise



* Develop an example of runtime polymorphism. 



Exercise Make up an example to showcase runtime polymorphism for the following type hierarchy:






Solution

Consider the following implementations for the `makeSound` method:

```java
public class Animal {
  public void makeSound() {
    System.out.println("Grr...");
  }
}
```

```java
public class Cat extends Animal {
  @Override
  public void makeSound() {
    System.out.println("Meow");
  }
}
```

```java
public class Dog extends Animal {
  @Override
  public void makeSound() {
    System.out.println("Woof");
  }
}
```

If we execute the following code, 

```java
Animal[] animals = new Animal[5];
animals[0] = new Animal();
animals[1] = new Cat();
animals[2] = new Dog();
animals[3] = new Dog();
animals[4] = new Cat();

for (Animal a: animals) {
  a.makeSound();
}
``` 

It will print

```text
Grr...
Meow
Woof
Woof
Meow
```

JVM dispatches the implementation of `makeSound` according to _actual type_ of `a` at each iteration, at runtime.

By the way, does this "for loop" look strange to you?

```js
for (Animal a: animals) {
  a.makeSound();
}
```

If yes, please review [Oracle's Java Tutorial: The for Statement](https://docs.oracle.com/javase/tutorial/java/nutsandbolts/for.html). The syntax is referred to as the **enhanced for statement**; it can be used with _Collections_ and _arrays_ to make your loops more compact and easy to read.


# Method Overriding



- Express rules of method overriding in Java.
- Explain why using `@Override` annotation is a good programming practice. 
- Distinguish method overloading from method overriding. 
- Relate polymorphism to method overriding and overloading. 



When you override a method in a sub-class, 

* The overriding method must have the same _signature_ (name and parameters) as the overridden one in the super-class. 

* In addition, it must have the same return type as the overridden one. Although, it could return a subtype of what is returned by the overridden method in the super-class. 

* Finally, it must be at least as visible as the overridden method. It is possible (yet strongly discouraged) to take a `private` or `protected` method and override it to a `public` one in a sub-class. The reverse is not possible; you can't override a `public` method to a `protected` or `private` one.

## `@Override` annotation

When we override a method in a sub-class, we _annotate_ it with `@Override` annotation. 



Annotations in Java are _meta data_: they provide data about a program, but they are not part of the program itself.  



Annotations have no direct effect on the operation of the code they annotate. That said, using `@Override` annotation is considered a good programming practice, and you must follow it (at least in this course) because of the following advantages:

* It improves the readability of the code; it indicates that a method declaration is intended to override a method in a super-class.

* If the annotation is used, the Java compiler will double-check the method signature and report an error if there is any mismatch. This feature prevents unintended mistakes such as misspelling the method name, wrong ordering of parameters' types, etc.

You can learn more about Java annotations at [Oracle's Java Tutorial: Annotations](https://docs.oracle.com/javase/tutorial/java/annotations/). However, we only use a few of these (primarily for unit testing) in this course.


Resources

* JavaPractices [Overridable methods need special care](http://www.javapractices.com/topic/TopicAction.do?Id=89).
* JavaPractices [Use @Override liberally](http://www.javapractices.com/topic/TopicAction.do?Id=223).


# Overriding vs. Overloading



* Distinguish **method overloading** from **method overriding**.




* Overloading is having two methods with the same name but different types or number of parameters (or both). An example of overloading is the helper `find` method in the previous chapter's recursive implementation of binary search.
 
* A different return type as the sole difference between the two methods is not sufficient for overloading. 

* The overloaded method is usually in the same class, but it can also be in derived classes. 

* The overriding method must have the same signature as the overridden one in the super-class. 

* Overriding never happens in the same class. It always happens in sub-classes. 



* Java decides at compile time which version of an overloaded method must be invoked (static polymorphism). 

* Java decides at runtime which version of an overridden method shall be dispatched (dynamic polymorphism).



Consider the following code snippets:

```java
public class Animal {

  // fields and constructors not shown!

  public void makeSound() {
    System.out.println("Grrr.......");
  }

  public void makeSound(int repeat) {
    for (int i = 0; i < repeat; i++) {
      makeSound();
    }
  }
}
```

```java
public class Dog extends Animal {

  @Override
  public void makeSound() {
    System.out.println("Wooof.......");
  }
}
```

```java
public class Cat extends Animal {

  @Override
  public void makeSound() {
    System.out.println("Meow.......");
  }
}
```

```java
Animal[] animals = new Animal[5];
animals[0] = new Animal();
animals[1] = new Dog();
animals[2] = new Cat();
animals[3] = new Cat();
animals[4] = new Dog();

for(Animal a: animals) {
  a.makeSound(2);
}
```

Exercise Identify examples of method overloading vs. method overriding.


Solution

The `makeSound` method is overloaded in the `Animal` class, and it is overridden in the `Dog` and the `Cat` classes.




Exercise Identify examples of static vs. dynamic polymorphism.


Solution

* Static polymorphism: objects of (actual) type `Dog` and `Cat` are stored in an array with a declared type of `Animal`.

* Dynamic polymorphism: `makeSound(2)` inside the for-loop will need to call to overloaded `makeSound` (without argument). During runtime, the dispatch of `makeSound` will be based on the actual type of the Animal object `a`.




Resources

* JavaPractices [Overloading can be tricky](http://www.javapractices.com/topic/TopicAction.do?Id=119).



# Is-a Relationship



- Recognize inheritance should be used to implement is-a relationship. 
- Distinguish between is-a and has-a relationships. 
- Arrange classes in a simple class diagram based on is-a/has-a relationships.



We have explored code reuse via inheritance. It is important to stress here the semantics of inheritance. Aside from reusing the implementation of the `Student` class, we've created a well-defined "is-a" relationship between the base type `Student` and the subtype `GradStudent`.



Inheritance should be used to model "is-a" relationships. Code reuse is an added advantage. 



It's all too easy to misuse inheritance merely for code reuse. Inheritance is intended for creating type hierarchies. Before employing it, you must put it to "is-a" test. For example, replace `GradStudent` and `Student` with your subtype and base type in the following sentence. If the sentence makes sense, then inheritance is the way to go.

> `GradStudent` is a `Student` with added attributes and/or behaviors.

## Has-a relationship 

The is-a relationship (Inheritance) can be contrasted with the has-a relationship (Composition) between types (classes).



**Composition**

Composition is observed when one type is made up of other types. It can be used to achieve code reuse and polymorphism. 



For example, the roster is made up of students: `Roster` has-a `Student`. (The has-a relationship does not imply a multiplicity of one; a `Roster` has zero or more `Student`).





Exercise We are building a software solution for an automobile repair company. So far, we have the following classes: `Car`, `AutomaticCar`, `Transmission`, `AutomaticTransmission`. Can you arrange these classes into the following _class diagram_?






Solution








# The `Object` class



* Explain how every object in Java has a toString method.



Every class in Java is descended from [the `Object` class](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html). This is an implicit inheritance (i.e., it happens without appending `extends Object` to your class declarations). And since the `Object` class contains a `toString()` method, we can call `toString()` on any programmer-defined type and get its string representation.

```java
Student john = new Student("John Doe", "john@e.mail");

System.out.println(john.toString()); // toString() inherited from the Object class
//-> it will print something like Student@1ee0005

System.out.println(john); // automatically calls john.toString()
//-> Student@1ee0005
```

By default, the `toString` method returns a string consisting of the name of the class and the (unsigned) hexadecimal representation of an object's hash code. 



**Aside:** here is the default implementation of `toString()`:

```java
getClass().getName() + '@' + Integer.toHexString(hashCode())
```

The `hashCode()` returns an integer representation of the object. That integer, by default, is derived from the memory address of the object.



The default behavior of `toString` is not that useful. You can _override_ it to return a different string representation of your object. It is common to make a string representation from the (value of) fields. For instance, for students, we can use their name followed by their email:

```java
Student john = new Student("John Doe", "john@email.com");
System.out.println(john); 
//-> John Doe (john@email.com)
```

Here is the code to produce the desired effect:

```java
public class Student {
   // fields and methods not shown for brevity

   @Override
   public String toString() {
      return name + " (" + email + ")";
   }
}
```

IntelliJ can automatically generate `toString` methods for you. Refer to [JetBrain's Help documentation](https://www.jetbrains.com/help/idea/generating-code.html#generate-tostring).



The `Object` class has several other methods, namely `equals` and `hashCode`, which we will learn to override in future chapters.




Resources

* JavaPractices [Implementing toString](http://www.javapractices.com/topic/TopicAction.do?Id=55).


# Abstract Data Type & Java Interface

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Criticize design decisions for a simple type hierarchy in Java made by employing the inheritance mechanism.
* Recognize the usefulness of **abstract classes** and **abstract methods** to provide code reuse as well as a common interface among subtypes.
* Identify the syntax of **Java Abstract Classes** and contrast it with **Java Interfaces** syntax.
* Contrast the utility of abstract classes and interfaces.
* Catalogue class relationships into is-a and has-a classes and separate is-a relationships into "extends" and "implements" types.
* Distinguish between *interfaces* and *implementations*.
* Define what an **Abstract Data Type** (ADT) is.
* Declare an ADT using a Java interface, with complete Javadoc comments.
* Understand the importance and the use of **pre-** and **post-conditions** for documenting a method's operation.
* Write pre- and post-conditions for a given method definition.


> [Starter code](../../zip/chap03-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap03-solution.zip) for this chapter.


# Two Types of Roster!



* Criticize design decisions for a simple type hierarchy in Java made by employing the inheritance mechanism.





 Recall the Roster class from previous chapters!

```java
public class Roster {
  private Student[] students;
  private int numStudents;

  public Roster(int size) {
    students = new Student[size];
    numStudents = 0;
  }

  public void add(Student s) {
    // stub
  }

  public void remove(Student s) {
    // stub
  }

  public Student find(String email) {
    return null; //stub
  }
}
```



Consider the following scenarios:

1. For courses at JHU, we search more frequently than we add/remove students. Therefore, we define `JhuRoster` such that its `find` implements binary search. The `add` and `remove` methods shall keep the underlying (students) data sorted to facilitate the binary search process.
   
2. For MOOCs (Massive Online Open Courses) offered by "JHU Engineering for Professionals," we add/remove students much more frequently than we perform searching. Therefore, we define `MoocRoster` such that its `find` method implements linear search. Furthermore, the `add` and `remove` methods do not bother keeping the underlying (students) data sorted.

We asked students to implement `JhuRoster` and `MoocRoster`, and here are some of the design decisions they have made:

1. `MoocRoster` extends `JhuRoster` (or the other way around)
2. `JhuRoster` and `MoocRoster` are entirely different data types (or "data structures" if you like). 
3. There is a `Roster` class where both `MoocRoster` and `JhuRoster` extend.

We will critique each design in the following section. 
# Critique Design Decisions



* Criticize design decisions for a simple type hierarchy in Java made by employing the inheritance mechanism.




Exercise critique the design decisions below.

1. `MoocRoster` extends `JhuRoster` (or the other way around)

    
    Solution

    * The advantage of having one type of roster extend the other one lies in type substitution benefits. For example, suppose `MoocRoster` extends `JhuRoster`. In that case, we can have an array of type `JhuRoster` and store rosters of `MoocRoster` there.

    * The main issue with this design is a minimal incentive for an inheritance; if `MoocRoster` extends `JhuRoster` (or the other way around), it must override all of its operations. There goes the incentive on code reuse. Besides, the semantic meaning of inheritance is not firmly met here either. It is not clear that `MoocRoster` is a `JhuRoster` (or the other way around).

    

2. `JhuRoster` and `MoocRoster` are entirely different data types (or "data structures" if you like).

    
    Solution

    * The advantage of having two independent types, `JhuRoster` and `MoocRoster`, is that we avoid the need to find a semantic "is-a" relationship. (It is not clear that `MoocRoster` is a `JhuRoster` or the other way around.)
      
    * The disadvantages, on the other hand, are twofold: 

      * It will add complexity (and possibly duplicate code) in clients of these types (i.e., all classes that need to maintain a roster of students).
 
      * The two types of rosters may evolve independently and diverge from having a unified interface. That means more work for clients of these rosters as they must learn to work with two potentially different interfaces.

    

3. There is a `Roster` class where both `MoocRoster` and `JhuRoster` extend it.

    
    Solution

    The final design, like the first one, benefits from the potentials of type substitution. It further offers a more justified inheritance relationship; indeed, `JhuRoster` is a `Roster` and `MoocRoster` is a `Roster`.
      
    The problem with this design occurs when implementing the base type `Roster`. We have two choices: 
      
    * Implement `Roster` as if it was a `JhuRoster` or `MoocRoster`. In this case, one of the `JhuRoster` or `MoocRoster` will be an _alias_ for the `Roster` class. The criticism applied to the first design will be applicable here.

    * Don't implement `Roster`! Instead, put stubs in place of its methods. The issue with this approach is that one can still instantiate the `Roster` even though its methods have no implementation!  
      ```java
      Student john = new Student("John Doe", "john@email.com");
      Roster myRoster = new Roster(30);
      myRoster.add(john); // what will happen??!
      ```

    
# Using an Abstract Class



- Recognize the use of abstract classes and abstract methods to provide code reuse as well as a common interface among subtypes.
- Identify the syntax of Java Abstract Classes.




Now we look at a better design where `Roster` is an **abstract** class:





Here is how the _abstract_ `Roster` is defined: 

```java
public abstract class Roster {
  protected Student[] students;
  protected int numStudents;

  public Roster(int size) {
    students = new Student[size];
    numStudents = 0;
  }

  public abstract void add(Student s);

  public abstract void remove(Student s);

  public abstract Student find(String email);
}
```



An **abstract** class is declared with the `abstract` keyword. It signals that it cannot be instantiated (even that it has a constructor).



An abstract class typically contains one or more abstract methods.



An **abstract method** is a method that is declared with the `abstract` keyword and without an implementation.



Instead of curly brackets (`{ }`), an abstract method's signature ends with a semicolon (`;`).



Any (non-abstract) sub-class that extends an abstract class is responsible for implementing its abstract methods.



Exercise Why is the design above better than those presented earlier?


Solution

* It defines semantically sound "is-a" relationships. 
  
* We can leverage from type substitution. For example, we can have an array of type `Roster` and store rosters of type `JhuRoster` and `MoocRoster` in there.
  
* `Roster` does not implement the core operations. Yet, it provides a unifying signature. The sub-classes must adhere to the "interface" (method signatures of core operations) provided by the `Roster`.
  
* `Roster` cannot be instantiated (which gets around the problem of declaring it as a regular class and leaving implementations as stubs).



**Aside:** An example of a (non-abstract) method that can be implemented in the abstract class `Roster` is 

```java
public int getNumStudents() {
  return numStudents;
}
```

This method can be used by all classes that extend `Roster`.


Resources

* The BeginnersBook has an article, [Abstract Class in Java with example](https://beginnersbook.com/2013/05/java-abstract-class-method/), which you may find helpful to review.



# Extending an Abstract Class



- Identify the syntax of Java Abstract Classes and contrast it with the syntax of Java Interfaces
- Distinguish between interfaces and implementations




Here is the abstract `Roster` class for your reference:


Roster

```java
public abstract class Roster {
  protected Student[] students;
  protected int numStudents;

  public Roster(int size) {
    students = new Student[size];
    numStudents = 0;
  }

  public abstract void add(Student s);

  public abstract void remove(Student s);

  public abstract Student find(String email);
}
```



Here is how `MoocRoster` extends `Roster`:

```java
public class MoocRoster extends Roster {

  public MoocRoster(int size) {
    super(size);
  }

  @Override
  public void add(Student s) {
    // Implementation omitted to save space
  }

  @Override
  public void remove(Student s) {
    // Implementation omitted to save space
  }

  @Override
  public Student find(String email) {
    // Implementation omitted to save space
  }
}
```

Notice:

* There is no `abstract` keyword in the declaration of `MoocRoster`. 
  
* There is no `abstract` keyword in the method declarations within the `MoocRoster` class; instead, there is `@Override` annotation.
  
* `MoocRoster` has access to `students` and `numStudents` in `Roster` since those fields were defined as `protected`.
  
* `MoocRoster`'s constructor invokes the constructor of `Roster` (using `super(size)`), which constructs the `students` array and initializes the `numStudents` attribute. 
# Abstract Data Type



* Define what is an Abstract Data Type (ADT).




Recall the definitions of "data type" and "data structure" from previous chapters:

> A **data type** consists of a _type_ (a collection of values) together with a collection of _operations_ to manipulate the type.

> A **data structure** encapsulates organized mechanisms for efficiently storing, accessing & manipulating data. 

Recall that I have previously mentioned any data type (even as simple as an integer) can be viewed as a simple data structure in the general sense.

Now, I introduce the concept of **Abstract Data Types**, sometimes abbreviated as **ADT**.



ADT is a description (representation) of some (type of) data and the operations that are allowed on that data, while abstracting (ignoring) implementation details.



ADT simply provides a _minimal_ expected _interface_ and set of behaviors without regard to how they will be implemented.



A data structure is an implementation of an ADT.



ADTs are used in the study of data structures, design and analysis of algorithms, programming languages, and software engineering.

* ADTs are a theoretical concept. They are often described in terms of [algebraic specifications](https://en.wikipedia.org/wiki/Algebraic_specification) with axioms that formalize the operations and their expected behaviors.

* Most programming languages do not directly support formally specified ADTs. In programming languages that support object-oriented programming, the abstract class construct comes close to the specification of an ADT. 

For example, the abstract class `Roster` can be seen as an ADT for "roster" which lays out the interface of its core operations `add`, `remove` and `find`. It is essential that these operations are not implemented (otherwise, it would not be an ADT). The responsibility of implementing the core operations falls on the _data structures_ `MoocRoster` and `JhuRoster`.



To specify an ADT, we need to supplement the abstract class construct with proper documentation and a suite of tests. We will explore these in later lessons and chapters. 




Resources

I found Eric Elliott's post on Medium [Abstract Data Types and the Software Crisis](https://medium.com/javascript-scene/abstract-data-types-and-the-software-crisis-671ea7fc72e7) an interesting read. You may want to check it out.



# Abstract Classes



- Recognize the use of abstract classes and abstract methods to provide code reuse as well as a common interface among subtypes.
- Identify the syntax of Java Abstract Classes.




An abstract class is just like a regular (non-abstract) class except that:

1. It contains the keyword `abstract` in its declaration, and 
2. It cannot be instantiated.

So, in the code snippet below, the last statement will cause a compiler error:

```java
Roster cs226 = new JhuRoster(100);   // fine
Roster ep202 = new MoocRoster(1000); // fine
Roster myRoster = new Roster(50);    // compiler error
```

In other words, while an abstract class can be used as the apparent type of an object, it cannot be used as its actual type. 

An abstract class may contain fields, constructors, and fully implemented methods, just like a regular class. 

An abstract class usually contains abstract methods, but it does not have to! On the other hand, a regular class cannot include abstract methods. So if you have an abstract method, it must be inside of an abstract class. 

An abstract class can be used as a parent class or extend another (regular or abstract) class. 



In programming languages that support object-oriented paradigm, there almost always is a construct similar to Java's abstract class (see [Abstract type on Wikipedia](https://en.wikipedia.org/wiki/Abstract_type)).



In C++, for instance, an abstract class is one that has one or more _pure virtual_ functions (the same idea as abstract methods in Java). 
# Java Interfaces



- Identify the syntax of Java Abstract Classes and contrast it with the syntax of Java Interfaces.
- Contrast the utility of abstract classes and interfaces.




Java has a construct called **Interface** which is closely related to abstract classes. 



In a nutshell, an interface is an abstract class that contains nothing but abstract methods.



You can think of an interface as another level of abstraction that is even more abstract than an abstract class!

Consider the following modified abstract `Roster` class where the fields and the constructors are removed. 

```java
public abstract class Roster {

  public abstract void add(Student s);

  public abstract void remove(Student s);

  public abstract Student find(String email);
}
```

The above can be rewritten as follows, which is arguably a more compact and cleaner representation of the Roster ADT.

```java
public interface Roster {

  void add(Student s);

  void remove(Student s);

  Student find(String email);
}
```

Notice the syntax of Java interface:

* We declare it with the keyword `interface` (instead of `class` or `abstract class`)
  
* It does not contain fields or constructors.

* Since all methods are _public_ and _abstract_, there is no need to include those keywords in their declaration.



**Aside:**  In recent versions of Java, an interface can technically contain constants, _default_ methods, _static_ methods, and nested types. 
In this course, however, we use Java's interface as an abstract class that has nothing but abstract methods.



Java interfaces, like abstract classes, can be used to declare objects but not to instantiate them. 


Resources

An article on Medium claims to be the [Easiest explanation of Abstract class and Interface](https://medium.com/@alifabdullah/easiest-explanation-of-abstract-class-and-interface-280741bc2daf)!



# Implementing Interfaces



* Distinguish between interfaces and implementations.



Interfaces can be used in type hierarchy just like abstract classes. However, the syntax is slightly different: a class **implements** an interface whereas it **extends** another (regular or abstract) class. 

For example, here `MoocRoster` *implements* the `Roster` interface:

```java
public class MoocRoster implements Roster {
  private Student[] students;
  private int numStudents;

  public MoocRoster(int size) {
    students = new Student[size];
    numStudents = 0;
  }

  @Override
  public void add(Student s) {
    // Implementation omitted to save space
  }

  @Override
  public void remove(Student s) {
    // Implementation omitted to save space
  }

  @Override
  public Student find(String email) {
    // Implementation omitted to save space
  }
}
```

Notice:

* The use of keyword `implements` (instead of `extends`).
  
* `MoocRoster` declares its fields (since the `Roster` interface does not include fields) and initializes them in its constructor (no `super` keyword). 


# Interface Nuances



- Contrast the utility of abstract classes and interfaces.
- Identify the syntax of Java Abstract Classes and contrast it with the syntax of Java Interfaces.




Recall in Java we cannot have _multiple_ inheritance. In other words, a class (regular or abstract) cannot be a direct child of more than one (regular or abstract) class.

On the other hand, a class may implement more than one interface. 

```java
public class SomeClass implements InterfaceA, InterfaceB {

}
```

The example above shows that the syntax involves a comma-separated list of interfaces that the respective class implements. This construct is the closest thing Java has to Multiple Inheritance. 

A class can extend another (regular or abstract) class and implement one or more interfaces:

```java
public class SomeClass extends OtherClass implements InterfaceA, InterfaceB {

}
```

When a class implements an interface, it must implement all its methods unless it is an abstract class. In this case, it can defer the implementation of those methods to its sub-classes. 

An interface itself can be a subtype of another interface in which case it is said that it _extends_ the other interface:

```java
public interface SomeInterface extends OtherInterface {

}
```
# Class Diagram



* Catalogue class relationships into is-a and has-a classes and further separate is-a relationships into "extends" and "implements" types.



We have created a simple shape drawing application that can compose basic geometric shapes into more complex ones, such as the smiley face below!





Here is a partial class diagram of our software. The links between classes are removed!





Exercise Add links (arrows) between classes to indicate "is-a" and "has-a" relationship. Catalogue is-a relationships into "extends" and "implements" categories. 


Solution





There could be an "is-a" link between `SmileyFace` and `Shape` too.



Exercise Based on the completed diagram, make up an example to showcase polymorphism. (Do this at home!)


Solution

Imagine there is an `area` method in the `Shape` interface. This method must be implemented appropriately in the `Circle` and `Square` classes. 

Further, assume there is the following method:

```java
public static void printArea(Shape s) {
  System.out.println(s.area());
}
```

The following code exhibits polymorphism:

```java
Circle ci = new Circle(2);
Square sq = new Square(3);

printArea(ci);
printArea(sq);
```

* Static polymorphism: During compile-time, `ci` and `sq` are implicitly upcasted to `Shape` when `printArea` is called.

* Dynamic polymorphism: During runtime, JVM dispatches the correct implementation of `s.area()` based on the actual type of `s` (which is, for example, `Circle` when `ci` is passed to `printArea`).


# IndexedList ADT



* Declare an ADT using a Java interface.




We are now ready to declare our first ADT in this course, the `IndexedList`. 

```java
public interface IndexedList {

  void put(int index, int value);

  int get(int index);

  int length();
}
```

The `IndexedList` ADT is an abstraction of _list_, a sequential collection of elements to which you can add and access (get) data using an _index_ (a non-negative integer representing the position of data in the sequence).

Notice the method and parameter names are descriptive. You can, for example, anticipate that the `get` method returns the value stored at the given `index`. First, however, we must adequately document each method, its parameters, return value, effects, etc. We will do this next. 
# Code Contracts Using Documentation Comments



- Declare an ADT using a Java interface, with complete JavaDoc comments.
- Understand the importance and use of pre- and post-conditions for methods documentation.



Here is the `IndexedList` **documented** using Java Documentation Comments (or simply _Javadoc_): 

```java
/**
 * IndexedList ADT.
 */
public interface IndexedList {

  /**
   * Change the value at the given index.
   *
   * @param index representing a position in this list.
   *              Pre: 0 <= index < length
   * @param value to be written at the given index.
   *              Post: this.get(index) == value
   */
  void put(int index, int value);

  /**
   * Retrieve the value stored at the given index.
   *
   * @param index representing a position in this list.
   *              Pre: 0 <= index < length
   * @return value at the given index.
   */
  int get(int index);

  /**
   * Get the declared capacity of this list.
   *
   * @return the length
   *         Inv: length() >= 0
   */
  int length();
}
```

Documentation comments are considered part of the specification of an ADT, as well as a general good programming practice.You are expected to follow this practice in this course.



Documentation comments provide **code contract**: a way to specify the behavior of your ADT as well as _preconditions_, _post-conditions_, _invariants_, etc.



* **Preconditions** are requirements that must be met when entering a method. For example, the `get` method specifies `Pre: 0 <= index < length()`, which means the value for the `index` parameter must be non-negative and smaller than the length of the list. If a client violates this condition, they cannot expect the `get` method to behave as specified. 

* **Postconditions** describe expectations at the time the method exits. For example, the `put` method specifies `Post: this.get(index) == value` which means once the method is successfully executed, you can call `get(index)` and expect it to return the `value`.

* **Invariants** describe the expected state for all objects of a class. For example, the `length` method specifies `Inv: length() >= 0` which means the length is always a non-negative value.


Resources

Here are two articles to help you get up to speed with Java Documentation Comments:

* [TutorialsPoint Guide to Java Documentation](https://www.tutorialspoint.com/java/java_documentation.htm).
* [Introduction to Javadoc by baeldung](https://www.baeldung.com/javadoc).

You can add a Javadoc using automatic comments in IntelliJ. See [this guideline](https://www.jetbrains.com/help/idea/working-with-code-documentation.html#add-new-comment) for more details. 



# Java Generics & Test First Development

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- Implement the `IndexedList` ADT using Java's built-in array.
- Explain why we use **generics** in ADT declarations.
- Identify the syntax of Java Generics.
- Translate the `IndexedList` ADT into a generic interface.
- Implement the generic `IndexedList` ADT with a concrete Java class.
- Create an array of generic types in Java with a fixed size.
- Explain the need for **testing** the implementations of an ADT.
- Define what a **unit test** is.
- Apply the **JUnit framework** to **unit testing** Java code.
- Express best practices of unit testing.
- Write simple unit tests using the JUnit framework.
- Use JUnit's `@BeforeEach` annotation to create setup methods.
- Appreciate the value of Unit Testing & **Test-First Development** in software construction.

> [Starter code](../../zip/chap04-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap04-solution.zip) for this chapter.


# IndexedList ADT Review



* Recall the IndexedList ADT declared using a Java interface.




Here is the `IndexedList` ADT from the last chapter:

```java
/**
 * IndexedList ADT.
 */
public interface IndexedList {

  /**
   * Change the value at the given index.
   *
   * @param index representing a position in this list.
   *              Pre: 0 <= index < length
   * @param value to be written at the given index.
   *              Post: this.get(index) == value
   */
  void put(int index, int value);

  /**
   * Retrieve the value stored at the given index.
   *
   * @param index representing a position in this list.
   *              Pre: 0 <= index < length
   * @return value at the given index.
   */
  int get(int index);

  /**
   * Get the declared capacity of this list.
   *
   * @return the length
   *         Inv: length() >= 0
   */
  int length();
}
```



The `IndexedList` ADT is an abstraction of _list_: a sequential set of elements to which you can add and access (get) data using an _index_. The index is a non-negative integer representing the data position in the sequence.



# An Implementation of IndexedList



* Implement the IndexedList ADT using Java's built-in array.




We are aiming for a very simple implementation of `IndexedList` that uses the built-in Java array as internal storage of data.

Exercise Completed the implementation of `ArrayIndexedList`

```java
/**
 * An implementation of IndexedList that takes a default value
 * to plaster over the entire data structure.
 */
public class ArrayIndexedList implements IndexedList {
  private int[] data;

  /**
   * Construct an ArrayIndexedList with given size and default value.
   *
   * @param size         the capacity of this list.
   * @param defaultValue an integer to plaster over the entire list.
   */
  public ArrayIndexedList(int size, int defaultValue) {
    // TODO Implement me!
  }

  @Override
  public void put(int index, int value) {
    // TODO Implement me!
  }

  @Override
  public int get(int index) {
    return 0; // TODO Implement me!
  }

  @Override
  public int length() {
    return 0; // TODO Implement me!
  }
}
```


Solution

```java
/**
 * An implementation of IndexedList that takes a default value
 * to plaster over the entire data structure.
 */
public class ArrayIndexedList implements IndexedList {
  private int[] data;

  /**
   * Construct an ArrayIndexedList with given size and default value.
   *
   * @param size         the capacity of this list.
   * @param defaultValue an integer to plaster over the entire list.
   */
  public ArrayIndexedList(int size, int defaultValue) {
    data = new int[size];
    for (int i = 0; i < size; i++) {
      data[i] = defaultValue;
    }
  }

  @Override
  public void put(int index, int value) {
    data[index] = value;
  }

  @Override
  public int get(int index) {
    return data[index];
  }

  @Override
  public int length() {
    return data.length;
  }
}
```

Notice 

* The `IndexedList` ADT does not specify how it must be implemented. Therefore, the decisions such as using an array internally, having a fixed length, and initializing the array with a default value are made by `ArrayIndexedList`. 

* There is no need to write JavaDoc for methods declared (and documented) in the `IndexedList` interface as one can always see the documentation there. 




Here is a class diagram that represents our type hierarchy so far:







`ArrayIndexedList` is our first official data structure in this course!	🎉



# Limited to Integer Values?!



* Explain why we use generics in our ADT declaration.




Notice the `IndexedList` ADT can only be used for integer values; the `put` method takes in an integer, and the `get` method returns an integer.

We asked students how we can make `IndexedList` work with types other than the integer. Here are three interesting ideas they came up with:

1. Overload the `put` and `get` methods for different data types.
   
2. Use method overriding to provide multiple concrete implementations of `IndexedList` that work for different data types.
   
3. Change the data type of _value_ from `int` to `Object` as in
   
    ```java
    void put(int index, Object value);
    Object get(int index);
    ```

Exercise Criticize each of the aforementioned ideas.


Solution

1. It's possible to overload `put` with different data types for `value`. However, we cannot overload `get` by varying its return type because changing the return type is not enough for overloading. Furthermore, even if this was possible, we would only account for types already defined. In other words, `IndexedList` will not work for programmer-defined types that it does not have an overloaded version of. Finally, assuming we knew all the data types in advance, this still would be an inelegant solution, to say the least, that would contain hundreds of overloaded methods. 
   
2. To override methods, the method's signature being overridden in a sub-class must be consistent with the overriding one in the parent class. So, we will not be able to override, e.g., `put` in a sub-class and accept values of type `boolean` instead of `int`.
   
3. This is by far the best idea and one which we used to employ (before Java had Generics -- soon to be explored!). Since every type (class) in Java is a subtype (sub-class) of `Object`, we can leverage type substitution and pass values of any type to the `put` method and return it from the `get` method. We will explore the potential issues with this strategy in the next lesson. 

Let's answer a question you may have: how can one pass primitive types (`int`, `float`, `char`, ...) to a method expecting `Object`? The answer is: you cannot do that!! However, every primitive type in Java has a corresponding reference type! For example, `int` has `Integer` **wrapper class** that provides a way to use primitive type `int` as an object.




Resources

You may find these resources helpful to familiarize yourself with **wrapper classes** in Java. In particular, it is useful to understand related concepts of **Autoboxing** and **Unboxing**):

* [BeginnersBook article on "Wrapper class in Java"](https://beginnersbook.com/2017/09/wrapper-class-in-java/)
* [TutorialsPoint article on "Why do we need a wrapper class in Java?"](https://www.tutorialspoint.com/why-do-we-need-a-wrapper-class-in-java)
* [Educative post on "What are wrapper classes in Java?"](https://www.educative.io/edpresso/what-are-wrapper-classes-in-java)
* [Baeldung article on "Wrapper Classes in Java"](https://www.baeldung.com/java-wrapper-classes)


# Lift the Limit with Java Generics!



- Translate the IndexedList ADT into a generic interface.
- Identify the syntax of Java Generics.




Here is a plausible solution to make `IndexedList` ADT work for _all_ types: change the data type of _value_ from `int` to `Object` as in
   
```java
void put(int index, Object value);
Object get(int index);
```

The potential issues with this approach are twofold: 

1. when you retrieve a value from `get`, you must explicitly downcast it to its _actual_ type (unless you only need to use the methods defined in `Object`). (Note: Java automatically upcasts the argument to the `Object` type when using `put`.)
   
2. You can potentially store values of different types in an instance of `IndexedList`. However, this feature is generally undesirable. For example, suppose you have an `IndexedList` called `apples`. In this list, you want to store items of type `Apple` and its subtypes like `McIntoshRed` and `GoldenDelicious`. Indeed, you do not want to store items of type `Orange` in `apples`. 

Java, in 2004 within version J2SE 5.0, introduced **Generics** to address the shortcomings of the aforementioned strategy. According to [Java's Documentation](https://docs.oracle.com/javase/1.5.0/docs/guide/language/index.html):



Generics extend Java's type system to allow "a type or method to operate on objects of various types while providing compile-time type safety."



Generic programming is not specific to Java. Many other programming languages support a similar construct. In C++, for example, generics are supported and called "templates."


# The syntax of Generics in Java



- Translate the IndexedList ADT into a generic interface.
- Identify the syntax of Java Generics.




Generics are simple to start with; here is `IndexedList` ADT with generic base type

```java
public interface IndexedList {

  void put(int index, T value);

  T get(int index);

  int length();
}
```

Note the three changes:

* The interface name is appended with `` (notice the angle brackets).
* The data type of `value` is changed to `T`.
* The return type of `get` is changed to `T`.

The `T` is a placeholder for any type for which a user might want to use `IndexedList`. There is no significance in calling it `T`. You can rename it to anything you want (subject to rules of variable naming in Java). However, in most resources, either `T` or `E` is used to declare a generic type. 

When declaring `IndexedList`, you must specify the intended base type:

```java
IndexedList numbers;
IndexedList apples;
```

Notice the use of angle brackets `<>` in the above statements.

When you declare `numbers` as `IndexedList`, you signal your intention to the compiler. All values stored in `numbers` must be of type `Integer`. In turn, the compiler will 

1. ensure that (providing _type safety_), so attempting to store, e.g., a string in `numbers` will result in a compile-time error.
2. not bother you to (down) cast a value retrieved from the ADT (e.g., through using the `get` method here). 

### A note on documenting generics

Generic variables must be documented similar to method parameters.

```java
/**
 * IndexedList ADT.
 * @param  the base type of the items in the IndexedList.
 */
public interface IndexedList {

}
```

In a way, generics enable types (classes and interfaces) to have parameters.



Generic parameters provide a way for you to re-use the same code with different types. 

 


Resources

You may find the following resources useful:

* [TutorialsPoint Java Generic Classes](https://www.tutorialspoint.com/java_generics/java_generics_classes.htm)
* [TutorialsPoint Java Generic Type Parameters](https://www.tutorialspoint.com/java_generics/java_generics_type_parameters.htm)
* [Oracles Java Tutorial on Generics](https://docs.oracle.com/javase/tutorial/java/generics/index.html)



# Generic Implementation



* Implement the generic IndexedList ADT with a concrete Java class.




We must also update the implementation of the `ArrayIndexedList` since the `IndexedList` ADT uses generics.

1. Update the class declaration, parameters, and return types.
    ```java
    public class ArrayIndexedList implements IndexedList {

      @Override
      public void put(int index, T value) {
        // stub
      }

      @Override
      public T get(int index) {
        return null; // stub
      }

      @Override
      public int length() {
        return 0; // stub
      }
    }
    ```

    Notice the `` follows class name and interface name.

2. Define the fields with a generic type. Here we need a generic array.
    ```java
    private T[] data;
    ```

3. Initialize the fields in a constructor (instantiate when needed). We will leave this for the next lesson!
   
4. Implement the methods.

    Here are implementations for `get` and `length`
    ```java
    @Override
    public T get(int index) {
      return data[index];
    }

    @Override
    public int length() {
      return data.length;
    }
    ```

    Exercise Implement `put`.

    
    Solution

    ```java
    @Override
    public void put(int index, T value) {
      data[index] = value;
    }
    ```

    Notice the implementation of `get`, `length` and `put` are identical to before using generics. This is the beauty of it: using generics in most cases requires very little change to your code. 
    
    
  
# Generic Array



* Create an array of generic types in Java with a fixed size.



The constructor of `ArrayIndexedList` must instantiate and initialize `data`, the generic array. In a perfect world, this would be identical to using any other (primitive or reference) type:

```java
public ArrayIndexedList(int size, T defaultValue) {
  data = new T[size];
  for (int i = 0; i < size; i++) {
    data[i] = defaultValue;
  }
}
```

But the statement `new T[size]` will not work! It results in a compiler error. You must instead do this:

```java
data = (T[]) new Object[size];
```

So, you must create an array of `Object` and then cast it to a generic array. This latter strategy is a _workaround_ to create generic arrays. However, it results in a compilation warning (Unchecked cast: `java.lang.Object[]` to `T[]`), which you may ignore.

The reasons why Java cannot create a generic array (as in `new T[size]`) is irrelevant to the study of data structures. (It has to do with how Java's compiler works). 

**Aside:** Here is another syntax for creating generic arrays:

```java
data = (T[]) Array.newInstance(Object.class, size);
```

There is no preference in using either syntax. The `Array.newInstance` under the hood does the same thing as the syntax presented earlier.  


Resources

So, why do we instantiate a generic array as `(T[]) new Object[size]`? It has to do with how **erasure** and **reification** are implemented in Java's compiler. These are, as mentioned before, irrelevant to the study of data structures. I know, for many of you, your curiosity is not satisfied until you understand these. Here are some resources if you are interested to learn more:

* [Baeldung article on "Java Generics"](https://www.baeldung.com/java-generics)
* [TutorialsPoint definition of "Type Erasure"](https://www.tutorialspoint.com/java_generics/java_generics_type_erasure.htm)
* [GeeksForGeeks article on "Type Erasure in Java"](https://www.geeksforgeeks.org/type-erasure-java/)
* [David Merrick's short post on "Reification vs Erasure in Java Collections"](https://www.david-merrick.com/2015/07/19/reification-vs-erasure-in-java-collections/)

This is the book that helped me to understand it:

> Naftalin, Maurice, and Philip Wadler. [Java generics and collections](https://www.oreilly.com/library/view/java-generics-and/0596527756/). Sebastopol, Calif: O'Reilly, 2006. Print.

Reading the first four chapters is enough to understand how type _erasure_ and _reification_ work in Java.



# Testing Correctness



* Explain the need for testing the implementations of an ADT.




How do we know our implementation of `IndexedList` in `ArrayIndexedList` is "correct"?

> Correctness here means each operation behaves as expected for all inputs, including the edge cases.
 
Well, the least we can do is **test** our implementation.



Through testing, we discover errors in our implementations. Moreover, we build confidence in the correctness of our code when testing does not find errors.



When we declare an ADT, we should provide a suite of tests alongside it. 
As an added advantage, tests further clarify the specification of operations. 

In this course, testing is a first-class citizen! It is as important as the design and implementation of ADTs. 
# JUnit Framework



- Apply the JUnit framework to unit testing Java code.
- Define what a unit test is.




In this course, we use a Java library called [JUnit](https://junit.org/junit5/) to write and run tests. 



JUnit provides _annotations_ to write tests using the familiar structure and syntax of Java classes and methods but treats them as "tests" once annotated as one.



The library is included in the `lib` folder, which we provide as the starter/solution for homework and chapters. 

Let's create a test class for the IndexedList ADT:

```java
import org.junit.jupiter.api.Test;

class IndexedListTest {

  @Test
  void put() {
  }

  @Test
  void get() {
  }

  @Test
  void length() {
  }
}
```

You must store the `IndexedListTest.java` file inside the folder `src/test/java`.

Conventionally, a test class has the word "Test" in its name, either preceding or following the class name the tests are written for.  

Each method that is annotated with `@Test` is a **unit test**. 



A "unit test" is a term used in software engineering to refer to a test written to examine the smallest block of code that can be isolated for testing. In most programming languages, such a "unit" is a function/method.



Notice, to use `@Test` annotation; to use it, you must have the following import at the top of your source code. 

```java
import org.junit.jupiter.api.Test;
```

IntelliJ is equipped with a plugin that facilitates running JUnit tests. For example, you can click on the green play arrow next to each test to run it. 






Resources

* [User Guide on JUnit's website](https://junit.org/junit5/docs/current/user-guide/).
* Baeldung has [several articles](https://www.baeldung.com/junit) on using JUnit, from testing fundamental to advanced topics.



# Unit Tests



* Write simple unit tests using the JUnit framework.



Let's add a unit test for the `length` method. 

```java
@Test
void length() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  assertEquals(5, numbers.length());
}
```

In the test above, I've created an IndexedList with a size of 5 and a default value of 10. Then, I _asserted_ the `length()` method returns the value 5, the size which was provided earlier.

Notes: 

* `IndexedList` cannot be instantiated (because it is an interface) so we must use a concrete implementation of it (e.g. `ArrayIndexedList`). Therefore, the test is actually testing an implementation of `length`.
   
* `assertEquals` is one of the many utility methods provided by JUnit. It ensures its two arguments are equal. If not, it will signal to "fail" the test. To use `assertEquals` you must _import_ it:
  ```java
  import static org.junit.jupiter.api.Assertions.assertEquals;
  ``` 


# Unit Tests: Easier said than done!



* Express best practices of unit testing.



Here is our unit test:

```java
@Test
void length() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  assertEquals(5, numbers.length());
}
```

This test is also (partially) testing the constructor of `ArrayIndexedList` (that it does its job of constructing a list with the given size). In fact, since the job of `length` is to simply return the size, it may be more appropriate to consider this test a unit test for the constructor of the `ArrayIndexedList`!

Note that in unit testing, the goal is to isolate each method (unit) and test it without involving any other method. Of course, this is easier said than done. There is no way, for example, to test any of the methods `put`, `get`, and `length`, before constructing an object of the type `IndexedList`. Nonetheless, we shall strive to write unit tests with minimal dependencies on units other than the one being tested. 



Keep tests focused and targeted so that when they fail, you know exactly what went wrong!


# Use Descriptive Names!



* Express best practices of unit testing.



Here is our unit test again:

```java
@Test
void length() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  assertEquals(5, numbers.length());
}
```

I prefer to rename the test so that it better reflects what is being tested.

```java
@Test
void testLengthAfterConstruction() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  assertEquals(5, numbers.length());
}
```

We can optionally use the `@DisplayName` annotation to provide a more elaborate description. 

```java
@Test
@DisplayName("length() returns the correct size after IndexedList is instantiated.")
void testLengthAfterConstruction() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  assertEquals(5, numbers.length());
}
```

When you run the test, you must see the following in IntelliJ





You are not required to use `@DisplayName` annotations. However, your unit tests must be named descriptively. Never, ever, name them generically as `test1`, `test2`, etc.

# Unit Tests: Exercise 



* Write simple unit tests using the JUnit framework.




Exercise complete the implementation of the following unit tests:

```java
@Test
@DisplayName("get() returns the default value after IndexedList is instantiated.")
void testGetAfterConstruction() {
  // TODO Implement me!
}

@Test
@DisplayName("put() changes the default value after IndexedList is instantiated.")
void testPutChangesValueAfterConstruction() {
  // TODO Implement me!
}

@Test
@DisplayName("put() overwrites the existing value at given index to provided value.")
void testPutUpdatesValueAtGivenIndex() {
  // TODO Implement me!
}
```

When you run your tests, they must all pass. 


Solution

```java
@Test
@DisplayName("get() returns the default value after IndexedList is instantiated.")
void testGetAfterConstruction() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  for (int index = 0; index < numbers.length(); index++) {
    assertEquals(10, numbers.get(index));
  }
}

@Test
@DisplayName("put() changes the default value after IndexedList is instantiated.")
void testPutChangesValueAfterConstruction() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  numbers.put(2, 7);
  assertEquals(7, numbers.get(2));
}

@Test
@DisplayName("put() overwrites the existing value at given index to provided value.")
void testPutUpdatesValueAtGivenIndex() {
  IndexedList numbers = new ArrayIndexedList<>(5, 10);
  numbers.put(1, 8);
  assertEquals(8, numbers.get(1));
  numbers.put(1, -5);
  assertEquals(-5, numbers.get(1));
}
```



Exercise What other unit tests can we add to `IndexedListTest`? You don't need to write the tests, only describe them.


Solution

Here are some ideas:
* Test `get` and `length` have no side effects; that is, calling it will not cause any changes to the data structure. 
* Test `put` has no unintended effects; for example, it does not change the size of the data structure, nor does it change a value at another index.



# `@BeforeEach`



* Use JUnit's BeforeEach annotation to create setup methods.



Notice every unit test we wrote included the following statement

IndexedList numbers = new ArrayIndexedList<>(5, 10);

Here, the size and default value are chosen arbitrarily. Of course, we could have varied them in each unit test, but every unit test certainly requires an IndexedList object. 



JUnit lets you annotate a setup method that will be run before each unit test.



Here is a _refactored_ `IndexedListTest`:

```java
class IndexedListTest {
  private IndexedList numbers;
  private final static int size = 5;
  private final static int defaultValue = 10;

  @BeforeEach
  void setUp() {
    numbers = new ArrayIndexedList<>(size, defaultValue);
  }

  @Test
  @DisplayName("get() returns the default value after IndexedList is instantiated.")
  void testGetAfterConstruction() {
    for (int index = 0; index < numbers.length(); index++) {
      assertEquals(defaultValue, numbers.get(index));
    }
  }

  @Test
  @DisplayName("put() changes the default value after IndexedList is instantiated.")
  void testPutChangesValueAfterConstruction() {
    numbers.put(2, 7);
    assertEquals(7, numbers.get(2));
  }

  @Test
  @DisplayName("put() overwrites the existing value at given index to provided value.")
  void testPutUpdatesValueAtGivenIndex() {
    numbers.put(1, 8);
    assertEquals(8, numbers.get(1));
    numbers.put(1, -5);
    assertEquals(-5, numbers.get(1));
  }

  @Test
  @DisplayName("length() returns the correct size after IndexedList is instantiated.")
  void testLengthAfterConstruction() {
    assertEquals(size, numbers.length());
  }
}
```

Notice I've also used _constant_ fields (`final static`) to declare size and default value. It is considered good practice to store such values in constant (final) variables.
# Unit Testing: Summary



* Appreciate the value of Unit Testing and Test-First Development in software construction.




Unit testing is the practice of testing individual units or components of an application to validate that each of those units is working correctly. In this course, it means testing every operation (method) of all implementations of an ADT. We will do this for every ADT.

In general, the anatomy of each unit test involves three logical steps:

1. **Arrange** everything we need to run the test. For example, instantiate an object with some parameters.
2. **Act** to set the test in motion; invoke the method to be tested (with some input).
3. **Assert** the output (effects) of the method under test checks against the expected outcome.

JUnit is a unit testing framework in Java that provides us with utilities to write, run, and automate unit testing.

### Unit Testing & Software Testing

Unit Testing is just one of the many different software testing strategies. You can read more on Software Testing at [Software Testing Fundamentals Website](http://softwaretestingfundamentals.com/)

It must be noted that software testing is not a substitute for [formal verification](https://en.wikipedia.org/wiki/Formal_verification). Formal verification means **proving** that your code/algorithm/process does what it is supposed to do.

### Test-First Development

In this class, we follow a widespread software development practice known as Test-First Development. 



Test first development is a practice where you write the unit tests before writing the code to test.



Don Wells has a great article, [Code the Unit Test First](http://www.extremeprogramming.org/rules/testfirst.html), where he elaborates on the values of this practice.



Test-First Development is closely related to another, more popular software development practice called [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development) or TDD. 




# Java Exceptions

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Design **robust methods** with Exceptions.
* Write Java code utilizing **Java Exceptions**.
* Extend Java's built-in exceptions to create custom exceptions.
* Build type hierarchies of exceptions. 
* Differentiate when to *throw* exceptions and when to *try/catch* the exceptions.
* Write JUnit tests to check whether exceptions are thrown when expected.
* Explain the difference between **checked** vs. **unchecked** exceptions.


> [Starter code](../../zip/chap05-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap05-solution.zip) for this chapter.


# Robust Methods: Programs with Exceptions 



* Design robust methods with Exceptions.



Consider the specification of the `IndexedList.get` method:

```java
/**
 * Retrieve the value stored at the given index.
 *
 * @param index representing a position in this list.
 *              Pre: 0 <= index < length
 * @return value at the given index.
 */
T get(int index);
```

A client of `get` (any method that calls it) is responsible to ensure the _pre-condition_ is met. The `get` method can check the validity of its input too. (It is not always the case that a method will be able to check and ensure its pre-conditions are met.)



A **robust** method is one that handles bad (invalid or absurd) inputs _reasonably_.



Java's exception handling mechanism supports the construction of robust methods. 

Let's update the specification/declaration of `IndexedList.get` to make it robust.

```java
/**
 * Retrieve the value stored at the given index.
 *
 * @param index representing a position in this list.
 * @return value at the given index.
 * @throws IndexOutOfBoundsException when index < 0 or index >= length.
 */
T get(int index) throws IndexOutOfBoundsException;
```

Notice we have removed the _pre-condition_. Instead, the method throws `IndexOutOfBoundsException` when `index` is out of range.


Resources

Here are some resources on robust coding:

* [Designing Robust Java Programs with Exceptions](https://www.cs.ubc.ca/labs/spl/papers/2000/fse00-exceptions.pdf)
* [Robust Programming](http://nob.cs.ucdavis.edu/bishop/secprog/robust.html)
* [Robust Definition](http://www.linfo.org/robust.html)

[Defensive programming](https://en.wikipedia.org/wiki/Defensive_programming) is a closely related concept. 





# Unit Testing Exceptions



- Write JUnit tests to check whether exceptions are thrown when expected.
- Differentiate when to throw exceptions and when to try/catch the exceptions.



Let's update `IndexedListTest` to include tests to check that the `IndexOutOfBoundsException` is thrown when expected. 

```java
@Test
@DisplayName("get() throws exception if index is below the valid range.")
void testGetWithIndexBelowRangeThrowsException() {
  try {
    numbers.get(-1);
    fail("IndexOutOfBoundsException was not thrown for index < 0");
  } catch (IndexOutOfBoundsException ex) {
    return;
  }
}

@Test
@DisplayName("get() throws exception if index is above the valid range.")
void testGetWithIndexAboveRangeThrowsException() {
  try {
    numbers.get(size + 1);
    fail("IndexOutOfBoundsException was not thrown for index > length");
  } catch (IndexOutOfBoundsException ex) {
    return;
  }
}
```

Notice the approach to test that an exception is thrown when it is expected to be thrown:

```java
try {
  // call a method with an invalid input
  obj.someMethodThatThrowsException(badInput);
  // we expect the execution stops 
  //   and goes to the catch block as a result of an exception
  // if we get passed this point, exception was not thrown
  //   we must deliberately signal the test has failed!
  fail("The expected exception was not thrown!");
} catch (ExpectedException ex) {
  // if we are here, the expected exception is thrown;
  //   there is nothing to do, we can "return" 
  //   to indicate the test has successfully passed
  return;
}
```

The `fail` method is part of JUnit's `Assertions` library

```java
import static org.junit.jupiter.api.Assertions.fail;
```

Run the tests and see them fail! 


Resources

If you need a refresher on Java's Exception, please visit [Exceptions on Oracle's [online] Java Tutorial](https://docs.oracle.com/javase/tutorial/essential/exceptions/index.html). Alternatively, you can read [this article on Java Exceptions](https://www.baeldung.com/java-exceptions).

JUnit 5 includes `assertThrows()` method that provides a more elegant approach to check an exception is thrown. We will not use this alternative approach because it involves the use of [Java's Lambda expression](https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html):

```java
assertThrows(IndexOutOfBoundsException.class, () -> {
  numbers.get(-1);
});
```

[This post](https://howtodoinjava.com/junit5/expected-exception-example/) provides a more detailed example on using `assertThrows()`. 




# Throw an Exception!



* Write Java code utilizing Java Exceptions.



Let's update `ArrayIndexedList.get` to throw the `IndexOutOfBoundsException`:

```java
@Override
public T get(int index) throws IndexOutOfBoundsException {
  if (index >= 0 && index < length()) {
    return data[index];
  } else {
    throw new IndexOutOfBoundsException();
  }
}
```

Make a note of the syntax; in particular, be careful with `throw` v.s. `throws` keywords.

Run `IndexedListTest` and see that all tests pass.
# Exercise



- Design robust methods with Exceptions.
- Write JUnit tests to check whether exceptions are thrown when expected.
- Write Java code utilizing Java Exceptions.



Exercise:

- Update the specification/declaration of `IndexedList.put` to make it robust.
- Write unit tests for `put` to check exception is thrown when it is expected. 
- Update implementation of `ArrayIndexedList.put` to throw exception.



Solution

Update `IndexedList.put`:

```java
/**
 * Change the value at the given index.
 *
 * @param index representing a position in this list.
 * @param value to be written at the given index.
 *              Post: this.get(index) == value
 * @throws IndexOutOfBoundsException when index < 0 or index >= length.
 */
void put(int index, T value) throws IndexOutOfBoundsException;
```

Add the following unit tests to `IndexedListTest`:

```java
@Test
@DisplayName("put() throws exception if index is below the valid range.")
void testPutWithIndexBelowRangeThrowsException() {
  try {
    numbers.put(-1, 10);
    fail("IndexOutOfBoundsException was not thrown for index < 0");
  } catch (IndexOutOfBoundsException ex) {
    return;
  }
}

@Test
@DisplayName("put() throws exception if index is above the valid range.")
void testPutWithIndexAboveRangeThrowsException() {
  try {
    numbers.put(size + 1, 10);
    fail("IndexOutOfBoundsException was not thrown for index > length");
  } catch (IndexOutOfBoundsException ex) {
    return;
  }
}
```

Update `ArrayIndexedList.put`:

```java
@Override
public void put(int index, T value) throws IndexOutOfBoundsException {
  if (index < 0 || index > length()) {
    throw new IndexOutOfBoundsException();
  }
  
  data[index] = value;
}
```

**Aside**: `IndexOutOfBoundsException` is a built-in Java Exception that will be thrown, e.g., when an array is indexed with a value out of the index range. So, the statement `data[index] = value;` will throw `IndexOutOfBoundsException` when `index` is invalid; we didn't need to change the implementation of `ArrayIndexedList.put`. I prefer the updated implementation as it provides more clarity to the behavior of the method.


# Custom Exceptions



* Extend Java's built-in exceptions to create custom exceptions.



Java comes with many built-in exceptions (see a list of most used exceptions and their description [here](https://www.baeldung.com/java-common-exceptions)) such as the `IndexOutOfBoundsException`. These exceptions cover most situations that are bound to happen in programs where there is a need to throw an exception. However, there will be times we need to supplement these built-in exceptions with our own. 



Creating custom exceptions can be as simple as extending an existing built-in exception. 



To learn the process, we make our own version of `IndexOutOfBoundsException`. We call ours `IndexException`.

* Create a Java class `IndexException` with the following content:
  ```java
  public class IndexException extends RuntimeException {

  }
  ```
* Update `IndexedList`, `IndexedListTest` and `ArrayIndexedList`: replace any usage of `IndexOutOfBoundsException` with `IndexException`.

All built-in exceptions have a default constructor and an overloaded constructor that takes a `String` (`message`) as a parameter. It is considered good practice to provide a similar overloaded constructor for your custom exceptions.

```java
/**
 * Exception for invalid index. Data structures using (integer) indices
 * throw IndexException if a given index is out of range.
 */
public class IndexException extends RuntimeException {

  /**
   * Constructs a new IndexException.
   */
  public IndexException() {
  }

  /**
   * Constructs a new IndexException with the specified detail message.
   *
   * @param message the detail message. The detail message is saved for
   *                later retrieval by the getMessage() method.
   */
  public IndexException(String message) {
    super(message);
  }
}
```


Resource

[Create a Custom Exception in Java](https://www.baeldung.com/java-new-custom-exception) by Baeldung is a good read.




# Exception Hierarchies



* Build type hierarchies of exceptions.



> The [Throwable class](https://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html) is the super-class of all errors and exceptions in the Java language. Only objects that are instances of this class (or one of its sub-classes) are thrown by the Java Virtual Machine or can be thrown by the Java throw statement. 

Exceptions are regular classes, and as such, one exception can sub-class another.

```java
class BankingException extends Throwable {...}
class InsufficientFundsException extends BankingException {...}
class NegativeAmountException extends BankingException {...}
```





If an exception is declared to be caught, any of the sub-classes of that exception will also be caught by that same catch statement.

```java
try {
  // Some code that might throw BankingException exception
  // or its sub-classes
} catch (BankingException e) {
  // deal with the exception
}
```

When you chain `catch` blocks, you must deal with more _specific_ exceptions first. 

```java
try {
  // Some code that might throw BankingException exception
  // or its sub-classes
} catch (InsufficientFundsException e) {
  // deal with InsufficientFundsException
} catch (BankingException e) {
  // deal with the exception
}
```
# Two Types of Exceptions



* Distinguish checked vs. unchecked exceptions.



There are two kinds of exceptions in Java: checked exceptions and unchecked exceptions. In this lesson, I'll provide some examples to understand the distinction.

## Checked Exceptions

Consider this code:

```java
public void doSomething() {
	doThisFirst();
	// more code here!
}
```

If `doThisFirst` method *throws* a **checked exception**, the **compiler** will not let you simply call it! You must handle the exception by either of these techniques:

1. Using `try`/`catch` block

```java
public void doSomething()  {
  try { 
    doThisFirst();
  } catch (SomeCheckedException e) {
    // Deal with the exception.
  }
  // more code here!
}
```

2. Allow the `doSomething` method to throw the exception raised by `doThisFirst`  

```java
public void doSomething() throws SomeCheckedException {
  doThisFirst();
  // more code here!
}
```


A method that throws a checked exception must include a `throws` clause in its declaration.



Java verifies checked exceptions at compile-time and forces you to provide a handling mechanism for it.

## Unchecked Exceptions

All Java exceptions are _checked_ unless they subtype the `Error` class or the `RuntimeException` class.






The compiler does not force you to handle unchecked exceptions. For instance, the compiler will let you simply call the `doThisFirst()` if it (may potentially) throws an unchecked exception:  

```java
public void doSomething() {
	doThisFirst();
	// more code here!
}
```



A method that throws an unchecked exception does not need to include a `throws` clause in its declaration.




## Why two types of Exceptions?

The distinction between checked v.s. unchecked exception is specific to Java.  In `C++`, for instance, all exceptions are unchecked, so it is up to the programmer to handle them or not.

Read [Unchecked Exceptions — The Controversy](https://docs.oracle.com/javase/tutorial/essential/exceptions/runtime.html) on Oracle's website for more information. In a nutshell, Java believes it must force programmers to handle exceptions. But, on the other hand, it makes allowance to declare errors that could happen, but rarely do, as unchecked exceptions. So, the programmer is not frustrated to, e.g., have to write try/catch every time they use division in a program even though division by zero causes `ArithmeticException`.


Resources

[Checked and Unchecked Exceptions in Java](https://www.baeldung.com/java-checked-unchecked-exceptions) by Baeldung is a good read.


# Exercise: `LengthException`



* Write Java code utilizing Java Exceptions.



Exercise
The constructor of `ArrayIndexedList` should throw an unchecked `LengthException` when the value provided for `size` is $\leq 0$. Take all the necessary steps to make this happen (don't forget writing tests).


Solution

Please refer to the posted solution.


# Java Iterator & Inner class

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Describe the iterator design pattern.
* Differentiate between the Java **`Iterable`** and **`Iterator`** interfaces.
* Declare, specify and test Iterable ADTs.
* Write a Java class that implements the `Iterable` interface.
* Write a Java class that implements the `Iterator` interface.
* Appreciate that an **inner class** (non-static) Iterator implementation is an instance member and has access to the instance members of the outer class.
* Understand the data encapsulation resulting from private inner classes.

> [Starter code](../../zip/chap06-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap06-solution.zip) for this chapter.


# Enhanced For Loop



* Recall enhanced for loop.




Here is a typical approach to stepping through all the elements of an array (in order):

```java
for (int i = 0; i < myArray.length; i++) {
  System.out.println(myArray[i]);
}
```

There is an alternative form, the so-called _enhanced_ for loop:

```java
// The colon in the syntax can be read as "in"
for (int element : myArray) {
  System.out.println(element);
}
```

The enhanced for loop was introduced in Java 5 as a simpler way to _iterate_ through all the elements of a _Collection_ (not just arrays but other data structures built into Java). It is important to note that not all data structures are "positional"; for example, there is no notion of position in a Set. So, the enhanced loop abstracts the idea of _traversing_ the data structure elements. 

In this chapter, we will provide the means to _iterate_ (use enhanced for loop) over instances of `IndexedList` ADT. At the moment, one can use the "standard" for loop:

```java
for (int i = 0; i < myIndexedList.length(); i++) {
  System.out.println(myIndexedList.get(i));
}
```

By the end of this chapter, we will be able to write:

```java
// Assume myIndexedList stores elements of type Integer
for (int element : myIndexedList) {
  System.out.println(element);
}
```
# Iterator Design Pattern



* Describe the iterator design pattern.



The [Iterator design pattern](https://en.wikipedia.org/wiki/Iterator_pattern) states the elements of a collection (an aggregate object) should be accessed and traversed **without exposing** its representation (underlying implementation).



This goal is achieved by defining an [Iterator object](https://en.wikipedia.org/wiki/Iterator) to traverse a data structure and access its elements. 



Iterator pattern is widely used in [Java Collection Framework](https://docs.oracle.com/javase/tutorial/collections/) (the data structures built into Java). To implement the Iterator pattern, Java API provides two interfaces: [`Iterable`](https://docs.oracle.com/javase/8/docs/api/java/lang/Iterable.html) and [`Iterator`](https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html).


The built-in array is special!

The built-in array is an exception because it is an _iterable_ (thus can be used with the enhanced for loop) but does not implement the `Iterable` interface. The built-in array is a unique construct, a cross between primitive and objects!



Iterator pattern hides the actual implementation of a data structure from the clients of it. We will apply this pattern to the design of our data structures in this course, starting with the `IndexedList` ADT.


Resources

Refactoring Guru has [a great article](https://refactoring.guru/design-patterns/iterator) on the Iterator pattern.




 

# `Iterable` Interface



* Declare an Iterable ADTs.



It is easy to declare that `IndexedList` ADT is _iterable_:

```java
/**
 * IndexedList ADT.
 * @param  the base type of the items in the IndexedList.
 */
public interface IndexedList extends Iterable {

  // No changes were made to the declaration of operations.

}
```

By inheritance, the `IndexedList` ADT now contains the following method declaration (inherited from the `Iterable` interface):

```java
/**
  * Returns an iterator over elements of this collection.
  *
  * @return an Iterator.
  */
Iterator iterator();
```

Any class that implements `IndexedList` (such as `ArrayIndexedList`) must provide an implementation for the `iterator` method.
# Test an Iterable ADTs



* Test an Iterable ADTs.



Let's add a simple test to the `IndexedListTest` to test the (inherited) `IndexedList.iterator` method:

```java
@Test
@DisplayName("test iterator after IndexedList is instantiated.")
void testEnhancedLoopAfterConstruction() {
  int counter = 0;
  for(int element: numbers) {
    assertEquals(defaultValue, element);
    counter++;
  }
  assertEquals(size, counter);
}
```
# Iterator Interface



* Write a Java class that implements the `Iterable` interface.



We must implement the inherited `iterator` method in the `ArrayIndexedList`. 

```java
public class ArrayIndexedList implements IndexedList {

  // No changes were made to other operations.

  @Override
  public Iterator iterator() {
    return Arrays.stream(data).iterator();
  }
}
```

[Java's `Arrays` class](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html) provides many utilities to work with arrays, including one to build an iterator for an array. We will shortly build our iterator from the ground up!

Note, the only addition to the `ArrayIndexedList` class is the method above. There are no other changes made (in particular, no changes made to the declaration of `ArrayIndexedList`). 

You need to import the `Iterator` interface and the `Arrays` class in the `ArrayIndexedList.java` file.

```java
import java.util.Arrays;
import java.util.Iterator;
```
# Implement the Iterator Pattern 



* Write a Java class that implements the `Iterator` interface.



## Part I

We are going to implement our _iterator_ for `ArrayIndexedList` (instead of using the `Arrays.stream(data).iterator`). 

* Add a new class `ArrayIndexedListIterator`:

  ```java
  public class ArrayIndexedListIterator {

  } 
  ```

* `ArrayIndexedListIterator` must implement the `Iterator` interface:
  
  ```java
  public class ArrayIndexedListIterator implements Iterator {

  } 
  ```

* The following operations from the `Iterator` interface are inherited (and need to be implemented): 
  
  ```java
  public class ArrayIndexedListIterator implements Iterator {

    @Override
    public boolean hasNext() {
      return false;
    }

    @Override
    public T next() {
      return null;
    }
  } 
  ```

Let us understand these methods first! 

When we use the enhanced for loop, we are using [syntactic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar):

```java
Iterator it = myCollection.iterator();
while (it.hasNext()) {
  MyType element = it.next();
  // do something with element
}

// the loop above is the same as the one below
for(MyType element : myCollection) {
  // do something with element
}
```

From the `while` loop, it is clear the `next` method returns the _next element_ in the collection. And, `hasNext` returns true if there is an element we have not iterated over.
# Implement the Iterator Pattern 



* Write a Java class that implements the `Iterator` interface.



## Part II

Here is an implementation for `hasNext` and `next` methods:

```java
public class ArrayIndexedListIterator implements Iterator {
  private T[] data;
  private int cursor;

  public ArrayIndexedListIterator(T[] data) {
    this.data = data;
    this.cursor = 0;
  }

  @Override
  public boolean hasNext() {
    return cursor < data.length;
  }

  @Override
  public T next() {
    if (!hasNext()) {
      throw new NoSuchElementException();
    }
    return data[cursor++];
  }
} 
```

Notes:

* We need to point to the data (elements) stored in the data structure we want to iterate over (the `ArrayIndexedList` here). Thus, we keep a reference (`data`) in the implementation provided here, which the constructor initializes.
* We use the `cursor` to keep track of the current element. The `cursor` is the index position of the current element. A "cursor" is not always as simple as an "index." Depending on the data structure, the "cursor" may require complex implementation.
* `hasNext` is true if the `cursor` has not reached the last element.
* `next` returns the current element pointed by the `cursor` and advances the `cursor`.
* `next` should throw `NoSuchElementException` when a client calls it after the iteration is over.


Now, update the implementation of `ArrayIndexedList.iterator` to use `ArrayIndexedListIterator`:

```java
public class ArrayIndexedList implements IndexedList {

  // No changes were made to other operations.

  @Override
  public Iterator iterator() {
    return new ArrayIndexedListIterator<>(this.data);
  } 
}
```

Run the tests and make sure they all pass!
# Implement the Iterator Pattern 



* Appreciate that an inner class (non-static) Iterator implementation is an instance member and has access to the instance members of the outer class.



## Part III

It is a common practice to place the iterator class for a data structure inside the data structure class itself.



In java, it is possible to define a class within another class. Such classes are known as nested (or inner) classes.



```java
public class ArrayIndexedList implements IndexedList {
  private final T[] data;

  // No changes were made to other operations. 

  @Override
  public Iterator iterator() {
    return new ArrayIndexedListIterator();
  }

  private class ArrayIndexedListIterator implements Iterator {
    private int cursor = 0;

    @Override
    public boolean hasNext() {
      return cursor < data.length;
    }

    @Override
    public T next() {
      if (!hasNext()) {
        throw new NoSuchElementException();
      }
      return data[cursor++];
    }
  }
}
```

There are advantages to using an _inner_ class:
- **It is a way of logically grouping classes that are only used in one place.** Suppose a class is needed by one other class only. Then, it is logical to embed it in that class and keep the two together. 
- **It increases encapsulation.** The `ArrayIndexedListIterator` needs access to `data` member of `ArrayIndexedList`. By nesting `ArrayIndexedListIterator` within class `ArrayIndexedList`, it gets access to all `ArrayIndexedList` members (including the private ones). In addition, `ArrayIndexedListIterator` itself can be hidden from the outside world (simply declare it as a `private` member).
- **It can lead to more readable and maintainable code.** Nesting small classes within top-level classes places the code closer to where it is used.



We always use an inner class to implement the `Iterator` interface for a data structure in this course.




  Resources

* Oracle's Java Tutorial, [Nested Classes](https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html).
* Baeldung's [Nested Classes in Java](https://www.baeldung.com/java-nested-classes).
* TutorialsPoint [Java - Inner classes](https://www.tutorialspoint.com/java/java_innerclasses.htm).


# Implement the Iterator Pattern 



* Declare, specify, and implement Iterable ADTs.



## Exercise

In the starter code, open the `IteratorExercise` class and uncomment the commented code. The uncommented code must be highlighted in red, indicating an error as to `myRoster` is not iterable. Fix the error.


Solution

Please refer to the posted solution code. 


# Linked List

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Define what a **Linked List** is.
* Enumerate the advantages & disadvantages of a linked list-based vs. an array-based implementation.
* **Trace** the basic operations of a (singly) linked-list implementation.
* Understand the basic operations of a (singly) linked list well enough to **implement** them.
* Implement an iterator for a (singly) linked list.
* Appreciate that members of a **static nested class** (Node for Linked List) can be accessed by the outer class, but not the other way around.
* Implement `IndexedList` operations with a (singly) linked list implementation (`LinkedIndexedList`).

> [Starter code](../../zip/chap07-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap07-solution.zip) for this chapter.


# Array: A Static Data Structure



* Define what an array is.



The array is the most fundamental data structure built into many programming languages. Moreover, it is employed to implement many other data structures. Indeed, our first choice was to use an array to implement IndexedList ADT.

Arrays have their limitations; they are static structures and cannot be easily extended or reduced to fit the data set. Furthermore, it is _expensive_ to insert/delete from the front or middle of an array. 

In this chapter, we explore another fundamental data structure called Linked List that does not have the limitations of arrays.
# Linked List: A Dynamic Data Structure



* Define what a linked list is.



A linked list is a linear data structure where each element is a separate object made of at least two items: the data and a reference to the next element. Conventionally, each element of a Linked List is called a **node**.

```java
public class Node {
  private E data;
  private Node next;
  
  // we can have constructors, setters, getters, etc.
}
```
Here is a minimal implementation for a Linked List:

```java
public class LinkedList {
  private Node head;
  
  // we can have constructors, methods to add/remove nodes, etc.
}
```





* The entry point into a linked list is called the `head` of the list. 
* The `head` is not a separate node but a reference to the first node. 
* The last node has a reference to `null`.
* If the list is empty, then the `head` is a `null` reference.


Aside: Java Reference vs. C/C++ Pointer

If you are here from Intermediate Programming, you may be wondering about C++ pointers. Java's reference variable plays a similar role to the C++ pointer. A Java variable of object type stores a reference to an object, which is just a pointer giving the address of that object in memory. When you use an object variable in a Java program, it automatically follows the pointer stored in that variable to find the actual object in memory. All this happens automatically, behind the scenes.






Resource

* Wikipedia's [entry on Linked List](https://en.wikipedia.org/wiki/Linked_list) is great!
* [Java References vs C++ Pointers and References](https://programming.guide/java-references-vs-c-pointers-and-references.html).
* [Why pointer concept not use in java?](http://net-informations.com/java/cjava/pointers.htm)
* [Differences between pointers and references in C++](https://www.educative.io/edpresso/differences-between-pointers-and-references-in-cpp).


# Static Nested Class



* Appreciate that members of a static nested class (Node for Linked List) can be accessed by the outer class, but not the other way around.



It is a common practice to nest the `Node` class inside the `LinkedList` class:

```java
public class LinkedList {
  private Node head;
  
  private static class Node {
    E data;
    Node next;
  }
  
  // we can have constructors, methods to add/remove nodes, etc.
}
```

Note the nested class is declared as `static`. 



A static nested class does not have access to the instance members of the outer class. 



On the other hand, the outer class has access to the instance members of the static nested class objects. The `Node` does not need access to the members of `LinkedList`, but `LinkedList` can access `data` and `next` on objects of `Node`, eliminating the need for getters/setters.


Inner vs Static Nested Class

| Inner Class | Static Nested Class |
| ----------- | ------------------- |
| It is an instance member of the outer class (not static!) and has access to all the members of the outer class (private, static, etc.) | It is a static member of the outer class and does not have access to the instance members of the outer class. Instead, the outer class has access to the nested class members. |

[Here](https://repl.it/@amadooei/NestedClasses#Outer.java) is a toy example that showcases the use of inner vs. static nested classes.




Resources

* Oracle's Java Tutorial, [Nested Classes](https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html)
* Baeldung's article [Nested Classes in Java](https://www.baeldung.com/java-nested-classes)
* Beginner's Book entry on [Java's Inner Class](https://beginnersbook.com/2013/05/inner-class/)


# Array vs. Linked List



* Enumerate the advantages & disadvantages of a dynamically linked list vs. an array-based implementation.



The array is a static data structure. Its length is set when the array is created, and it is fixed. A linked list is a dynamic data structure. The size (number of nodes in a list) is not set and can grow/shrink on demand. 

Insertion/deletion to the front or at the middle of an array is expensive; it requires shifting other elements to make/fill a gap. However, insertion/deletion in a Linked List can be done as cheaply as updating a few reference variables (we will see this soon).

One disadvantage of a linked list is that it does not allow direct access to the individual elements. For example, suppose you want to access a particular item. In that case, you have to start at the `head` and follow the references until you get to that item.

Another disadvantage of a linked list is that it uses more memory than an array (to store a reference to the next node for each element).


Resource

* Interviewbit's article on [Arrays vs Linked Lists](https://www.interviewbit.com/tutorial/arrays-vs-linked-lists/).
* Towards Data Science's article on [Arrays vs Linked Lists](https://towardsdatascience.com/linked-lists-vs-arrays-78746f983267).
* A detailed article (with nice visualization) on [Array vs Linked List Data Structures](https://levelup.gitconnected.com/array-vs-linked-list-data-structure-c5c0ff405f16) from git-connected.
* The [difference between Linked List and Arrays](https://www.faceprep.in/data-structures/linked-list-vs-array/) has a nice table for comparison. 


# Build a Linked List



* Trace the basic operations of a (singly) linked-list implementation.



Consider the following implementation of `Node` (with package-private visibility modifier):

```java
class Node {
  int data;
  Node next;
  
  Node(int data) { 
    this.data = data; 
    this.next = null; // we can eliminate this line; it happens by default 
  }
}
```

Exercise Draw a schematic representation of a linked list pointed to by the `head` variable after the following code snippet has been executed.

```java
Node head = new Node(7);
head.next = new Node(5);
head.next.next = new Node(10);

Node node = new Node(9);
node.next = head;
head = node;
```


Solution








# Follow the References!



* Trace the basic operations of a (singly) linked-list implementation.



Consider the following linked list:





Exercise In each case, draw a schematic representation of the linked list after the statement is executed. For each statement, start with the linked list displayed above.

```java
head = head.next;
```


Solution





We effectively removed the first element!



```java
head.next = head.next.next;
```


Solution





We effectively removed the second element!



```java
head.next.next.next.next = head;
```


Solution
  




We effectively created a _circular_ linked list!




# Prepend Operation



- Trace the basic operations of a (singly) linked list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them.



Suppose we have the following linked list and want to add a new node to its front. 





Exercise Complete the implementation of the `addFirst` method that creates a node and adds it to the front of the list.

```java
public void addFirst (T data) {
  // TODO Implement Me!
}
```

Hint: Use the following visualization as a guidance:






Solution

```java
public void addFirst(T t) {
  Node node = new Node<>(t);
  // node.next = null; // no need: done by default!
  node.next = head;
  head = node;
}
```




# Traversing a Linked List



- Trace the basic operations of a (singly) linked-list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them. 



Suppose we have a linked list with $n$ elements (nodes), and we want to go over every element and print out the data stored in it. 





Exercise Complete the implementation of `traverse` that iterates over a linked list and prints the data stored at every node.

```java
public void traverse() {
  // TODO Implement me!
}
```

Hint: the front of the linked list is marked by the `head` pointer. If you were to follow the `next` references, how would you know when you reached the last node?


Solution

```java
public void traverse() {
  Node current = head;
  while (current != null) {
    System.out.println(current.data);
    current = current.next;
  }
}
```

If you count the number of nodes in the linked list, you can also write this with a counter-controlled loop.

```java
public void traverse() {
  for (Node current = head; current != null; current = current.next) {
    System.out.println(current.data);
  }
}
```

You can also implement this using a counter controlled loop, checking the count against the `numElements`.




# Get Operation



- Trace the basic operations of a (singly) linked list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them.



Suppose we have a linked list with $n$ elements (nodes), and we want to get the data stored in the $k^{th}$ element (at index $k-1$). 





Exercise Complete the implementation of the `get` method which returns data stored at a given index.

```java
public T get(int index) {
  return null; // TODO Implement me!
}
```

Hint: you cannot directly jump to $K^{th}$ node. You need to start at the `head` and follow the `next` references to _get there_!


Solution

```java
public T get(int index) {
  return find(index).data;
}

// PRE: 0 <= index < numElements 
private Node find(int index) {
  Node target = head;
  for(int counter = 0; counter < index; ounter++) {
    target = target.next;
  }
  return target;
}
```

**Caution**: the implementation above fails to account for an edge case!




# Append Operation



- Trace the basic operations of a (singly) linked-list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them.



Suppose we have the following linked list and we want to append (add to the end) a new node.





Exercise Complete the implementation of the `addLast` method that creates a node and add it to the back of the list.

```java
public void addLast(T t) {
  // TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void addLast(T t) {
  Node tail = head;
  while (tail.next != null) {
    tail = tail.next;
  }
  
  tail.next = new Node(t);
}
```

If you count the number of nodes in the linked list, you can also write this with a counter-controlled loop. In that case, the `find` helper method from when we implemented `get` can be used here to go to the last element. 

**Caution**: the implementation above fails to account for an edge case!




# Insert Operation



- Trace the basic operations of a (singly) linked list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them.



Suppose we have a linked list with $n$ elements (nodes), and we want to insert a new node at index $k$. That is, we insert a new node between elements at indices $k-1$ and $k$. After the insertion, we will have $n + 1$ elements:

* The node at index $k - 1$ before the insertion will remain at that index after the insertion.
* The node at index $k$ before the insertion will be at index $k+1$ after the insertion. 
* The newly added node will be at index $k$.

For example, we have a linked list with three nodes at indices `0`, `1`, and `2`.





We will insert a new node at index `2`:





Exercise Complete the implementation of `insert` which adds a new node at the given index.

```java
public void insert(int index, T t) {
	// TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void insert(int index, T t) {
  Node target = head;
  for (int counter = 0; counter < index - 1; counter++) {
    target = target.next;
  }

  Node node = new Node(t);
  node.next = target.next;
  target.next = node;
}
```

**Caution**: the implementation above fails to account for edge cases or cases where `index` is invalid!




# Delete Operation



- Trace the basic operations of a (singly) linked list implementation.
- Understand the basic operations of a (singly) linked list well enough to implement them.



Suppose we have a linked list with $n$ elements (nodes) and we want to delete an element at a given index $k$. This means we remove the $k^{th}$ node from the list. After deletion we  have $n - 1$ elements: 

* The node which was at index $k - 1$ before the deletion will remain at that index after the deletion.
* The nodes at index $k + i$ for $i \in  [1, n-1-k ]$ before the deletion will be at index $k+ j$ after the deletion for $j \in [0, n-2-k]$.

For example, we have a linked list with four nodes at indices `0`, `1`, `2`, and `3`.





We will delete the node at index `2`:





Exercise Complete the implementation of `delete` which removes a node at the given index.

```java
public void delete(int index) {
	// TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void delete(int index) {
  Node beforeTarget = head;
  for(int counter = 0; counter < index - 1; counter++) {
    beforeTarget = beforeTarget.next;
  }

  beforeTarget.next = beforeTarget.next.next;
}
```

**Caution**: the implementation above fails to account for edge cases and cases where `index` is invalid!




# Linked List Iteration



* Implement an iterator for a (singly) linked list.



Assume we pass the `head` reference variable, which points to the front of a linked list to the following Iterator class. 

Exercise Complete the implementation of `hasNext` and `next` methods.

```java
public class LinkedList implements Iterable {

  private Node head;

  // other fields/methods are not shown here!

  @Override
  public Iterator iterator() {
    return new LinkedListIterator();
  }

  private class LinkedListIterator implements Iterator {

    @Override
    public T next() {
      return null; // TODO Implement me!
    }

    @Override
    public boolean hasNext() {
      return false; // TODO Implement me!
    }
  }
}
```

Hint: describe the responsibilities of `hasNext` and `next` before implementing them.


Solution

```java
private class LinkedListIterator implements Iterator {
  private Node current;

  public LinkedListIterator() {
    current = head;
  }

  @Override
  public T next() {
    if (!hasNext()) {
      throw new NoSuchElementException();
    }

    T t = current.data;
    current = current.next;
    return t;
  }

  @Override
  public boolean hasNext() {
    return current != null;
  }
}
```




# Exercise



* Implement IndexedList operations with a (singly) linked list implementation (LinkedIndexedList).



Open the starter code and look for the `LinkedIndexedList.java` file. This file is a linked list implementation of IndexedList ADT. 

Exercise Complete the implementation of `LinkedIndexedList` (at home!)

Note here we cannot start with an empty list. Instead, we must build a complete list (with the given size and the default value) in the `LinkedIndexedList` constructor.



We have written our unit tests based on the specification of the IndexedList ADT. Therefore, we can use the same suite of tests to test both implementations of IndexedList (that is `ArrayIndexedList` and `LinkedIndexedList`). We need to switch between these two implementations. Open the starter code and find out how this is done by leveraging inheritance. (Hint: look for the abstract method `createUnit`).




Solution

Refer to the solution code posted.


# RAM model of Computation

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Describe what is meant by **efficiency** of algorithms.
* Explain the purpose of using the **RAM model of computation** for the analysis of algorithms.
* Identify what counts as a "step" in an algorithm.
* Explain how algorithm runtime is calculated under the RAM model of computation.
* Describe runtime by counting up the number of RAM instructions for a given code snippet.
* Recognize the importance of **input size** on evaluating algorithm efficiency.
* Know that what we choose to call the size of input can vary from problem to problem.
* Differentiate between the **best-case** vs the **worst-case** analysis.
* Explain why we focus on the worst-case analysis.
* Express the number of steps for a given code segment as a function of the input size in the worst-case scenario.
* Appreciate that it can be very difficult to work out precisely the exact number of steps (RAM instructions) in an algorithm.

> This chapter does not have a starter/solution code.

# A pragmatic definition of "efficiency"



* Describe what is meant by the efficiency of algorithms.



There are multiple ways to solve a computational problem, as we have seen with linear and binary search. Measuring their tradeoffs involves analyzing their **efficiency**.



Efficiency here means how an algorithm utilizes resources, namely the amount of time (and space) it uses. Moreover, we want to know how its resource consumption will _scale_ with increasing input size.

 

We need to establish the vocabulary and the required mathematical machinery to talk about the efficiency of algorithms. To that end, we introduce the **RAM** model of computation in this chapter and **asymptotic notation** in the next chapter. 



Resources

* Wikipedia entry on [Algorithmic Efficiency](https://en.wikipedia.org/wiki/Algorithmic_efficiency).
* Wikipedia entry on [Analysis of Algorithms](https://en.wikipedia.org/wiki/Analysis_of_algorithms).



# RAM: A Hypothetical Computer!



* Explain why we use the RAM model of computation for the analysis of algorithms.




Algorithms can be studied independently of the programming language used to implement them and the hardware used to execute them. This is desired as it enables us to compare algorithms and design more efficient ones before implementation. 

In this course, we stick to the Java programming language to express algorithms. For a machine-independent analysis of algorithms, we introduce a hypothetical computer called the *Random Access Machine* or **RAM** (as defined in "Algorithm Design Manual" by Steven Sekiena [with some adjustments]). 


Resources

* [The Algorithm Design Manual, 2nd Edition by Steven Skiena](http://www.algorist.com/).
* Wikipedia entry on [Random-access machine](https://en.wikipedia.org/wiki/Random-access_machine).



# Runtime under RAM



- Identify what counts as a "step" in an algorithm.
- Explain how algorithm run time is calculated under the RAM model of computation.




Under the RAM model of computation:

* **We assume we have unlimited memory.** (So we can assume the program input gets arbitrarily large). 

* **Memory access is free!** Examples include a variable declaration, looking up the value of a variable in memory, accessing an array element, accessing an object instance variable, invoking an instance method or function (just the process of method dispatch, not what the method actually does), etc. It does not matter where the item is stored (i.e., register, stack, heap, disk, or over the network).

* **Each simple operation takes exactly one time step**. Simple operations include:
  * assignment statement (`=`)
  * arithmetic operations (`+`, `-`, `*`, `/`, `%`)
  * increment/decrement operations (`++`, `--`)
  * combined assignment (`+=`, `*=`, etc), 
  * comparison/relational operations (`==`, `!=`, `<`, `<=`, etc), 
  * logical operations (`!`, `&&`, `||`) operations
  * jump operations (`return`, `break`, `continue`, etc), 
  * branch operations (`if`, `case`, etc)
  * user input, program output (print statements). 

* **Loops are not considered simple operation**. Instead, they are the composition of many single-step (simple) operations. 
  
* While calling a function/method is a free operation, the algorithm's runtime is affected by the function/method's time steps.



We measure algorithm runtime by counting the number of steps it takes on a given problem instance under the RAM model of computation.



If we assume each time step under RAM takes constant time to run on an actual computer, we can count the number of steps an algorithm takes and convert that to real time. 
# Counting Steps



* Count RAM instructions for a given code snippet.




Exercise Under the RAM model, what is the total number of *steps* it will take to execute the following code snippet? 

```java
int count = 0;  
for (int i = 0; i < 10; i++) {  
    for (int j = 0; j < 5; j++) {  
        count++;  
    }   
}  
System.out.println(count);  
```

A) about $50$ \
B) about $200$ \
C) about $1000$ 



Solution

**Answer:** about 200.

```java
int count = 0;                     // 1
for (int i = 0; i < 10; i++) {     // 1 + 11 + 10
    for (int j = 0; j < 5; j++) {  // (1 + 6 + 5) * 10
        count++;                   // (1 * 5) * 10
    }   				  
}  					   
System.out.println(count);         // 1
```

Note we ask for approximate because the details can get tricky (e.g. `i < 10` runs 11 times where ten times `i` is less than `10` and one last time it is equal to `10`). 


# Size of the Input



* Describe the runtime as a function of the input size.




Consider the following program:

```java
public class Arrays {
  public static  void print(T[] arr) {
    for (int i = 0; i < arr.length; i++) {
      System.out.println(arr[i]);
    }
  }
}
```

To determine the runtime of `Arrays.print`, we need to know the size of the input array (`arr`). 



The runtime here, as in many cases, is a _function_ of the input size.



Exercise Count the number of steps `Arrays.print` takes to execute. Write the count as a function $T(N)$ where $N$ is the size of the input array.


Solution

```java
public class Arrays {
  public static  void print(T[] arr) {
    for (int i = 0; i < arr.length; i++) { // 1 + N+1 + N
      System.out.println(arr[i]);          // 1 * N
    }
  }
}
```

$$
T(N) = 3N + 2
$$

We assumed memory access is "free" under the RAM model (i.e., `arr.length` and `arr[i]` are free operations).



# Choosing the Input



* Appreciate what we choose to call the size of input can vary from problem to problem.




Expressing the runtime as a function of the input size allows us to analyze how performance _scales_ with increasing input size.



What we choose as the input size can vary from problem to problem.



In the previous example, it was the size of the input array. However, it is the value of the variable `num` in the following example.

```java
public int myStrangeSum (int num) {
  int sum = 0;
  int count = 2 * num;
  for (int i = 0; i < count; i += 4) {
      sum += i;
  }
  return sum;
}
```

Exercise Count the number of steps `myStrangeSum` takes to execute. Write the count as a function $T(N)$ where $N$ is the value of variable `num`.


Solution

```java
public int myStrangeSum (int num) {
  int sum = 0;                            // 1
  int count = 2 * num;                    // 2
  for (int i = 0; i < count; i += 4) {    // 1 + (N/2 + 1) + N/2
      sum += i;                           // 1 * N/2
  }
  return sum;                             // 1
} 
```

$$
T(N) = \frac{3}{2}N + 6
$$

**Explanation:** At each iteration of the loop, the variable `i` is being incremented by `4`. The loop is running until `i < 2 * N`, so the loop will run $\frac{2 \times N}{4} = \frac{N}{2}$ times

 
# Best vs. Worst Case



* Differentiate between the best-case vs. the worst-case analysis.




Consider the following program:

```java
public int indexOf (String str, char target) {
  for (int i = 0; i < str.length; i++) {
    if (str.charAt(i) == target) {
      return i;
    }
  }
  return NOT_FOUND; 
}
```

The runtime of `indexOf` depends on where the `target` value is situated in the input String `str` (if it is to be found at all). Thus, we can consider two extreme cases:

* **Best-case scenario**: the minimum number of steps an algorithm takes for any input size. 
  
* **Worst-case scenario**: the maximum number of steps an algorithm takes for any input size.

Exercise Count the number of steps `indexOf` takes to execute. Write the count as a function $T(N)$ where $N$ is the size (length) of the input String (`str`). Consider the following cases: 

Assume `charAt` method is similar to array indexing and therefore a "for free" operation. Also note, the `if(/* condition */)` under the RAM model is counted as #steps in `/* condition */` expression plus $1$ for the branching itself.  

* Part 1: Best case: `str.charAt(0) == target` 

    
    Solution

    ```java
    public int indexOf (String str, char target) {
      for (int i = 0; i < str.length; i++) { // 1 + 1
        if (str.charAt(i) == target) {       // 1 + 1
          return i;                          // 1
        }
      }
      return NOT_FOUND; 
    }
    ```

    $$
    T(N) = 5
    $$

     


* Part 2: Worst case: `target` is not to be found in `str`

    
    Solution

    ```java
    public int indexOf (String str, char target) {
      for (int i = 0; i < str.length; i++) { // 1 + N+1 + N 
        if (str.charAt(i) == target) {       // (1 + 1) * N
          return i;
        }
      }
      return NOT_FOUND;                      // 1
    }
    ```

    $$
    T(N) = 4N + 3
    $$

     




When we measure performance, we typically focus on the worst-case. 



The best-case analysis is not often useful. It rarely represents a practical, real-life situation and provides little information about the performance of an algorithm.


Resources

* Wikipedia entry on [Best, worst, and average case](https://en.wikipedia.org/wiki/Best,_worst_and_average_case).



# Average Case 



* Explain why we focus on the worst-case analysis.




In addition to the best-case and the worst-case, an algorithm's **average-case** runtime is the function defined by an average number of steps taken on any instance of size $N$. This measure is typically calculated considering "random" input instances under some assumption about the relative frequencies of different inputs.  

The average-case helps to understand how good or bad an algorithm is in general, over all instances. However, it often requires domain knowledge and usually becomes too complex to calculate. 



We will focus on the worst-case analysis. It is generally easy to do and usually reflects the average case. 



So, assume I am asking for worst-case analysis unless otherwise specified!

To focus on the worst-case may seem draconian: what if an algorithm performs well in most cases except for a few pathological inputs? This case undoubtedly will be true for some algorithms. Still, in general, the worst-case analysis has been found to do a reasonable job of capturing algorithm efficiency in practice.



Resources

* Stackoverflow, [Can anyone simply explain to me what is meant by Best, worst, and average-case running times of an algorithm ???](https://stackoverflow.com/questions/9561242/best-worst-and-average-case-running-times)
* Wikipedia entry on [average-case complexity](https://en.wikipedia.org/wiki/Average-case_complexity).
* Steven J. Zeil lecture notes on [Analysis of Algorithms: Average Case Analysis](https://www.cs.odu.edu/~zeil/cs361/f17/Public/averagecase/index.html).



# Exercise I



- Express the number of steps for a given code segment as a function of the input size in the worst-case scenario.
- Appreciate that counting the exact number of RAM instructions requires too many details.



Consider the following code snippet:

```java
int sum = 0;
String s = keyboard.nextLine(); // keyboard is an initialized Scanner
String v = "aeiou";
for (int  i = 0; i < v.length(); i++) {
  if (s.indexOf(v.charAt(i)) >= 0) {
    sum++;
  }
}
```

Exercise Count the number of steps taken by the above program. Write the count as a function $T(N)$ where $N$ is the size of the "input." You need to determine what quantity is a suitable choice for the "size of input" here.


Solution

Note that while `indexOf` may seem like a constant operation, it is actually a linear search; it tells you to search through an array (in this case, a word) until you find the proper position. So, we are effectively running a linear search for each vowel (i.e., a constant number of times)

The running time of `indexOf` on string `s` is a function of the length of the string `s`. The program's running time can then be expressed as a function of $N$ where $N$ is the length of `s`. 

```java
int sum = 0;                            // 1
String s = keyboard.nextLine();         // 1 + 1
String v = "aeiou";                     // 1
for (int  i = 0; i < v.length(); i++) { // 1 + 6 + 5
  if (s.indexOf(v.charAt(i)) >= 0) {    // (1 + (4N + 3) + 1) * 5
    sum++;                              // 1 * 5
  }
}
```

$$
T(N) = 20N + 56
$$

We don't know the _exact_ running time of `indexOf` function. However, we do know it performs a linear search. Therefore, we can consider it $4N + 3$ by assuming its running time is the same as the (worst-case) running time of `indexOf` we analyzed earlier.



**Case in point:** It requires detailed knowledge of the program's constituents to count the exact number of RAM instructions a program takes to execute.



 

# Exercise II



- Express the number of steps for a given code segment as a function of the input size in the worst-case scenario.
- Appreciate it can be tough to work out precisely the exact number of RAM instructions executed.




Consider this program

```java
public int myStrangeSum (int num) {
  int sum = 0;
  int numnum = 3 * num;
  for (int i = 0; i < numnum; i++) {
    for (int j = 0; j < 5; j++) {
      for (int k = i; k > 1; k--) {
        sum++;
      }
    }
  }
  return sum;
}
```

Exercise Count the number of steps taken by the above program. Write the count as a function $T(N)$ where $N$ is the size of the "input." You need to determine what quantity is a suitable choice for the "size of input" here.


Solution

The runtime can be described as a function of $N$, variable `num`'s value.

```java
public int myStrangeSum (int num) {
  int sum = 0;                         // 1
  int numnum = 3 * num;                // 2
  for (int i = 0; i < numnum; i++) {   // 1 + (3N + 1) + 3N
    for (int j = 0; j < 5; j++) {      // (1 + 6 + 5) * (3N)
      for (int k = i; k > 1; k--) {    // (tricky! let's call it x) * 15N
        sum++;                         //  1 * x * 15N
      }
    }
  }
  return sum;                          // 1
}
```

The variable `k` takes the values of `i`:

| `i` | # iteration inner loop (not considering middle loop) | values of `k` |
| :--- | :---------------------------: | :------------- |
| `0` | will not run                | `0`           |
| `1` | will not run                | `1`           |
| `2` | once                        | `2`, `1`      |
| `3` | twice                       | `3`, `2`, `1` |
| `4` | 3                           | `4`, `3`, `2`, `1` |
| $\vdots$ | $\vdots$               | $\vdots$ |
| `N` | $N-1$                       | `N` $\dots$ `1` |
| $\vdots$ | $\vdots$               | $\vdots$ |
| `3N - 1` | $3N-2$                 | (`3N-1`) $\dots$ `1` |

Let's see how many times each statement in the most inner loop will be executed for different values of `i`:

| `i`  | `int k = i` | `k > 1` | `k--`  | `sum++` |
| :--- | :---------- | :------ | :----- | :------ |
| `0`  | $1$         | $1$     | $0$    | $0$     |
| `1`  | $1$         | $1$     | $0$    | $0$     |
| `2`  | $1$         | $2$     | $1$    | $1$     |
| `3`  | $1$         | $3$     | $2$    | $2$     |
| `4`  | $1$         | $4$     | $3$    | $3$     |
| $\vdots$  | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| `N`  | $1$         | $N$     | $N-1$  | $N-1$   |
| $\vdots$  | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| `3N - 1` | $1$         | $3N - 1$    | $3N-2$ | $3N-2$ |

Let's add up the number of times each statement is executed throughout the execution of the code snippet.

| Expression | Total count |
| ---------- | ----------- |
| `int k = i` | $5 \times (1 + 1 + 1 + \dots + 1) = 15N$ |
| `k > 1` | $5 \times (1 + 2 + 3 + \dots + (3N-1)) = 5 \times \left ( \frac{9N^2}{2} + \frac{3N}{2} \right )$  |
| `k--`   | $5 \times (1 + 2 + 3 + \dots + (3N-2)) = 5 \times \left ( \frac{9N^2}{2} - \frac{9N}{2} + 1 \right )$
| `sum++` | $5 \times (1 + 2 + 3 + \dots + (3N-2)) = 5 \times \left ( \frac{9N^2}{2} - \frac{9N}{2} + 1 \right )$

The $5$ multiplier is the effect of the second loop.

To calculate the arithmetic sums, I've used the [Gauss formula](https://mathbitsnotebook.com/Algebra2/Sequences/SSGauss.html):

$$
\sum_{i=1}^{n} = \frac{n(n + 1)}{2}
$$

Whew! Let's add up all the _counts_ (for the entire program):

$$
42N + 3 + \left (\frac{135n^2 - 75n + 20}{2} \right ) + 1
$$

Simplifying the expression, we get the following:

$$
T(N) = \frac{135N^2+9N+28}{2}
$$

I hope you will appreciate how tedious it can be to work out precisely the number of RAM instructions executed. I am uncertain as to whether I've made a mistake in counting! 

As it turns out, we will not need such precise calculations. It is sufficient for us to know that the runtime is roughly proportional to the square of the input size! More on this to come!

 
# It's hard to count precisely!



- Appreciate that counting the exact number of RAM instructions requires too many details.
- Appreciate it can be tough to work out precisely the exact number of RAM instructions executed.



From the two previous exercises, you know it can be tedious and challenging to work out precisely the exact number of RAM instructions executed by a program/algorithm. Moreover, it often requires detailed information about any function/method called by the program. 



It proves to be much easier to describe the running time of algorithms in terms of upper and lower bounds.



In the next chapter, we will introduce the **Big-Oh** notation, which significantly simplifies our analysis by ignoring details that do not impact the performance of an algorithm when the input size gets arbitrarily large. 

# Asymptotic Growth Rate

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Understand and appreciate why we do asymptotic analysis using **Big-Oh** notation.
* Use Big-Oh notation to **describe asymptotic running time** of a program when you are given the program code (or its running time as a function of input size under the RAM model).
* Use Big-Oh notation to describe the asymptotic running time of the operations of the data structures we have implemented so far.
* Understand that Big-Oh (asymptotic) notation groups runtime functions into a set of classes. 
* Enumerate **common running time functions** when the runtime is described in asymptotic (Big-Oh) notation.
* Determine the **asymptotic growth rates** of a given function using Big-Oh notation.
* **Rank** asymptotic complexities from smallest to largest.

> This chapter does not have a starter/solution code.
# Recap



* Explain why we use Big-Oh notation.




**Algorithm Efficiency**: how an algorithm utilizes resources, namely the amount of time it uses. 

**Random Access Machine or RAM**: a hypothetical computer with infinite memory and accessing memory is "free." All basic operations take one step. Loops/functions are the compositions of many single-step operations.

 **Algorithm Runtime**: the number of (RAM) steps as a function of input size. We consider worst-case analysis unless specified otherwise.



It can be difficult to precisely work out the exact number of RAM instructions executed by a program.



We will now introduce the **Big-Oh** (read it as "big O") notation which allows us to describe the growth rate of functions. The Big-Oh notion makes it easier to compare the runtime of algorithms without having to precisely count the number of (RAM) steps they take.
# Big-Oh Notation



* Use Big-Oh notation to describe the asymptotic running time of a program given its precise running time as a function of input size.




Suppose you have counted the number of steps a program takes and described the worst-case runtime as follows:

$$
T(n) = 12754n^{2} + 4353n + 834\lg n + 13546
$$

We can describe the runtime of this program simply as $\Omicron(n^2)$ (read it "big O of $n$ squared"). 



$T(n)$ is the **precise** running time whereas $\Omicron(n^2)$ is the program's **asymptotic** running time.



**How do we go from $T(n)$ to $\Omicron(n^2)$?**

We suppress the constant factors (set them to $1$, resulting in $n^{2} + n + \lg n + 1$) and drop (ignore) the lower-order terms (resulting in $n^{2}$).

This simplification may seem excessively imprecise. In particular, we put so much effort into calculating the running time under the RAM model in the previous chapter! But, it turns out this _approximation_ is sufficient to compare the efficiency of algorithms. 
# The Gist!



* Use Big-Oh notation to describe the asymptotic running time of a program given its precise running time as a function of input size.




It turns out, in an expression like $12754n^{2} + 4353n + 834\lg n + 13546$, those constant factors will change based on how you *express* an algorithm. 

For example, consider these three code snippets:

```java
public void countUp(int num) {
  for (int i = 1; i <= num; i++) {    
      System.out.println(i);                          
  }
}
```

```java
public void countUp(int num) {
  int count = 1;
  for (int i = 0; i < num * 2; i+=2) {    
      System.out.println(count++);                          
  }
}
```

```java
public void countUp(int num) {
  int count = 1;
  int upper = num * 2;
  for (int i = 1; i <= upper; i+=2) {    
      System.out.println(count++);                          
  }
}
```

The three programs above do the same thing (printing out numbers from $1$ to $N$ where $N$ is the value of `num`). So the running time of all three will be in the form of a first-degree polynomial $T(N) = aN + b$ where the coefficients $a$ and $b$ will be different for each program. In general, the coefficients (constant factors) depend on the following factors.

* How we express an algorithm. 
* The programming language used to implement the algorithm
* The compiler which converts the program to machine instructions.
* The [Instruction Set Architecture](https://en.wikipedia.org/wiki/Instruction_set_architecture) of the hardware it will eventually run on. 

> To normalize these variations, we can drop the constants (set the coefficients to $1$).


So $T_1(n) = 12754n^{2} + 4353n + 834\lg n + 13546$ becomes $T_2(n) = n^{2} + n + \lg n + 1$.

Moreover, when the input gets larger and larger, the highest-order term, $n^{2}$, is going to be much much larger than all the lower-order terms combined (i.e., $n + \lg n + 1$). 







> So, when the input gets arbitrarily large, the performance comes down to the _dominant_ term.


Why does the dominant term dictate the performance?

The following table is from the book "Algorithm Design" by Jon Kleinberg and Eva Tardos (2006). You can see how functions' runtime grows with increasing the input size.









**So the Big-Oh notation says:**

* Suppress constant factors as they are dependent on how the algorithm is expressed.
* Ignore lower-order terms as they are irrelevant when the input gets arbitrarily large.




Resources

* freeCodeCamp has a nice article, [Big O Notation — Simply explained with illustrations and video](https://www.freecodecamp.org/news/big-o-notation-simply-explained-with-illustrations-and-video-87d5a71c0174/).
* Khan Academy has an article on [Asymptotic notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation).
* This is an interesting article: [Big-O notation explained by a self-taught programmer](https://justin.abrah.ms/computer-science/big-o-notation-explained.html).
* Chapter 2: Basics of Algorithm Analysis in the book "Algorithm Design" by Jon Kleinberg and Eva Tardos, 2006.



# Exercise I



* Use Big-Oh notation to describe the asymptotic running time of a program given its precise running time as a function of input size.




Assume that each of the expressions below gives the processing time $T(n)$ spent by an algorithm for solving a problem of size $n$. Specify the asymptotic running time in Big-Oh notation.

Exercise $T_1(n) = 5 + 0.001n^3 + 0.025n$


Solution

The dominant term is $n^3$, so the runtime is $\Omicron(n^3)$.



Exercise $T_2(n)=3\lg n^2+(\lg n)^2 + (n+1)^2\lg (4n)$


Solution

Let's simplify the expression; this requires a certain level of mathematical literacy. Most of what is done below are based on the [laws of logarithms](https://en.wikipedia.org/wiki/Logarithm#Logarithmic_identities).

$$
= 3\times 2\lg n + \lg^2 n + (n^2 + 2n + 1)(\lg 4 + \lg n)
$$

$$
= 6\lg n + \lg^2 n + (n^2+2n+1)(2+\lg n)
$$

$$
= 6\lg n + \lg^2 n + 2n^2 + 4n + 2 + n^2\lg n + 2n\lg n + 2\lg n
$$

$$
= n^2\lg n + 2n^2 + 2n\lg n + 4n + \lg^2 n + 8\lg n + 2
$$

The dominant term is $n^2\lg n$, so the runtime is $\Omicron(n^2\lg n)$.

Note, $\lg n$ is $\log_2 n$ and $n^2\lg n$ is larger than $2n^2$ for all $n > 4$.

You might be wondering how I know which term has the highest order (fastest rate of growth). This, again, requires a certain level of mathematical literacy. However, a dozen functions show up in most practical cases, and we will survey them in a later lesson.  




# Exercise II



* Use Big-Oh notation to describe the asymptotic runtime of a program.




Consider the following program

```java
public boolean contains (int[] arr, int target) {
  for (int i = 0; i < arr.length; i++) {
    if (arr[i] == target) {
      return true;
    }
  }
  return false; 
}
```

Exercise What is the asymptotic running time of the code above for searching one array, as a function of the array length $n$?

A) $\Omicron(n)$ \
B) $\Omicron(\lg n)$ \
C) $\Omicron(n^2)$


Solution

The correct answer is $\Omicron(n)$. The key observation is that the code performs a constant number of operations for each array entry. Here "constant" means some number independent of $n$. In the Big-Oh notation, we suppress the constant factors.

Reminder: we are considering the worst-case scenario. 



# Exercise III



* Use Big-Oh notation to describe the asymptotic runtime of a program.




Consider the following program

```java
public boolean contains (int[] a, int[] b, int target) {
  for (int i = 0; i < a.length; i++) {
    if (a[i] == target) {
      return true;
    }
  }

  for (int i = 0; i < b.length; i++) {
    if (b[i] == target) {
      return true;
    }
  }

  return false; 
}
```

Exercise What is the asymptotic running time of the code above for searching two arrays, as a function of the array lengths $n$?

A) $\Omicron(n^2)$ \
B) $\Omicron(2n)$ \
C) $\Omicron(n)$


Solution

The answer is the same as before, $\Omicron(n)$. The reason is that the worst-case number of operations performed (in an unsuccessful search) is twice that of the program in the previous exercise. This extra factor of 2 contributes only to the leading constant in the running time, and it will be suppressed in Big-Oh notation.


# Exercise IV



* Use Big-Oh notation to describe the asymptotic runtime of a program.




Consider the following program

```java
public boolean hasCommonElement (int[] a, int[] b) {
  for (int i = 0; i < a.length; i++) {
    for (int j = 0; j < b.length; j++) {
      if (a[i] == b[j]) {
        return true;
      }
    }
  }

  return false; 
}
```

Exercise What is the asymptotic running time of the code above for checking for a common element, as a function of the array lengths $n$? Assume both arrays are of the same size.

A) $\Omicron(n^2)$ \
B) $\Omicron(n!)$ \
C) $\Omicron(n)$


Solution

The answer is $\Omicron(n^2)$. The program performs a constant number of operations for each loop iteration (for each choice of the indices `i` and `j`). However, for each iteration of the outer for loop, the code performs $n$ iterations of the inner for-loop. This gives $n \times n = n^2$ iterations in all.


# Exercise V



* Use Big-Oh notation to describe the asymptotic runtime of a program.




Consider the following program

```java
public boolean hasDuplicates (int[] a) {
  for (int i = 0; i < a.length; i++) {
    for (int j = i + 1; j < a.length; j++) {
      if (a[i] == a[j]) {
        return true;
      }
    }
  }

  return false; 
}
```

Exercise What is the asymptotic running time of the code above for checking for duplicates as a function of the array lengths $n$?

A) $\Omicron(n^2)$ \
B) $\Omicron(\lg n)$ \
C) $\Omicron(n)$


Solution

The answer is the same as before: $\Omicron(n^2)$. The running time is proportional to the number of nested loops iterations (with a constant number of operations per iteration). 

So how many iterations are there? The answer is roughly $n^2$:

* One way to see this is to remember that the program performs roughly half the work in the previous exercise (since the inner loop starts at `j = i + 1` rather than `j = 0`). 
  
* A second way is to observe when `i = 0` the inner loop runs $(n - 1)$ times. When `i = 1` the inner loop runs $(n-2)$ times. $\dots$ When `i = n - 1`, the inner loop runs $0$ times. So the total number of times the inner loop runs (which is where the program does its most work) is: 

$$
(n-1) + (n-2) + \dots + 0 = \frac{n(n-1)}{2} \in \Omicron(n^2)
$$


# Exercise VI



* Use Big-Oh notation to describe the asymptotic runtime of a program.



Consider the following program

```java
public int myStrangeSum (int num) {
  int sum = 0;
  for (int i = 1; i < num; i *= 2) {
    sum += i;
  }

  return sum; 
}
```

Exercise What is the asymptotic running time of the code above as a function of $n$ where $n$ is the value of `num`?

A) $\Omicron(n^2)$ \
B) $\Omicron(\lg n)$ \
C) $\Omicron(2^n)$


Solution

The answer is $\Omicron(\lg n)$. 

It might be easier to understand this if, without lack of generality, we assume `num` is a power of $2$ and rewrite the loop as 

```java
for (int i = num / 2; i > 0; i /= 2) {
  sum += i;
}
```

How many times can you divide `num` (i.e., $n$) to get to $1$? We answered this question when we analyzed the running time of the binary search algorithm. The answer was $\lg n$.




Resources

This video might be helpful: [Deeply Understanding Logarithms In Time Complexities & Their Role In Computer Science](https://youtu.be/M4ubFru2O80).



# Exercise VII



* Use Big-Oh notation to describe the asymptotic runtime of a program.



Consider the following program

```java
public int indexOf (String str, char target) {
  for (int i = 0; i < str.length; i++) {
    if (str.charAt(i) == target) {
      return i;
    }
  }
  return NOT_FOUND; 
}
```

We have counted the number of steps `indexOf` takes to execute and expressed it as a function $T(N)$ where $N$ is the size of the input String (`str`). 

| Scenario   | Exact running time | Asymptotic running time |
| :--------- | :----------------: | :---------------------: |
| Best-case  | $T(N) = 4$         |                         |
| Worst-case | $T(N) = 4N + 3$    |                         |

Exercise Complete the table above. Use Big-Oh for expressing asymptotic running time.


Solution

| Scenario | Exact running time | Asymptotic running time |
| :------- | :-----------------: | :----------------------: |
| Best-case | $T(N) = 4$ |  $\Omicron(1)$ |
| Worst-case | $T(N) = 4N + 3$ | $\Omicron(N)$ |

The $\Omicron(1)$ represents the running time that does not depend on the input size. However, it does not mean it only takes one step!
  

# Exercise IIX



* Use Big-Oh notation to describe the asymptotic running time of the operations of the data structures we have implemented so far.




Consider the array data structure (i.e., Java's built-in array). Then, we define the following operations:

* **insert front**: insert an element to the front of an array (prepend the array). This is not updating the value of an existing element.
* **insert middle**: insert an element at a random index $i$ where $0 < i < \text{length} - 1$
* **insert back**: insert an element to the end of the array (append the array). assume array has enough capacity.
* corresponding **delete** operations (delete front, delete middle, delete back).
* **access**: read an element at a random index.

Exercise Based on your understanding of the array data structure, complete the following table. Use Big-Oh notation for expressing asymptotic runtime.

| Operation     | Asymptotic runtime |
| :------------ | :----------------: |
| insert front  |                    |
| insert middle |                    |
| insert back   |                    |
| delete front  |                    |
| delete middle |                    |
| delete back   |                    |
| access        |                    |


Solution

| Operation     | Asymptotic runtime |
| :------------ | :------------  |
| insert front  | $\Omicron(n)$ — shift all elements to right to make room for new one |
| insert middle | $\Omicron(n)$ — shift elements to make a gap |
| insert back   | $\Omicron(1)$ — insert at `arr[numElements]` |
| delete front  | $\Omicron(n)$ — shift all elements to left to fill the gap |
| delete middle | $\Omicron(n)$ — shift elements to fill a gap |
| delete back   | $\Omicron(1)$ — decrement `numElements` |
| access        | $\Omicron(1)$ — performs constant # arithmetic operations! |

Accessing an element at a given position is $\Omicron(1)$:

* It takes a constant number of arithmetic operations to figure out where the element is located in the computer memory; the program knows the beginning of the array and the size of each element. 
* Under the RAM model, it is "free" to access each memory location. In reality, it takes a constant time (a bounded time) to access each address in the computer memory.
* Since finding the address is $\Omicron(1)$, and retrieving the element in it is also $\Omicron(1)$, it gives you a total of $\Omicron(1)$.

Please note that some operations can be implemented differently and will have a different asymptotic runtime. For example, if we don't care for the order in which the values were inserted, we can implement "delete front" and "delete middle" to replace the front or middle element with the last and then perform "delete back." This alternative implementation will be $\Omicron(1)$. 




# Exercise IX



* Use Big-Oh notation to describe the asymptotic running time of the operations of the data structures we have implemented so far.




Consider the (singly) linked list data structure (as it was described in an earlier chapter). Then, we define the following operations:

* **insert front**: insert a element to the front of a linked list (prepend the list). This is not updating the value of an existing element.
* **insert middle**: insert an element at a random index at a random index $i$ where $0 < i < \text{length} - 1$
* **insert back**: insert an element to the end of the list (append the list). Assume you have only a pointer to the front of the list.
* corresponding **delete** operations (delete front, delete middle, delete back).
* **access** read the data stored in a node at a random position.

Exercise Based on your understanding of the linked list data structure, complete the following table. Use Big-Oh notation for expressing asymptotic runtime.

| Operation     | Asymptotic runtime |
| :-----------  | :----------------: |
| insert front  |                    |
| insert middle |                    |
| insert back   |                    |
| delete front  |                    |
| delete middle |                    |
| delete back   |                    |
| access        |                    |


Solution

| Operation     | Asymptotic runtime |
| :------------ | :------------  |
| insert front  | $\Omicron(1)$ — point the `head` to a new node |
| insert middle | $\Omicron(n)$ — reach the element before target position |
| insert back   | $\Omicron(n)$ — reach the last element |
| delete front  | $\Omicron(1)$ — update the `head` pointer |
| delete middle | $\Omicron(n)$ — reach the element before target position |
| delete back   | $\Omicron(n)$ — reach the last element |
| access        | $\Omicron(n)$ — reach the target element |
  

# Exercise X

For your reference, below are the iterators for an array-based and a linked-based implementation of the IndexedList ADT.


ArrayIndexedListIterator

```java
public class ArrayIndexedList implements IndexedList {

  private T[] data;

  /* other fields/methods not shown */

  @Override
  public Iterator iterator() {
    return new ArrayIndexedListIterator();
  }

  private class ArrayIndexedListIterator implements Iterator {
    private int nextIndex;

    private ArrayIndexedListIterator() {
      nextIndex = 0;
    }

    @Override
    public boolean hasNext() {
      // you can directly access the private member 'data'
      return nextIndex < data.length;
    }

    @Override
    public T next() {
      if (!hasNext()) {
        throw new NoSuchElementException();
      }
      T t = data[nextIndex];
      nextIndex += 1;
      return t;
    }
  }
}
```




LinkedIndexListIterator

```java
public class LinkedIndexedList implements IndexedList {

  private Node head;

  /* other fields/methods not shown */

  @Override
  public Iterator iterator() {
    return new LinkedIndexListIterator();
  }

  // Node does not have access to members of LinkedIndexedList
  // because it is static
  private static class Node {
    T data;
    Node next;
  }

  // An iterator to traverse the linked list from front (head) to back.
  private class LinkedIndexListIterator implements Iterator {
    private Node current;

    LinkedIndexListIterator() {
      current = head;
    }

    @Override
    public boolean hasNext() {
      return current != null;
    }

    @Override
    public T next() {
      if (!hasNext()) {
        throw new NoSuchElementException();
      }
      T t = current.data;
      current = current.next;
      return t;
    }
  }
}
```



Exercise Complete the following table. Use Big-Oh notation for expressing asymptotic runtime.

| Operation   | `ArrayIndexedListIterator` | `LinkedIndexListIterator` |
| :---------  | :-----------------------:  | :-----------------------: |
| constructor |                            |                           |
| `next`      |                            |                           |
| `hasNext`   |                            |                           |


Solution

| Operation   | `ArrayIndexedListIterator` | `LinkedIndexListIterator` |
| :---------  | :-----------------------:  | :-----------------------: |
| constructor |           $\Omicron(1)$           |            $\Omicron(1)$         |
| `next`      |           $\Omicron(1)$           |            $\Omicron(1)$         |
| `hasNext`   |           $\Omicron(1)$           |            $\Omicron(1)$         |
  

# Growth Rate



* Understand Big-Oh notation groups functions into sets of functions that share a common characteristic about their rate of growth.



We have, so far, taken an informal approach to the application of the Big-Oh notion. If I tell you the runtime of a function is $T_1(n) = 3n + 5$ for instance, you know its running time is $\Omicron(n)$. You know this because you learned to suppress constant factors and ignore lower terms in $T_1(n)$. As such, you know the running times $T_2(n)=0.34n$ and $T_3(n) = 234234n$ are also $\Omicron(n)$.



It seems that $\Omicron(n)$ describes a group of functions that all share a characteristic: they all exhibit a runtime that linearly _grows_ as the size of input increases.



The second algorithm ($T_2$) is obviously faster among the three, but the difference between them is negligible in comparison to another algorithm with, e.g., the runtime of $\Omicron(n^2)$. 



The $g(n) = n^2$ function **grows much faster** than $f(n) = cn$ when **$n$ get arbitrarily large** for any constant $c > 0$ — constant means not a function of $n$.





Why does $n^2$ grow faster than $n$?








Why are we analyzing runtime when the input gets arbitrarily large?!


When it comes to computational problems, the input usually gets bigger. Think about it! Your social network app is likely to gain more users and more cat pictures as time goes on!! We are living in the era of "big data," after all!



We can, more formally, define Big-Oh notation as follows.



$\Omicron(f(n))$ is a **set (or class) of functions** that grow _no faster than_ $f(n)$.



So when we say the running time of a program is in $\Omicron(n)$ for instance, we say its running time is a function, one of many in a _set_ of functions that all share a *linear rate of growth*. 

This also explains the use of set membershp operator $\in$. Indeed, we write $T_1(n) \in \Omicron(n)$. Although, many references write $T_1(n) = \Omicron(n)$ which (in this context) means the same thing.



**Case in point!**

Big-Oh notation groups runtime functions. All functions in a particular group are considered to have the same (asymptotic) efficiency.



This means we consider $T_1(n)$, $T_2(n)$, and $T_3(n)$, all having the same _efficiency_!
# Exercise XI



* Rank asymptotic complexities from smallest to largest.



You are the CEO of a startup software company. The app your company is building requires a component that performs an image processing task. Your intern is told to survey the literature for potential solutions to this computational task. Upon studying the literature, the intern finds three algorithms that solve the problem at hand. The algorithms have the following runtimes: $\Omicron(\lg n)$, $\Omicron(2^n)$, and $\Omicron(n)$. 

Exercise Which algorithm will you choose to be implemented?

A) The one with $\Omicron(n)$ runtime. \
B) The one with $\Omicron(2^n)$ runtime. \
C) The one with $\Omicron(\lg n)$ runtime. 


Solution

The answer we are looking for here is the one with $\Omicron(\lg n)$ runtime. 



The $\lg n$ grows slower than $n$, and they both grow much slower than $2^n$, as $n$ gets arbitrarily large.



So an algorithm with runtime proportional to $\lg n$ is faster than one with runtime proportional to $n$. Moreover, they both are much quicker than one with runtime proportional to $2^n$, when the input size gets arbitrarily large.

Please note:

* In reality, you need to consider other information for choosing a solution. Information such as the resources required to implement/run each algorithm, their tradeoffs other than their runtime, etc.

* We are considering the use of Big-Oh notion in _conversational language_ that programmers use to describe how fast algorithms run. To be pedantic, Big-Oh describes an upper-bound on the growth rate of a function. Relying only on the mathematical definition of Big-Oh, these three algorithms might all have the same runtime! We will explore this point further in the next chapter.



# Common Running Times



* Rank asymptotic complexities from smallest to largest.




Here are the common running times in the order of increasing growth rate.

* $\Omicron(1)$ — Constant functions
  * Running time does not depend on the input size.
  * It does not mean it only takes one step!
* $\Omicron(\log n)$ — Logarithmic functions
  * Also called **sub-linear** functions.
  * Runtime grows slower than the size of the problem.
  * Example: Binary Search.
* $\Omicron(n)$ — Linear functions
  * Running time is proportional to input size.
  * Example: Linear search.
* $\Omicron(n\log n)$ — Superlinear functions
  * Also called **Linearithmic** functions
  * Arises in [divide-and-conquer algorithms](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm).
  * Example:  Merge sort and heapsort (we'll cover later).
* $\Omicron(n^2)$ — Quadratic functions
  * Running time is proportional to the square of the input size.
  * Example: Bubble/Insertion/Selection sort (we'll cover later).
* $\Omicron(n^k)$ — Polynomial functions
  * $k$ is a constant $>1$
  * $\Omicron(n^2)$ is a special case of this 
  * Example: Enumerate all subsets of $k$ nodes among $n$ nodes.
* $\Omicron(c^n)$ — Exponential functions
  * $c$ is a constant $>1$
  * Example: Enumerating all subsets of $n$ items. 
* $\Omicron(n!)$ — Factorial functions
  * Example: Generating all permutations or orderings of $n$ items.



All you need to appreciate is the order of increasing growth rate:

$$
n! \gg 2^n \gg n^3 \gg n^2 \gg n\log n \gg n \gg \log n \gg 1
$$



There are other (more esoteric) functions that arise in the study of algorithms. However, the functions above account for describing the runtime of basic algorithms (those used in this course and you may come by during a coding interview).

The following illustration is taken from [bigocheatsheet.com](https://www.bigocheatsheet.com/):






Resources

[This article](https://apelbaum.wordpress.com/2011/05/05/big-o/) contains an explanation of Big-Oh notation, common running times, and a good-looking plot of those runtimes!



# Exercise XII



* Rank asymptotic complexities from smallest to largest.



Exercise Two alternative algorithms, $A$ and $B$, have _logarithmic_ and _linear_ runtimes, respectively. Which statement is true?

A) $A$'s runtime function grows faster thus $A$ is faster \
B) $A$'s runtime function grows slower thus $A$ is faster \
C) $B$'s runtime function grows faster thus $B$ is faster \
D) $B$'s runtime function grows slower thus $B$ is faster


Solution

You can imagine A is binary search and B is linear search. 
Based on the information we have, A runs faster. How do we know that?
Because $\lg n$ grows _slower_ than $n$, as $n$ gets larger and larger, which means it takes less time for $A$ to do its work. So the answer is (B).







# Exercise XIII



* Rank asymptotic complexities from smallest to largest.




Exercise For each of the $T(n)$ running time listed below, figure out the asymptotic runtime and express it in Big-Oh notation. Then rank the functions in growth rate order, **starting with the slowest growth rate (i.e those resulting in a fast runtime), and ending with the fastest growth rate (worst runtime).** If two functions have the same asymptotic runtime, then rank them based on the original expressions (including constants).

| $T(n)$                               | Big-Oh | Rank |
| :----------------------------------- | ------ | ---- |
| $\left ( \lg \frac{n}{4} \right )^3$ |        |      |
| $(n^2-4)/(n+2)$                      |        |      |
| $\left ( 3n + \lg n \right )^2$      |        |      |
| $2 \lg n^2$                          |        |      |
| $\left ( 2^n \right )^2 + \lg n$     |        |      |
| $n^2 \lg 16$                         |        |      |
| $2n\lg\lg n$                         |        |      |
| $4n^2+n(10 + \lg n)$                 |        |      |



Solution

| $T(n)$                               | Big-Oh                                                       | Rank |
| :----------------------------------- | ------------------------------------------------------------ | :----: |
| $\left ( \lg \frac{n}{4} \right )^3$ | $\left ( \lg n - \lg 4 \right )^3 = \left ( \lg n - 2 \right )^3 \in \Omicron(\lg^3 n)$ | 2    |
| $(n^2-4)/(n+2)$                      | $(n-2)(n+2)/(n+2) \in \Omicron(n)$                                  | 3    |
| $\left ( 3n + \lg n \right )^2$      | $9n^2+\lg^2 n + 6n\lg n \in \Omicron(n^2)$                          | 7    |
| $2 \lg n^2$                          | $4 \lg n \in \Omicron(\lg n)$                                       | 1    |
| $\left ( 2^n \right )^2 + \lg n$     | $4^n+\lg n \in \Omicron(4^n)$                                       | 8    |
| $n^2 \lg 16$                         | $4n^2 \in \Omicron(n^2)$                                            | 5    |
| $2n\lg\lg n$                         | $\Omicron(n\lg\lg n)$                                               | 4    |
| $4n^2+n(10 + \lg n)$                 | $4n^2+10n+n\lg n \in \Omicron(n^2)$                                 | 6    |





Resources

[Logarithmic identities](https://en.wikipedia.org/wiki/Logarithm#Logarithmic_identities) are essential for solving the above exercise.



# Asymptotic Analysis

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain what is meant by **asymptotic complexity analysis** of an algorithm.
* Contrast between **time vs. space complexity**.
* Express the space requirements for a given code segment as a function of the input size in the worst-case scenario.
* Express the formal, **mathematical definition of Big-Oh**.
* Use the mathematical definition of Big-Oh to prove the asymptotic running time of a given program.
* Express the **mathematical definition of Big Omega**.
* Use the mathematical definition of Big Omega to prove the asymptotic running time of a given program.
* Recognize that Big-Oh and big Omega are **not** necessarily tight bounds.
* Recognize growth rate type (upper or lower bound) is different from worst-case vs. best-case analysis.
* Express the **mathematical definition of Big Theta**.
* Use the definition of Big Theta to show the asymptotic running time of a given program.
* Enumerate various asymptotic notations used in this course.
* Elaborate on the benefits of using asymptotic notation and worst-case analysis to study the computational complexity of algorithms.

> This topic will be the most mathematically demanding part of this course. However, once you understand the intuition behind formalism, it becomes a lot easier to deal with.

# Time Complexity



- Explain what is meant by asymptotic complexity analysis of an algorithm.



Asymptotic analysis (or asymptotic complexity analysis) uses asymptotic notation (like Big-Oh) to describe the _computational complexity_ of an algorithm. 

The computational complexity of an algorithm is (generally) about how it consumes computational resources, namely time complexity and space complexity.



Time complexity measures the amount of time an algorithm needs to run as a function of its input size. 



We've measured time complexity so far. Next, we will look at space complexity!


Resources

* Wikipedia entry on [Asymptotic computational complexity](https://en.wikipedia.org/wiki/Asymptotic_computational_complexity).



# Space Complexity



- Contrast between time vs. space complexity.



Space complexity measures the amount of memory that an algorithm needs to run as a function of its input size. That means how much memory is required in the worst case at any point in the algorithm. 

Similar to time complexity, we're concerned with how the space consumption grows, in asymptotic terms, as the size of the input increases.



Often space complexity is taken to mean **auxiliary space**.



* Auxiliary space is the *extra space* or the *temporary space* used by an algorithm during its execution. 
  
* We can say Space Complexity $=$ Auxiliary Space $+$ Input space. 
  
* When we compare two algorithms for solving a computational problem, it is arguably more useful to compare their auxiliary space usage since the input space will be the same for a given problem.



Auxiliary space and space complexity are not the same. In this class, we will specify when to find auxiliary space, but when asked for space complexity, consider that Space Complexity $=$ Auxiliary Space $+$ Input space.



Exercise Complete the following table. Use Big-Oh notation to asymptotically describe time/space complexities.

| Algorithm | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| :--------- | :---------------: | :----------------: | :----------- | :---------------: |
| Linear search |   |    |   |   |
| Binary search |   |    |   |   |


Solution

The following is based on the worst-case analysis.

| Algorithm | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| :--------- | :---------------: | :----------------: | :-----------: | :---------------: |
| Linear search | $\Omicron(n)$      |    $\Omicron(n)$        |  $\Omicron(n)$     |  $\Omicron(1)$         |
| Binary search | $\Omicron(\lg n)$  |    $\Omicron(n)$        |  $\Omicron(n)$     |  $\Omicron(\lg n)$ or $\Omicron(1)$  |

The auxiliary space of binary search depends on its implementation. An iterative implementation takes $\Omicron(1)$, but a recursive implementation could be $\Omicron(\lg n)$ — unless the programming language used has optimization for _tail recursion_ (beyond the scope of this course).

Please refer to [this](https://iq.opengenus.org/binary-search-iterative-recursive/) article for "Iterative vs. Recursive Binary Search Algorithm." 




Resources

* Baeldung's article [Understanding Space Complexity](https://www.baeldung.com/cs/space-complexity).
* StudyTonight's article [Space Complexity of Algorithms](https://www.studytonight.com/data-structures/space-complexity-of-algorithms).
* North Western University's EECS 311 lecture notes on [Space Complexity](https://courses.cs.northwestern.edu/311/html/space-complexity.html).



# Exercise I



* Express the space requirements for a given code segment as a function of the input size in the worst-case scenario.




Consider the following implementation of `countup`:

```java
public static void countup(int n) {
  for (int i = 1; i <= n; i++) {
    System.out.println(i);
  }
}
```

```java
public static void countup(int n) {
  if (n > 1) {
    countup(n - 1);
  } 
  System.out.println(n);
}
```

Exercise Complete the following table. Use Big-Oh notation to asymptotically describe time/space complexities.

| `countup` | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| --------- | --------------- | ---------------- | ----------- | --------------- |
| First program |   |    |   |   |
| Second prog. |   |    |   |   |



Solution

| `countup` | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| --------- | :---------------: | :----------------: | :-----------: | :---------------: |
| First | $\Omicron(n)$      |    $\Omicron(1)$        |  $\Omicron(1)$     |  $\Omicron(1)$         |
| Second | $\Omicron(n)$  |    $\Omicron(n)$        |  $\Omicron(1)$     |  $\Omicron(n)$  |

Each recursive call creates a *function activation record* (or **stack frame**) in memory. Each activation record is a memory allocation for the function with space for local variables, etc. Each activation record for the recursive `countup` requires $\Omicron(1)$ space, and there will be $n$ of them, thus $\Omicron(n)$ total space.




Resources

* [Activation Record](https://wiki.c2.com/?ActivationRecord) entry on C2 Wiki.
* Wikipedia's entry on [Call stack](https://en.wikipedia.org/wiki/Call_stack).



# Exercise II



* Express the space requirements for a given code segment as a function of the input size in the worst-case scenario.




Consider the following implementations of `Arrays.reverse`:

```java
public class Arrays {
  public static  void reverse(T[] arr) {
    T temp;
    for (int i = 0, j = arr.length - 1; i < j; i++, j--) {
      temp = arr[i];
      arr[i] = arr[j];
      arr[j] = temp;
    }
  }
}
```

```java
public class Arrays {
  public static  void reverse(T[] arr) {
    int n = arr.length;
    T[] tmp = (T[]) new Object[n];

    for (int i = 0; i < n; i++) {
      tmp[i] = arr[n - i - 1];
    }

    for (int i = 0; i < n; i++) {
      arr[i] = tmp[i];
    }
  }
}
```

Exercise Complete the following table. Use Big-Oh notation to asymptotically describe time/space complexities.

| `Arrays.reverse` | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| --------- | --------------- | ---------------- | ----------- | --------------- |
| First program |   |    |   |   |
| Second prog. |   |    |   |   |



Solution

| `Arrays.revers` | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| --------- | :---------------: | :----------------: | :-----------: | :---------------: |
| First program | $\Omicron(n)$      |    $\Omicron(n)$        |  $\Omicron(n)$     |  $\Omicron(1)$         |
| Second prog. | $\Omicron(n)$     |    $\Omicron(n)$        |  $\Omicron(n)$     |  $\Omicron(n)$  |


# Big-Oh: Mathematical Definition



* Express the mathematical definition of Big-Oh.





$$
T(n) \in \Omicron(f(n))\Leftrightarrow \exists \\; c > 0, n_0 > 0 \\; \\; s.t. \\; \\; T(n) \le c\cdot f(n) \\; \\; \forall \\; n \ge n_0
$$



Read it as 



$T(n)$ is a member of $\Omicron(f(n))$ if and only if there exist positive constants $c$ and $n_0$ such that $T(n)\le c\cdot f(n)$ for all $n\ge n_0$.



Here is a pictorial illustration of the above definition:





If you want to _show_ that $T(n) \in \Omicron(f(n))$, you need to choose the constants $c$ and $n_0$ so that above definition holds whenever $n \ge n_0$. 

Exercise The running time of an algorithm is $T(n)=7n^2+5$. Show that this algorithm has quadratic runtime, i.e., show $T(n) \in \Omicron(n^2)$.


Solution

We can choose $c=12$ and $n_0=1$ for the definition of Big-Oh to hold.

There is not a magic formula to find the constants. However, you can usually guess the values or deduct them by simple algebraic modification of $T(n)$.

$$
T(n) = 7n^2 + 5 \le 7n^2 + 5n^2 \le 12n^2 
$$

Please note the choice of $n_0$ and $c$ are not unique. Moreover, these constants are not necessarily integers. According to the definition, these constants are non-zero and positive real values. Of course, if $n$ represents the program input size, then it is imperative to be an integer. 

Please also note I have deliberately asked you to "show" $T(n) \in \Omicron(n^2)$ and not "to prove." However, a proof generally would require finding a pair of $c$ and $n_0$ for the definition to hold. Of course, you should present your argument more formally. For example, the following would make reasonable proof. Notice I deliberately chose a different pair of $c$ and $n_0$ from those selected above.

**Proof:** Let's set $c = 10$. Now suppose that $7n^2 + 5 > 10n^2$. Then by algebraic simplification, we get $3n^2 < 5$, and thus $-\sqrt{\frac{5}{3}} < n < \sqrt{\frac{5}{3}}$. So by the counterpositive, $7n^2 + 5 \le 10n^2$ for all $n \ge \sqrt{\frac{5}{3}}$. So if we set $n_0 = \left \lceil \sqrt{\frac{5}{3}} \right \rceil = 2$, we have proved that $7n^2 + 5 \le cn^2$ for $c = 10$ for all $n > n_0$. Thus we have proved that $T(n) \in \Omicron(n^2)$. **Q.E.D.**




Resources

* Khan academy's article on [Big O notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation).
* Wikipedia's entry on [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation).
* This article is well written: [Big-O Notation — A Primer](https://jeremykun.com/2011/06/14/big-o-notation-a-primer/).



# Exercise III



* Use the mathematical definition of Big-Oh to show the asymptotic running time of a given program.



Consider the following function $T(n)$ describes the precise running time of an algorithm:

$$
T(n) = 3n^2 - 100n + 6
$$



Exercise Show $T(n) \in \Omicron(n^2)$.


Solution

We can choose $c=3$ and $n_0=1$ for the definition of Big-Oh to hold.

$$
3n^2 - 100n + 6 \le 3n^2
$$

> Recall the choice of $n_0$ and $c$ are not unique.

There can be many (actually, infinitely many) different
combinations of $n_0$ and $c$ that would make the Big-Oh definition to work.
It depends on what inequalities you use while doing the upper-bounding.



Exercise Show $T(n) \in \Omicron(n^3)$.


Solution

We can choose $c=1$ and $n_0=1$ for the definition of Big-Oh to hold.

$$
3n^2 - 100n + 6 \le n^3
$$

> Big-Oh expresses an upper bound but does not necessarily provide a tight upper bound.

For example, you can easily show $T(n) \in  \Omicron(n^p)$ for any $p \ge 2$.



Exercise Show $T(n) \notin \Omicron(n)$.


Solution

There simply is no $c$ and $n_0$ where the Big-Oh definition would hold for $T(n) \in \Omicron(n)$.



# Exercise IV



* Use the mathematical definition of Big-Oh to prove the asymptotic running time of a given program.







Consider the following facts:

* Baby chicken might be larger than baby turkey at the beginning. 
* But after a certain "breakpoint," the chicken size will be surpassed by the turkey size.
* From the breakpoint on, the chicken size will always be smaller than the turkey size.

Exercise Which statement is true?

A) chicken size is in $\Omicron($turkey size$)$. \
B) turkey size is in $\Omicron($chicken size$)$.


Solution

Chicken's growth order is smaller than turkey's, or chicken size is in $\Omicron($turkey size$)$.

The breakout point is the $n_0$ in the mathematical definition of Big-Oh.






# Asymptotic Upper Bounds



* Appreciate that Big-Oh describes an upper bound on the running time of a program.



Recall this exercise from the last chapter:



Your intern is told to survey the literature for potential solutions to this computational task. Upon studying the literature, the intern finds three algorithms that solve the problem at hand. The algorithms have the following runtimes: $\Omicron(\lg n)$, $\Omicron(2^n)$, and $\Omicron(n)$. Which algorithm will you choose to be implemented?



Relying only on the mathematical notion of Big-Oh, these three algorithms might all take a logarithmic amount of time to run because $\lg n \in \Omicron(\lg n)$, $\lg n \in \Omicron(n)$, and $\lg n \in \Omicron(2^n)$.

It would be mathematically justified, but very strange (and unhelpful), for the literature which the intern studied to have said that three different logarithmic-time algorithms were in $\Omicron(\lg n)$, $\Omicron(n)$, and $\Omicron(2^n)$, respectively.



**Case in point:** When we use Big-Oh to communicate how fast an algorithm is, we give the smallest Big-Oh time (tightest upper bound) we can prove to be true.





Here is another example that uses Big-Oh to describe upper bounds!





# Big Omega



* Express the mathematical definition of Big Omega.



Big Omega is used to describe asymptotic lower bounds!



$$
T(n) \in \Omega(f(n))\Leftrightarrow \exists \\; c > 0, n_0 > 0 \\; \\; s.t. \\; \\; T(n) \ge c\cdot f(n) \\; \\; \forall \\; n \ge n_0.
$$



Read it as 



$T(n)$ is a member of $\Omega(f(n))$ if and only if there exist positive constants $c$ and $n_0$ such that $T(n)\ge c\cdot f(n)$ for all $n\ge n_0$.



> $\Omega(f(n))$ is set of functions that grow no slower than $f(n)$.

Here is a pictorial illustration of the above definition:





If you want to _prove_ that $T(n) \in \Omega(f(n))$, you need to choose the constants $c$ and $n_0$ so that above definition holds whenever $n \ge n_0$. 

Exercise The running time of an algorithm is $T(n)=7n^2+5$. Show that $T(n) \in \Omega(n^2)$.


Solution

We can choose $c=7$ and $n_0=1$ for the definition of Big Omega to hold.

$$
T(n) = 7n^2 + 5 \ge 7n^2 
$$




Resources

* Khan academy's article on [Big Omega notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation).
* Wikipedia's entry on [Big Omega notation](https://en.wikipedia.org/wiki/Big_O_notation#Big_Omega_notation).




# Exercise V



* Use the mathematical definition of Big Omega to prove the asymptotic running time of a given program.



Consider the following function $T(n)$ describes the precise running time of an algorithm:

$$
T(n) = 3n^2 - 100n + 6
$$



Exercise Show $T(n) \in \Omega(n^2)$.


Solution

We can choose $c=2$ and $n_0=100$ for the definition of Big-Oh to hold.

$$
2n^2 < 3n^2 - 100n + 6 
$$

Similar to the definition of Big-Oh, the choice of $n_0$ and $c$ are not unique.



Exercise Show $T(n) \in \Omega(n)$.


Solution

For any $c$ and $n_0=100c$, the definition of Big-Oh to hold.

$$
cn \le 3n^2 - 100n + 6
$$



Big-Omega expresses a lower bound, but it is not necessarily a tight lower bound.





Exercise Show $T(n) \notin \Omega(n^3)$.


Solution

We can choose $c=1$ and for any $n > 1$ we have

$$
3n^2 - 100n + 6 \le n^3
$$

So $n^3$ is really an upper bound for $T(n)$ and **not** a lower bound.



# Exercise IV



- Recognize that Big-Oh and big Omega are not necessarily tight bounds.
- Recognize growth rate type (upper or lower bound) is different from worst-case vs. best-case analysis.



Exercise Which of the following are correct asymptotic runtime for the binary search algorithm? (There may be more than one correct answer!)

**A)** $\Omicron(n^2)$ \
**B)** $\Omicron(n)$ \
**C)** $\Omicron(\lg n)$ \
**D)** $\Omega(\lg n)$ \
**E)** $\Omega(1)$


Solution

All of the above!

Relying only on the formal definitions of Big-Oh and big Omega, it would be mathematically justified but rather strange (and unhelpful) to use all of the above to describe the asymptotic running time of the binary search algorithm.



When we use Big-Oh/Omega to communicate how fast an algorithm is, we give the tightest upper/lower bound we can prove true.




# Exercise VII



- Recognize that Big-Oh and big Omega are not necessarily tight bounds.
- Recognize growth rate type (upper or lower bound) is different from worst-case vs. best-case analysis.



Exercise Which of the following statements are correct? (There may be more than one correct answer!)

**A)** Big-Oh describes the worst-case, whereas big Omega describes the best-case running time. \
**B)** Big-Oh describes the upper bound, whereas big Omega describes the lower bound on the worst-case running time. \
**C)** Big-Oh and big Omega can describe either upper or lower bound, but typically we use Big-Oh for upper bounds and big Omega for lower bounds. \
**D)** Big-Oh and big Omega can be used to describe best-, average- or worst-case running time.


Solution

The correct answers are (B) and (D)


# Big Theta



* Express the mathematical definition of Big Theta.



Big Theta is used to describe asymptotic tight bound!



$$
T(n) \in \Theta(f(n))\Leftrightarrow \exists \\; c_1 > 0, c_2 > 0, n_0 > 0 
$$

$$
s.t. \\; \\; c_1\cdot f(n) \le T(n) \le c_2\cdot f(n) \\; \\; \forall \\; n \ge n_0
$$



Read it as 



$T(n)$ is a member of $\Theta(f(n))$ if and only if there exist positive constants $c_1$, $c_2$ and $n_0$ such that $c_1\cdot f(n) \le T(n)\le c_2\cdot f(n)$ for all $n\ge n_0$.



> $\Theta(f(n))$ is the set of functions that grow **no faster and no slower** than $f(n)$.

Here is a pictorial illustration of the above definition:





If you want to _show_ that $T(n) \in \Theta(f(n))$, you need to choose the constants $c_1$, $c_2$, and $n_0$ so that above definition holds whenever $n \ge n_0$. 

Exercise The running time of an algorithm is $T(n)=7n^2+5$. Show that $T(n) \in \Theta(n^2)$.


Solution

We can choose $c_1=7$, $c_2=12$, and $n_0=1$ for the definition of Big Theta to hold.

$$
7n^2 \le 7n^2 + 5 \le 12n^2 
$$



Please note, another way of defining Big Theta is as follows:



$T(n) \in \Theta(f(n))$ if and only if $T(n) \in \Omicron(f(n))$ and $T(n) \in \Omega(f(n))$.



Note that Big Theta describes a *tight bound*. So it is a stronger statement than Big-Oh and big Omega.


Resources

* Khan academy's article on [Big Theta notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-theta-notation)
* freeCodeCamp's article [Big Theta and Asymptotic Notation Explained](https://www.freecodecamp.org/news/big-theta-and-asymptotic-notation-explained/)
* [What exactly does big Ө notation represent?](https://stackoverflow.com/questions/10376740/what-exactly-does-big-%D3%A8-notation-represent) on StackOverFlow.




# Exercise IIX



* Use the definition of Big Theta to show the asymptotic running time of a given program.



Consider the following function $T(n)$ describes the precise running time of an algorithm:

$$
T(n) = 3n^2 - 100n + 6
$$



Exercise Show $T(n) \in \Theta(n^2)$.


Solution

We have shown in earlier lessons that $T(n) \in \Omicron(n^2)$ and $T(n) \in \Omega(n^2)$. Therefore, we can conclude $T(n) \in \Theta(n^2)$. 



Exercise Show $T(n) \notin \Theta(n)$.


Solution

We have shown [earlier]() that $T(n) \in Omega(n)$ but $T(n) \notin \Omicron(n)$. Therefore, we can conclude $T(n) \notin \Theta(n)$. 




Exercise Show $T(n) \notin \Theta(n^3)$.


Solution

We have shown [earlier](/steps/3/) lessons that $T(n) \in \Omicron(n^3)$ but $T(n) \notin \Omega(n^3)$. Therefore, we can conclude $T(n) \notin \Theta(n^3)$. 



# Summary



* Elaborate on the benefits of using asymptotic notation and worst-case analysis to study the computational complexity of algorithms.






The Big-Oh notation and worst-case analysis are tools that greatly simplify our ability to compare the efficiency of algorithms.



* Asymptotic analysis means using asymptotic notation to describe the computational complexity of an algorithm. 
* Computational complexity generally means time complexity and space complexity.
* If it is not specified, we mean time complexity.
* If it is not specified, we mean worst-case analysis.
* If it is not specified, we want Big-Oh notation.
* When using Big-Oh, give the tightest upper bound you can find.
* When using big Omega, give the tightest lower bound you can find.
* If it is not specified, space complexity should include input and auxiliary space.

| Notation   | Definition  |
| :--------: | :---------- |
| $\Omicron$ | **Big-Oh**: $T(n)$ is a member of $\Omicron(f(n))$ if and only if there exist positive constants $c$ and $n_0$ such that $T(n)\le c f(n)$ for all $n\ge n_0$. |
| $\Omega$ | **Big Omega**: $T(n)$ is a member of $\Omega(f(n))$ if and only if there exist positive constants $c$ and $n_0$ such that $T(n)\ge c f(n)$ for all $n\ge n_0$. |
| $\Theta$ | **Big Theta**: $T(n)$ is a member of $\Theta(f(n))$ if and only if there exist positive constants $c_1$, $c_2$ and $n_0$ such that $c_1 f(n) \le T(n) \le c_2 f(n)$ for $n\ge n_0$. |

The following illustration is taken from the bible of Algorithm, the [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition) book:





There are other asymptotic notations (Little Oh, Little Omega, etc.) which I have spared you the grief of knowing!



Resources

For a brief & concise overview, refer to any of the following:

* Educative's short article on [Time complexity vs space complexity](https://www.educative.io/edpresso/time-complexity-vs-space-complexity)
* InterviewCake's article [Big-O notation](https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity)
* HackerEarch's tutorial [Time and Space Complexity](https://www.hackerearth.com/practice/basic-programming/complexity-analysis/time-and-space-complexity/tutorial/)
* Cornell's [cs3110 online lecture notes](https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec19-asymp/review.html)
* MIT's [16.070 online lecture notes](https://web.mit.edu/16.070/www/lecture/big_o.pdf)
* [Big-O notation in 5 minutes — The basics](https://youtu.be/__vX2sjlpXU) on YouTube.
* [Asymptotic Bounding 101: Big O, Big Omega, & Theta (Deeply Understanding Asymptotic Analysis](https://youtu.be/0oDAlMwTrLo) on YouTube.
* Online notes on [Analysis of Algorithms](https://algs4.cs.princeton.edu/14analysis/) accompanying Robert Sedgewick's book "Algorithms."
* TowardsDataScience's article [The Math Behind "Big O" and Other Asymptotic Notations](https://towardsdatascience.com/the-math-behind-big-o-and-other-asymptotic-notations-64487889f33f)

For an in-depth, detailed, formal (mathematical) discussion of asymptotic analysis, refer to chapter 3, "Growth of Functions," in [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition): **Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein, Introduction to Algorithms. 3rd ed, MIT Press, 2009.**






# Quadratic Sorts

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- Describe **Selection**, **Insertion**, and **Bubble** sorts at high level.
- Trace each sorting algorithm on a given sequence of data.
- Understand each sorting algorithm well enough to implement them.
- Recognize the effect of applying a particular sorting strategy on a data sequence.
- Work out the asymptotic complexity of each sorting algorithm.
- Identify the number of **comparisons** and **swaps** for each of these algorithms in the worst case, based on the data size.
- Understand the use and operation of Java's **Comparable** interface.

> [Starter code](../../zip/chap11-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap11-solution.zip) for this chapter.


# Selection Sort: The Code



- Understand the implementation of selection sort.
- Understand the use and operations of the SortingAlgorithm interface.
- Understand the use and operation of Java's Comparable interface.




Please download the starter code for this chapter. 

Here is how we have implemented selection sort:

```java
/**
 * The basic Selection Sort algorithm.
 *
 * @param  Element type.
 */
public final class SelectionSort>
    implements SortingAlgorithm {

  // is a less than b?
  private boolean less(T a, T b) {
    return a.compareTo(b) < 0;
  }

  @Override
  public void sort(IndexedList indexedList) {
    // We try to put "correct" values into a[0], a[1], ... a[n-2];
    // once a "correct" value is in a[n-2], the very last value
    // has to be the largest one anyway; thus it's also "correct".
    for (int i = 0; i < indexedList.length() - 1; i++) {
      // We're trying to put the "correct" element in a[i].
      // We need to find the smallest element in a[i..n-1].
      // We start by assuming a[i] is the smallest one.
      int min = i;
      // Now we try to find a smaller one in a[i+1..n-1].
      for (int j = i + 1; j < indexedList.length(); j++) {
        if (this.less(indexedList.get(j), indexedList.get(min))) {
          min = j;
        }
      }
      // Now we have the "true" minimum at a[min], and we
      // swap it with a[i], unless i == min of course.
      if (min != i) {
        T t = indexedList.get(i);
        indexedList.put(i, indexedList.get(min));
        indexedList.put(min, t);
      }
    }
  }

  @Override
  public String name() {
    return "Selection Sort";
  }
}
```


Frequently asked questions!

**Q:** Why `SelectionSort` implements `SortingAlgorithm`?


Answer

`SortingAlgorithm` is an interface we provided which encapsulates the essence of a sorting algorithm:

```java
public interface SortingAlgorithm> {
  void sort(IndexedList list);
  String name();
}
```
Given an object of `SortingAlgorithm`, we can (a) ask it to sort a given `IndexedList` and (b) ask it for its name (e.g., "Selection Sort").



**Q:** Why the generic parameter `T` extends `Comparable`?


Answer

Things to be sorted need an *order*, and we need a way to compare items, not just for equality (there's an `equals()` method on all objects) but for less-than and greater-than as well.

Therefore, we need a **bounded** type parameter. The elements to be sorted (of type `T`) must be `Comparable`. That, in turn, ensures that we have a method `compareTo()` that we can use to establish order. 



**Q:** What is `Comparable`?


Answer

It is an interface in Java.  

```java
public interface Comparable {
  /**
  * Compares this object with the specified object for order.
  *
  * @param  o the object to be compared.
  * @return a negative integer, zero, or a positive integer 
  *         as this object is less than, equal to, or greater 
  *         than the specified object.
  */
  int compareTo(T o);
}
```

* Most Java built-in types implement `Comparable` (e.g. `String`, `Integer`, etc.)

* We implement this interface in many classes, particularly if we want to sort a collection of their objects.

See [Comparable](https://docs.oracle.com/javase/8/docs/api/java/lang/Comparable.html) on Oracle's website for more.



**Q:** Why is there a `less` method?


Answer

The `less` method encapsulates the notion of comparing two elements: "is `a` less than `b`?". It simply makes for a more readable code that is easier to update if the notion of "$<$" changes.  




# Selection Sort: Tracing Exercise



- Trace selection sort algorithm on a given sequence of data.
- Understand selection sort well enough to implement it.



Please run the selection sort code in "debug" mode for the following sequence of values:

$$
14, 10, 23, 34, 6, 17, 50, 14
$$

You can use the unit test in `SortingAlgorithmTest` for this purpose. 

Exercise complete the following trace table.






Making sense of the table!

Each row of the table above represents a complete inner pass through the selection sort algorithm. Two full passes have been filled as an example of the sort. The arrow in each row indicates where the smallest element in the unsorted part should go.




Solution







# Selection Sort: The Algorithm



- Describe selection sort at a high level.
- Work out the asymptotic complexity of selection sort.
- Identify the number of comparisons and swaps selection sort takes in the worst case, based on the data size.




Now that you've seen the code and have done a tracing activity, please work out the following exercises. 

Exercise Explain the selection sort algorithm (at high-level). Supply your explanation with a concise pseudocode. 


Demo

The following slides assist in building an intuition for the answer:






Solution

The idea of Selection Sort is that we repeatedly find the smallest element in the list and bring it to the left side.

```js
for i gets the values from 0 to length-2
  min = i
  for j gets the values from i+1 to length-1
    if val[j] < val[min]
      min = j
  swap val[min] and val[i] 
  // at this point, all elements with an index i or smaller are sorted.
```

* In the outer loop of the code, there would have to be a variable keeping track of the index with the smallest element in the unsorted part of the array. 
  
* To find the actual smallest element's index, the inner loop would have to update this variable as it loops through all the elements in the unsorted part of the array.
   
* Then, a swap between the front of the unsorted part of the array and the smallest element would occur as the last line in the outer loop.
 
* This pattern continues as the unsorted part of the array becomes smaller and smaller.



Exercise Analyze the running time of the  selection sort algorithm. In particular, think about where comparisons and swaps are being made and how many of them occur in each pass through the collection to be sorted. 


Solution

The algorithm has a quadratic running time, i.e., $\Omicron(n^2)$.

Looking at the pseudocode (previous exercise):

* Each inner pass compares each element in the unsorted part of the array with the smallest element. 
  
* There is only one swap to place the smallest unsorted element in the correct position. 
  
* In the worst case, when one has an array in descending order, there is no change to the number of comparisons and swaps. However, the reference to the smallest unsorted element constantly changes. 
  
* There would then be $(n-1) + (n-2) + ... 1 = \frac{n(n-1)}{2}$ comparisons, and $1 \times n = n$ swaps, where $n$ is the number of elements; therefore, the time complexity of the worst case is $\Omicron(n^2)$, due to the number of comparisons.

Selection sort requires $\Omicron(n)$ space: $\Omicron(n)$ input and $\Omicron(1)$ auxiliary space.




Resources

* Wikipedia's entry on [Selection sort](https://en.wikipedia.org/wiki/Selection_sort).
* Programiz article on the [Selection Sort Algorithm](https://www.programiz.com/dsa/selection-sort); pictorial examples & implementation in multiple programming languages.
* HackerEarth's [Selection Sort Tutorial](https://www.hackerearth.com/practice/algorithms/sorting/selection-sort/tutorial/) with visualizer and coding questions.
* InterviewBit's [Selection Sort Tutorial](https://www.interviewbit.com/tutorial/selection-sort/) with pictorial examples and a video explanation.
* Toptal's page on [Selection Sort](https://www.toptal.com/developers/sorting-algorithms/selection-sort) with animation, code, analysis, and discussion.



# Insertion Sort: The Algorithm



- Describe insertion sort at a high level.
- Trace insertion sort algorithm on a given sequence of data.
- Understand insertion sort well enough to implement it.
- Work out the asymptotic complexity of insertion sort.
- Identify the number of comparisons and swaps insertion sort takes in the worst case, based on the data size.



You are asked, as part of homework-3, to implement the Insertion Sort algorithm. This lesson helps you to understand the insertion sort process.



In Insertion Sort, we divide the list into two parts, a sorted and an unsorted part. 
At each iteration, insertion sort removes one element from the unsorted part, finds its location within the sorted part, and inserts it there. 
It repeats this process until no elements remain in the unsorted part.





Demo

The following slides assist in building an intuition for the insertion sort:





Here is the process described in pseudocode. 

```js
for i gets values from 1 to length-1
  j gets i
  while j > 0 and val[j] < val[j-1]
    swap val[j] and val[j-1]
    decrement j by 1
  // at this point, all elements to the left of index i are sorted.
```


Imagine running the insertion sort algorithm on the following sequence of values:

$$
14, 10, 23, 34, 6, 17, 50, 14
$$

Exercise Complete the following trace table.






Making sense of the table!

Each row of the table above represents a complete inner pass through the insertion sort algorithm. Two full passes have been filled as an example of the sort. You are responsible for manually performing the insertion sort algorithm on the remaining passes through the data. The upwards arrow in each row indicates which element needs to be partially sorted next (in other words, sorted concerning all elements to its left). 




Solution







Exercise Analyze the running time of the insertion sort algorithm. In particular, think about where comparisons and swaps are being made and how many of them occur in each pass through the collection to be sorted. 


Solution

The algorithm has a quadratic running time, i.e., $\Omicron(n^2)$.

Looking at the pseudocode:

* Each inner pass has a comparison and swap for as many positions as it needs to move the next element into its sorted spot, which at max is the number of elements in the partially sorted part of the array. 
  
* In the worst case, when one has an array in descending order, each next element would need to be moved to the beginning of the array.

* There would then be $1 + 2 + \dots + (n-1) = \frac{n(n-1)}{2}$ comparisons and swaps, where $n$ is the number of elements; the time complexity of the worst case is $\Omicron(n^2)$.

Insertion sort requires $\Omicron(n)$ space: $\Omicron(n)$ input and $\Omicron(1)$ auxiliary space.




Resources

* Wikipedia's entry on [Insertion sort](https://en.wikipedia.org/wiki/Insertion_sort).
* Geeks for geeks's article on the [Insertion Sort Algorithm](https://www.geeksforgeeks.org/insertion-sort/); pictorial examples & implementation in multiple programming languages.
* HackerEarth's [Insertion Sort Tutorial](https://www.hackerearth.com/practice/algorithms/sorting/insertion-sort/tutorial/) with a visualizer.
* Khan Academy's article on [Insertion Sort](https://www.khanacademy.org/computing/computer-science/algorithms/insertion-sort/a/insertion-sort).
* Toptal's page on [Insertion Sort](https://www.toptal.com/developers/sorting-algorithms/insertion-sort) with animation, code, analysis, and discussion.


# Bubble Sort: The Code



- Understand the implementation of bubble sort.
- Explain the "stop early if no swaps" optimization for bubble sort.



Here is how we have implemented bubble sort:

```java
/**
 * The Bubble Sort algorithm with the optimized "quick" break to exit
 * if the array is sorted.
 *
 * @param  The type being sorted.
 */
public final class BubbleSort>
    implements SortingAlgorithm {

  // is a less than b?
  private boolean less(T a, T b) {
    return a.compareTo(b) < 0;
  }

  @Override
  public void sort(IndexedList indexedList) {
    boolean swapped;
    for (int i = indexedList.length() - 1; i > 0; i--) {
      swapped = false;
      for (int j = 0; j < i; j++) {
        if (less(indexedList.get(j + 1), indexedList.get(j))) {
          swap(indexedList, j, j + 1);
          swapped = true;
        }
      }
      if (!swapped) {
        return;
      }
    }
  }

  // Pre: i & j are valid indices.
  // Post: elements at i & j are swapped.
  private void swap(IndexedList indexedList, int i, int j) {
    T t = indexedList.get(i);
    indexedList.put(i, indexedList.get(j));
    indexedList.put(j, t);
  }

  @Override
  public String name() {
    return "Bubble Sort";
  }
}
```

Please run the bubble sort code in "debug" mode for the following sequence of values:

$$
14, 10, 23, 34, 6, 17, 50, 14
$$

You can use the unit test in `SortingAlgorithmTest` for this purpose. 

Exercise Complete the following trace table. As you fill out the table, think about the role of the boolean variable `swapped` as it is used in the code above.






Making sense of the table!

Each row of the table above represents a complete inner pass through the bubble sort algorithm. Two full passes have been filled as an example of the sort. You are responsible for manually performing the bubble sort algorithm on the remaining passes through the data. The top-left portion of some boxes represents an intermediate value held at that position, and the bottom-right portions represent the final value held at that position for that particular row/inner pass. 




Solution





The `swapped` variable would allow us to **stop early** if no swaps were made in a full pass of the outer loop. When no swap is made in a complete pass, the items are already in sorted order.  We call this the "stop early if no swaps" optimization!



# Bubble Sort: The Algorithm



- Describe bubble sort at a high level.
- Work out the asymptotic complexity of bubble sort.
- Identify the number of comparisons and swaps bubble sort takes in the worst case, based on the data size.



Now that you've seen the code and have done a tracing activity, please work out the following exercises. 

Exercise Explain the bubble sort algorithm (at high-level). Supply your explanation with a concise pseudocode. 


Demo

The following slides assist in building an intuition for the answer:






Solution

Bubble Sort works on the idea of repeatedly stepping through the list, comparing adjacent elements and swapping them if they are in the wrong order. The pass through the list is repeated until the list is sorted. In each iteration, the highest unsorted element "bubbles up" to its correct position, hence the name "Bubble Sort."

```js
for i gets the values from last index to 1
  for j gets the values from 0 to i-1
    if val[j] > val[j+1]
      swap val[j] & val[j+1]
```

* The pseudocode does not have the "stop early if no swaps" optimization.



Exercise Analyze the running time of the bubble sort algorithm. In particular, think about where comparisons and swaps are being made and how many of them occur in each pass through the collection to be sorted. 



Solution

The algorithm has a quadratic running time, i.e., $\Omicron(n^2)$.

Looking at the pseudocode:

* Each inner pass has a comparison for each neighboring pair in the unsorted part of the array. Thus, swaps occur any time a neighboring pair is "out of order." 

* In the worst case, every comparison would result in a swap when one has an array in descending order. There would then be $(n-1) + (n-2) + \dots + 1 = \frac{n(n-1)}{2}$ comparisons and swaps, where $n$ is the number of elements; the time complexity of the worst case is $\Omicron(n^2)$.

Bubble sort requires $\Omicron(n)$ space: $\Omicron(n)$ input and $\Omicron(1)$ auxiliary space.




Resources

* Wikipedia's entry on [Bubble sort](https://en.wikipedia.org/wiki/Bubble_sort).
* Brilliant's article on [Bubble Sort](https://brilliant.org/wiki/bubble-sort/) with detailed complexity analysis.
* HackerEarth's [Bubble Sort Tutorial](https://www.hackerearth.com/practice/algorithms/sorting/bubble-sort/tutorial/) with a visualizer.
* Toptal's page on [Bubble Sort](https://www.toptal.com/developers/sorting-algorithms/bubble-sort) with animation, code, analysis, and discussion.


# Summary



* Recognize the effect of applying a particular sorting strategy on a data sequence.



Efficiency for all three is $\Omicron(n^2)$, hence "quadratic sorting algorithms."

| Sort | Elevator pitch! |
| :---- | :--------------- |
| Bubble | compare adjacent values, swap if out of order |
| Selection | find next smallest, swap into final position |
| Insertion | consider each element, move to left as far as it needs to go |


Exercise Suppose we have the following array contents after the **third pass** of the outer loop of some quadratic sorting algorithms meant to put the array in ascending order:

$$
3, 5, 7, 8, 2, 9, 4, 10, 15, 20
$$

Which sorting algorithm could be operating on this array? 

A) bubble (up) sort \
B) (min) selection sort \
C) insertion sort \
D) none of these


Solution

The answer is (A) & (C).

The last three elements of the array are the three largest elements. Therefore, it could be the bubble sort operating on it.

The first three elements are not the three smallest elements. Thus, it could not be the selection sort. 

The first three elements are in order. Hence, it could result from the insertion sort. (If you implement the insertion sort as to start from the second element, it can still be argued the first four elements are in sorted order).



Exercise Recursivley implement bubble, selection, and insertion sorts! 


Solution
This exercise is left for you as an unsolved activity!




Resources

There are great demos on the [VisuAlgo](https://visualgo.net/) website. In particular to show sorting algorithms:

  1. Go to https://visualgo.net/en/sorting
  2. Press Esc to get rid of the popup (if you log in, then it will not show up)
  3. Choose the type of sort in the top bar (Bubble, Sel, Ins) 
  4. On the bottom left, press create and add the data set (here, you can choose if you want it in random, sorted, or nearly sorted order)
  5. In the bottom left corner, you can adjust the speed by moving the bar
  6. Press sort and then Go (no need to check the box for computing inversion index)



# Stack & Amortized Analysis

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the **core operations of Stack** (push, pop, top, empty).
* Describe the difference between `top` and `pop`.
* Implement the core operations of Stack efficiently (array-based and linked-based).
* Implement an array-based Stack that **dynamically grows** as more space is needed.  
* Determine the cost of dynamically resizing an array-based implementation of Stack.
* Describe what **amortized analysis** is.
* Explain the difference between amortized constant time and actual constant time.
* Elaborate on the importance of array **growth factor** to maintain amortized constant time push operation.


> [Starter code](../../zip/chap12-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap12-solution.zip) for this chapter.


# Limited Access Data Structures



* Explain what is meant by the limited-access data structure.



An *array* is a **random-access** data structure, where each element can be accessed directly and in constant time. Random access is critical to many algorithms, for example, to binary search.

* A typical illustration of random access is a book — each book page can be reached independently of others. 

A *linked list* is a **sequential-access** data structure, where each element can be accessed only by traversing the list. 

* A typical illustration of sequential access is a roll of paper or tape — all prior material must be unrolled to get to the data you want.



A *stack* and a *queue* are examples of _restricted_ or **limited-access** data structures. You can only access (add, read, and delete) the element at the front or back position (or both). Access to other parts is forbidden.



By limiting operations, we can create very efficient implementations.




# Stack Abstract Data Type



* Describe the Stack ADT.



A Stack ADT supports two main operations:

* **Push** which adds an element to the data structure.
* **Pop** which removes the most recently added element that was not yet removed.

The order in which elements are removed gives rise to the term **LIFO** (last in, first out) to describe a stack.
 




It helps to think of a stack as an ADT where operations are done on one end of the collection and access to all other positions is restricted.

A stack has a variety of applications such as:

* Function call stack in program execution.
* Reversing the order of elements.
* Undo mechanism (e.g., undo functionality in text editors).
* Evaluating/parsing expressions (e.g., reverse polish notations).


Resources

* Wikipedia entry on [Stack](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)).
* CS50 YouTube video on [Stack](https://youtu.be/hVsNqhEthOk).
* Paul Programming YouTube video [What is a Stack Data Structure - An Introduction to Stacks](https://youtu.be/FNZ5o9S9prU).
* [VisuAlgo visualization and interactive demo of lists](https://visualgo.net/en/list).



# Stack Interface



- Identify the operation of Stack ADT.
- Describe the difference between top and pop.



Here is the `Stack` interface which we use in this course:

```java
/**
 * Stack ADT.
 *
 * @param  base type.
 */
public interface Stack {

  /**
   * Checks if empty.
   *
   * @return true if this stack is empty and false otherwise.
   */
  boolean empty();

  /**
   * Peeks at top value without removing it.
   *
   * @return the value at the top of this stack.
   * @throws EmptyException when empty() == true.
   */
  T top() throws EmptyException;

  /**
   * Removes the top element.
   *
   * @throws EmptyException when empty() == true.
   */
  void pop() throws EmptyException;

  /**
   * Adds a new element to top of stack.
   *
   * @param t value to be added to the top of this stack.
   *          Post: top() == t
   */
  void push(T t);
}
```

Make a note of `pop` and `top`:

* `pop` removes the top element but does not return it.
* `top` returns the top element but does not remove it.
# Linked Implementation of Stack



* Implement the core operations of stack efficiently (linked base).



A singly linked list referenced by a `head` pointer naturally lends itself to the implementation of the Stack ADT.

Exercise Think about how to implement a stack using a singly linked list 
such that the core operations are **constant time**. 

| Operation | How? | Time    |
| --------- | ---- | :-------: |
| `push`    |      |  $\Omicron(1)$ |
| `pop`     |      |  $\Omicron(1)$ |
| `top`     |      |  $\Omicron(1)$ |
| `empty`   |      |  $\Omicron(1)$ |


Solution

The top of the stack would be the HEAD node of the singly linked list. This
works because every time we want to `push` an additional value to the stack,
we would create a new node and set it as the new HEAD node (with its `.next`
pointing to the original HEAD node, or NULL if the stack was empty). Then when
we want to `pop` a value from the top of the stack, we set the
HEAD to the current HEAD node's `.next`. As such, the HEAD node will always be
at the top of the stack, and calling `top` would return the value of the HEAD
node.

| Operation | How?                                     | Time    |
| --------- | ---------------------------------------- | :-----: |
| `push`    |  prepend the list and update the `head`  |  $\Omicron(1)$ |
| `pop`     |  delete from front: `head = head.next`   |  $\Omicron(1)$ |
| `top`     |  return `head.data`                      |  $\Omicron(1)$ |
| `empty`   |  check if `head` is `null`               |  $\Omicron(1)$ |


# Trace Linked Implementation



* Trace the core operations of stack (linked implementation).



Think about a linked implementation of Stack such that the core operations are **constant time**.

Exercise Complete the table below: update the linked list as you trace the operations; show value returned if any.

| Operation    | Linked List            | Value Returned |
| :----------- | :--------------------- | :------------- |
| `push(6)`    | `HEAD -> (6)`          |                |
| `push(10)`   | `HEAD -> (10) -> (6)`  |                |
| `push(2)`    |                        |                |
| `top()`      |                        |                |
| `pop()`      |                        |                |
| `push(15)`   |                        |                | 
| `pop()`      |                        |                | 
| `pop()`      |                        |                |
| `push(5)`    |                        |                |
| `top()`      |                        |                |



Solution

| Operation   | Linked List                   | Value Returned |
| :----------- | :--------------------------- | :------------- |
| `push(6)`   | `HEAD -> (6)`                 |                |
| `push(10)`  | `HEAD -> (10) -> (6)`         |                |
| `push(2)`   | `HEAD -> (2) -> (10) -> (6)`  |                |
| `top()`     | `HEAD -> (2) -> (10) -> (6)`  |  $2$           |
| `pop()`     | `HEAD -> (10) -> (6)`         |                |
| `push(15)`  | `HEAD -> (15) -> (10) -> (6)` |                |
| `pop()`     | `HEAD -> (10) -> (6)`         |                |
| `pop()`     | `HEAD -> (6)`                 |                |
| `push(5)`   | `HEAD -> (5) -> (6)`          |                |
| `top()`     | `HEAD -> (5) -> (6)`          |  $5$           |




Resources

* USFCA interactive demo of [Stack (Linked List Implementation)](https://www.cs.usfca.edu/~galles/visualization/StackLL.html).



# LinkedStack



* Implement the core operations of Stack efficiently (linked base).



Exercise Open the starter code and complete the implementation of `LinkedStack`.


Solution

Please check the posted solution. 



# Array Implementation of Stack



* Implement the core operations of stack efficiently (array-based).



We want to implement the `Stack` interface using an array as an internal data storage. 

Exercise Think about how to implement a stack using an array such that the core operations are **constant time**. Assume the array is sufficiently large.

| Operation | How? | Time    |
| --------- | ---- | :-----: |
| `push`    |      |  $\Omicron(1)$ |
| `pop`     |      |  $\Omicron(1)$ |
| `top`     |      |  $\Omicron(1)$ |
| `empty`   |      |  $\Omicron(1)$ |


Solution

Consider the "top" of the stack as the "end" of the array. Thus, elements can be added at the end of the collection in constant time.

Assume there is a variable `numElement`. It is initialized to $0$ when the stack is constructed (and it is therefore empty). The `numElement` is incremented/decremented as data is pushed to/popped from the stack. Therefore, the `numElement` continuously points to the next empty slot in the array. Thus, we can use it to index into the collection.

| Operation | How?                                 | Time   |
| --------- | ------------------------------------ | :----: |
| `push`    |  add to end: `arr[numElement++]`     | $\Omicron(1)$ |
| `pop`     |  delete from end: `numElement--`     | $\Omicron(1)$ |
| `top`     |  return last:  `arr[numElement - 1]` | $\Omicron(1)$ |
| `empty`   |  check if `numElement == 0`          | $\Omicron(1)$ |


# Trace Array Implementation



* Trace the core operations of Stack (array-based implementation).



Think about an array-based implementation of Stack such that the core operations are **constant time**.

Exercise Complete the table below: update the array values at indices `0`, `1`, `2`, `3`, and `4` as you trace the operations; show value returned if any.

| Operation  | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | Return Value  |
| :--------- | :---: | :---: | :---: | :---: | :---: | :-----------: |
| `push(6)`  |       |       |       |       |       |               |
| `push(10)` |       |       |       |       |       |               |
| `push(2)`  |       |       |       |       |       |               |
| `top()`    |       |       |       |       |       |               |
| `pop()`    |       |       |       |       |       |               |
| `push(15)` |       |       |       |       |       |               |
| `pop()`    |       |       |       |       |       |               |
| `pop()`    |       |       |       |       |       |               |
| `push(5)`  |       |       |       |       |       |               |
| `top()`    |       |       |       |       |       |               |


Solution

| Operation  | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | Return Value  |
| :--------- | :---: | :---: | :---: | :---: | :---: | :-----------: |
| `push(6)`  |   6   |       |       |       |       |               |
| `push(10)` |   6   |   10  |       |       |       |               |
| `push(2)`  |   6   |   10  |   2   |       |       |               |
| `top()`    |   6   |   10  |   2   |       |       |       2       |
| `pop()`    |   6   |   10  |       |       |       |               |
| `push(15)` |   6   |   10  |   15  |       |       |               |
| `pop()`    |   6   |   10  |       |       |       |               |
| `pop()`    |   6   |       |       |       |       |               |
| `push(5)`  |   6   |   5   |       |       |       |               |
| `top()`    |   6   |   5   |       |       |       |       5       |




Resources

* USFCA interactive demo of [Stack (Array Implementation)](https://www.cs.usfca.edu/~galles/visualization/StackArray.html).


# ArrayStack



* Implement the core operations of Stack efficiently (array-based).



Exercise Open the starter code and complete the implementation of `ArrayStack`. 

Notice the constructor of `ArrayStack` does not take in a parameter for the size of the array. So, for now, initialize it to an arbitrary chosen (and sufficiently large) capacity. 


Solution

Please check the posted solution. 


# Dynamic ArrayStack



* Implement an array-based Stack that dynamically grows as more space is needed.



Array implementation of Stack can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. Then, elements can be pushed/popped at the end of the array in constant time until this space is entirely consumed. 

What should happen then?

* Well, when the Stack is _full_, and a client invokes "push," we can throw an exception to signal there is no space left.

* Alternatively, we can _grow_ the underlying array. We can allocate a new underlying array and copy each element from the original array. 


Consider the following implementation of `push`. Assume the array `data` has the `capacity` of 10 elements.

```java
@Override
public void push(T value) {
  data[numElements++] = value;
  if (numElements == capacity) {
    grow();
  }
}
```

In the code above, `numElement` is the logical size of the array, whereas the `capacity` is its actual physical size. 

Exercise Complete the implementation of the helper method `grow`. The new capacity must be double the old one.

```java
private void grow() {
  // TODO: Implement me
}
```


Solution

In the following, we double the capacity each time growing the array.

```java
private void grow() {
  capacity *= 2; 
  T[] tmp = (T[]) new Object[capacity];
  for (int i = 0; i < numElements; i++) {
    tmp[i] = data[i];
  }
  data = tmp;
}
```




Exercise What is the complexity of the `grow()` operation?


Solution

`grow()` runtime (and required auxiliary space) is $\Omicron(n)$ where $n$ is the number of the elements (stored in the Stack).



**Aside:** In the implementation of `push`, I have _preemptively_ grew the array when increasing the capacity was not needed yet (not until the next `push`). However, it can be argued that a better approach is to grow the array _reactivity_ when a `push` is requested, and the capacity is full.



Resources

* Wikipedia's entry on [Dynamic Array](https://en.wikipedia.org/wiki/Dynamic_array).
* InterviewCake entry on [Dynamic Array](https://www.interviewcake.com/concept/java/dynamic-array).




# Cost of Resizing



* Determine the cost of dynamically resizing an array-based implementation of a stack.



Here is the implementation of `push`. 

```java
@Override
public void push(T value) {
  data[numElements++] = value;
  if (numElements == capacity) {
    grow();
  }
}
```

We know `grow()` is $\Omicron(n)$.

Exercise What is the running time of `push`?


Solution

The worst-case asymptotic running time of `push` is also $\Omicron(n)$.




Consider the `data` array is constructed initially with the capacity of $n$. We then perform $n+1$ "push" operation one after another. 

Exercise What is the worst-case running time of `push` **per operation** (rather than *per algorithm*). 


Hint

We know the `grow` operation will only be called for the $n^{th}$ push. Therefore,  
* the first time we call `push`, its cost is really $\Omicron(1)$, 
* the second time we call `push` its cost is $\dots$ 
* $\dots$
* the $n^{th}$ time we call `push` $\dots$
* $\dots$




Solution

The cost of `push` is $\Omicron(1)$ for the first $n - 1$ pushes. Then, for the $n^{th}$ push, we must grow the array, and so it will cost $\Omicron(n)$. After that, the $n+1$ push is again $\Omicron(1)$.



# Amortized Analysis



* Describe what amortized analysis is.



So far we analyzed the (worst-case) cost of each operation. What about (worst-case) cost of sequence of operations?

In the study of data structures, it is common to perform a sequence of operations to solve a computational task. Therefore, we may be interested to analyze the running time over a sequence of operations. For example, consider the Roster class from earlier chapters. We analyzed the cost of `find` when implemented as linear or binary search. However, we may be interested to analyze the running time when we perform a sequence of `add`s and `find`s. 

We use **amortized analysis** to analyze the cost of a sequence of operations!



The amortized cost of each operation in a sequence of $n$ operations is the total cost divided by $n$.



You might have heard the term "amortized cost" in Finance or Accounting. In that context, it means to gradually write off the initial cost of an asset over a period of time. However, here, it means the _average cost_ per operation of a sequence of operations. Here, we mean "averaged" over the number of operations (or the number of times an operation is invoked). Please do not confuse amortized with _average case analysis_. The latter provides the _expected_ runtime when the input is _randomly_ drawn from a distribution *assumed* to represent the typical inputs to the algorithm.



For example, suppose we perform $10$ operations of cost $1$ and then $1$ operation of cost $10$. The amortized cost for this sequence of operations would be $20/11 \approx 2$.

We can also perform the same operation several times and calculate the amortized cost per operation! 

Exercise In a stack of size $n$, we run `push` $n - 1$ times where each run involves $\Omicron(1)$ work, followed by another run that involves $\Omicron(n)$ steps (due to dynamically growing the underlying array). What is the amortized cost of `push`?. 


Solution

$$
\frac{(n - 1) \times \Omicron(1) + \Omicron(n)}{n} = \frac{2 \times \Omicron(n)}{n} \in \Omicron(1)
$$



More generally, the amortized cost per operation for running an operation $n$ times is the total cost divided by $n$. For example, if we run an operation $n - 1$ times where each run involves $\Omicron(1)$ work, followed by another run of the same operation that involves $\Omicron(n)$ steps (due to some worst-case condition being met), the amortized cost of this operation is $\Omicron(1)$.

Note the normal worst-case analysis for the operation (example above) would yield $\Omicron(n)$ (which is too *pessimistic*), but amortized (worst-case) analysis gives $\Omicron(1)$ (more *realistic*).



The motivation for amortized analysis is that looking at the (worst-case) runtime per operation rather than sequence of operations can be too pessimistic.



Amortized cost analysis is helpful because core operations of some data structures occasionally incur a significant cost as they rebalance or improve the data structures' internal state. Those expensive processes, however, do not occur too frequently. Therefore, the amortized analysis yields an asymptotic bound closer to the actual cost of using the data structure than a standard worst-case bound. 


Resources

* Wikipedia's entry on [Amortized Analysis](https://en.wikipedia.org/wiki/Amortized_analysis).
* Brilliant's article on [Amortized Analysis](https://brilliant.org/wiki/amortized-analysis/).
* Cornell's CS3110 online lecture notes on [Amortized Analysis](https://www.cs.cornell.edu/courses/cs3110/2011sp/Lectures/lec20-amortized/amortized.htm).
* [What is amortized analysis of algorithms?](https://stackoverflow.com/questions/11102585/what-is-amortized-analysis-of-algorithms) on StackOverflow.
* [Difference between average case and amortized analysis](https://stackoverflow.com/questions/7333376/difference-between-average-case-and-amortized-analysis) on StackOverflow.



# Growth Factor



* Elaborate on the importance of array growth factor to maintain amortized constant time push operation.



Consider a dynamic array-based implementation of Stack. The amortized cost of `push` depends on the _growth factor_ employed to expand (resize) the array.

Exercise If the array size starts at $1$, how expensive will it be to grow to $1$ million if we grow the array one element at a time?


Solution

When we call `grow` in the `push` method, if we grow the array by one (or few) elements, then the number of times we call "grow" **linearly increases** with the number of times we call "push." 





The function `grow` itself is a linear-time operation. So we have a situation that looks like $\Omicron(n) \times \Omicron(n)$, which gives us $\Omicron(n^2)$ quadratic runtime for $n$ "push" operations.

Another way to see this is that for one million push, the (computational) work performed by the "grow" operation (in total) will be as follows.

$$
1 + 2 + \dots + 999999 = 499999500000 \approxeq \text{1 billion}
$$ 

The above shows the $\Omicron(n^2)$ total runtime for $n$ "push" operations. The **amortized cost** of `push` will then be $\frac{\Omicron(n^2)}{n}=\Omicron(n)$.



Exercise If the array size starts at $1$, how expensive will it be to grow to $1$ million if we double the size of the array each time we reach its capacity?


Solution

If we grow the array by doubling its size, the number of times we call `grow` **logarithmically increases** with the number of pushes. 





Let's say we start with an array of 1 element, and then we do $n$ push. The total work done is as follows. 

$$
1 + 2 + 4 + 8 + \dots + 2^{\lg n} = 2^{(\lg n) + 1} - 1 
$$

The total above is calculated by [adding up all the powers of two](https://en.wikipedia.org/wiki/1_%2B_2_%2B_4_%2B_8_%2B_%E2%8B%AF).

Note that $2^{\lg n} = n$ (recall $\lg n$ is $\log_{2} n$. Moreover, look at rule #7 of [logarithm rules](https://www.chilimath.com/lessons/advanced-algebra/logarithm-rules/)). 

So we have the following:

$$
2^{(\lg n) + 1} - 1 = \left ( 2^{(\lg n)} \times 2 \right ) - 1 = 2n - 1 \in \Omicron(n)
$$

Thus the total runtime is $\Omicron(n)$ for $n$ "push" operations. The **amortized cost** of `push` will then be $\frac{\Omicron(n)}{n}=\Omicron(1)$.



Dynamic arrays resize by a multiplicative factor, such as doubling in size, to avoid incurring the cost of resizing many times. That is, as $n$ elements are inserted, the values of increased capacities form a [geometric progression](https://en.wikipedia.org/wiki/Geometric_progression).






Resources

* Wikipedia entry: [Dynamic array — Geometric expansion and amortized cost](https://en.wikipedia.org/wiki/Dynamic_array#Geometric_expansion_and_amortized_cost).





# Queue

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the **core operations of Queue** (enqueue, dequeue, front, empty).
* Describe the difference between `enqueue` and `dequeue`.
* Implement the core operations of Queue efficiently (array based and linked base).
* Explain why an efficient linked implementation of Queue requires a **tail** pointer.  
* Explain why an efficient array based implementation of Queue can logically be viewed as a **circular data structure**.
* Define *Steque*, *Quack*, and **Deque** ADT.

> [Starter code](../../zip/chap13-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap13-solution.zip) for this chapter.


# Queue Abstract Data Type



* Describe the Queue ADT.



A **queue** ADT supports two main operations:

* **enqueue** which adds an element to the data structure
* **dequeue** which removes the least recently added element that was not yet removed

The order in which elements are removed gives rise to the term **FIFO** (first in, first out) to describe a queue.
 




It helps to visualize a queue as an ADT where elements are removed from the front but added to the back, analogously to when people line up to wait for goods or services.

A queue has a variety of applications such as:

* Serving requests on a shared resource, like a printer, CPU, etc.
* Any application that follows a "first come, first served" policy, e.g. customer-service, call-center/ticketing system, etc.
* Whenever it is necessary to preserve sequencing order


Resources

* Wikipedia entry on [Queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)).
* HackerRank YouTube video on [Stack & Queue](https://youtu.be/wjI1WNcIntg).
* [VisuAlgo visualization and interactive demo of lists](https://visualgo.net/en/list).



# Queue Interface



- Identify the operation of Queue ADT.
- Describe the difference between `enqueue` and `dequeue`.




Here is the `Queue` interface which we use in this course:

```java
/**
 * Queue ADT.
 *
 * @param  base type.
 */
public interface Queue {

  /**
   * Adds a new element to the back of this queue.
   *
   * @param value to be added
   */
  void enqueue(T value);

  /**
   * Removes the element at the front of this queue.
   *
   * @throws EmptyException when empty() == true.
   */
  void dequeue() throws EmptyException;

  /**
   * Peeks at the front value without removing it.
   *
   * @return the value at the front of this queue.
   * @throws EmptyException when empty() == true.
   */
  T front() throws EmptyException;

  /**
   * Checks if empty.
   *
   * @return true if this queue is empty and false otherwise.
   */
  boolean empty();
}
```

Make note of `enqueue` and `dequeue`: one adds the other removes. Also, there is a `front` which allows you to peek at the front of the queue without removing the front element. 
# Linked Implementation of Queue



- Implement the core operations of Queue efficiently (linked base).
- Explain why an efficient linked implementation of Queue requires a tail pointer.



A singly linked list can be used to implement the Queue ADT efficiently as long as it has a `tail` pointer (as well as the `head` pointer). The `tail` pointer is a reference variable pointing to the other end (opposite of `head`) of the queue.

Exercise Think about how to implement a queue using a singly linked list such that the core operations are **constant time**. 

| Operation | How? | Time    |
| --------- | ---- | :-------: |
| `enqueue` |      |  $\Omicron(1)$ |
| `dequeue` |      |  $\Omicron(1)$ |
| `front`   |      |  $\Omicron(1)$ |
| `empty`   |      |  $\Omicron(1)$ |


Solution

The front of the queue would be the HEAD node of the singly linked list. The back of the queue would be the TAIL node of the singly linked list. Every time we want to `enqueue` an additional value to the queue, we would create a new node and set it as the new TAIL node. Then when we want to `dequeue` a value from the front of the queue, we set the HEAD to the current HEAD node's `.next`. As such, the HEAD node will always be at the front of the queue, and calling `front` would return the value of the HEAD node.

| Operation | How?                                     | Time    |
| --------- | ---------------------------------------- | :-----: |
| `enqueue` |  apend the list and update the `tail`  |  $\Omicron(1)$ |
| `dequeue` |  delete from front: `head = head.next`   |  $\Omicron(1)$ |
| `front`   |  return `head.data`                      |  $\Omicron(1)$ |
| `empty`   |  check if `head` is `null`               |  $\Omicron(1)$ |


# Trace Linked Implementation



* Trace the core operations of Queue (linked implementation).



Think about a linked implementation of queue such that the core operations are **constant time**.

Exercise Complete the table below: update the linked list as you trace the operations; show value returned if any.

| Operation    | Linked List                    | Value Returned |
| :----------- | :----------------------------- | :------------- |
| `enqueue(1)` | `HEAD -> (1) <- TAIL`          |                |
| `enqueue(2)` | `HEAD -> (1) -> (2) <- TAIL`   |                |
| `enqueue(3)` |                                |                |
| `dequeue()`  |                                |                |
| `enqueue(4)` |                                |                |
| `dequeue()`  |                                |                | 
| `enqueue(5)` |                                |                | 
| `front()`    |                                |                |
| `dequeue()`  |                                |                |
| `front()`    |                                |                |


Solution

| Operation    | Linked List                    | Value Returned |
| :----------- | :----------------------------- | :------------- |
| `enqueue(1)` | `HEAD -> (1) <- TAIL`          |                |
| `enqueue(2)` | `HEAD -> (1) -> (2) <- TAIL`   |                |
| `enqueue(3)` | `HEAD -> (1) -> (2) -> (3) <- TAIL`   |         |
| `dequeue()`  | `HEAD -> (2) -> (3) <- TAIL`   |                |
| `enqueue(4)` | `HEAD -> (2) -> (3) -> (4) <- TAIL`   |         |
| `dequeue()`  | `HEAD -> (3) -> (4) <- TAIL`   |                | 
| `enqueue(5)` | `HEAD -> (3) -> (4) -> (5) <- TAIL`   |         | 
| `front()`    | `HEAD -> (3) -> (4) -> (5) <- TAIL`   |     3   | 
| `dequeue()`  | `HEAD -> (4) -> (5) <- TAIL`   |                |
| `front()`    | `HEAD -> (4) -> (5) <- TAIL`   |        4       |





Resources

* USFCA interactive demo of [Queue (Linked List Implementation)](https://www.cs.usfca.edu/~galles/visualization/QueueLL.html).



# LinkedQueue



* Implement the core operations of Queue efficiently (linked base).



Exercise Open the starter code and complete the implementation of `LinkedQueue`.


Solution

Please check the posted solution. 



# Array Implementation of Queue



- Implement the core operations of Queue efficiently (array-based).
- Explain why an efficient array-based implementation of Queue can logically be viewed as a circular data structure.



We want to implement the `Queue` interface using an array as an internal data storage.

Exercise Think about how to implement a queue using an array such that the core operations are **constant time**. Assume the array is sufficiently large.

| Operation | How? |  Time  |
| --------- | ---- | :----: |
| `enqueue` |      | $\Omicron(1)$ |
| `dequeue` |      | $\Omicron(1)$ |
| `front`   |      | $\Omicron(1)$ |
| `empty`   |      | $\Omicron(1)$ |


Solution

Initialize two variables `front` and `back` to zero.
When you `equeue`, add to the back. Use the `back` variable to index into the array. Then increment it.
When you are asked for front element, simply return `arr[front]`.
When you dequeue, simply increment `front`.

Since you remove from the "front" of the array, then there will be empty positions at the front. So, when the `back` variable reached the end of the array, it can **wrap around it** and write to the (actual) "front" of the array, to positions that were removed earlier.

This gives rise to a **logical view** of array being a circular data structure.





You can also dynamically grow the array when `back` reaches `front`.

| Operation | How?                                          |  Time  |
| --------- | --------------------------------------------- | :----: |
| `enqueue` | `data[back] = value` and `back = ++back % length` | $\Omicron(1)$ |
| `dequeue` | `front = ++front % length`                    | $\Omicron(1)$ |
| `front`     | return `arr[front]`                           | $\Omicron(1)$ |
| `empty`   | check if `numElement == 0`                    | $\Omicron(1)$ |

The `% length` is a trick we use to reset the index when it reaches the length of the array. We could rewrite it as

```java
font = front + 1;
if (front == length) {
  front = 0;
}
```

The example above replaces `front = ++front % length`. The same idea can be applied to updating `back` variable.




Resources

* [Data structures: Array implementation of Queue](https://youtu.be/okr-XE8yTO8) on YouTube.
* [Using an Array to represent a Circular Queue](https://youtu.be/ia__kyuwGag) on YouTube.



# Trace Array Implementation



* Trace the core operations of Queue (array-based implementation).



Think about an array-based implementation of queue such that the core operations are **constant time**.

Exercise Complete the table below: update the array values at indices `0`, `1`, `2`, `3`, and `4` as you trace the operations; show value returned if any.

| Operation     | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | Return Value |
| :------------ | :---: | :---: | :---: | :---: | :---: | :-----------: |
| `enqueue(1)`  |       |       |       |       |       |               |
| `enqueue(2)`  |       |       |       |       |       |               |
| `enqueue(3)`  |       |       |       |       |       |               |
| `dequeue()`   |       |       |       |       |       |               |
| `enqueue(4)`  |       |       |       |       |       |               |
| `dequeue()`   |       |       |       |       |       |               |
| `enqueue(5)`  |       |       |       |       |       |               |
| `front()`     |       |       |       |       |       |               |
| `dequeue()`   |       |       |       |       |       |               |
| `front()`     |       |       |       |       |       |               |


Solution

| Operation     | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | Return Value |
| :------------ | :---: | :---: | :---: | :---: | :---: | :-----------: |
| `enqueue(1)`  |   1   |       |       |       |       |               |
| `enqueue(2)`  |   1   |   2   |       |       |       |               |
| `enqueue(3)`  |   1   |   2   |   3   |       |       |               |
| `dequeue()`   |       |   2   |   3   |       |       |               |
| `enqueue(4)`  |       |   2   |   3   |   4   |       |               |
| `dequeue()`   |       |       |   3   |   4   |       |               |
| `enqueue(5)`  |       |       |   3   |   4   |   5   |               |
| `front()`     |       |       |   3   |   4   |   5   |       3       |
| `dequeue()`   |       |       |       |   4   |   5   |               |
| `front()`     |       |       |       |   4   |   5   |       4       |




Resources

* USFCA interactive demo of [Queue (Array Implementation)](https://www.cs.usfca.edu/~galles/visualization/QueueArray.html)


# ArrayQueue



* Implement the core operations of Queue efficiently (array base).



Exercise Open the starter code and complete the implementation of `ArrayQueue`. 

Notice the constructor of `ArrayQueue` does not take in a parameter for the size of the array. Feel free to initialize an array with an arbitrarily chosen capacity. 


Solution

Please check the posted solution. 


# Other Restricted Data Structures!



* Define Steque, Quack, and Deque ADT.



We can mix and match the operations of Stack and Queue to build up new data structures! Here are a few that you may come by in various references. 


Steque ADT

A stack-ended queue or _steque_ is a data structure that supports `push`, `pop`, and `enqueue`. Here is an example interface:

```java
// A stack-ended queue.
public interface Steque {
  // is empty?
  boolean empty();

  // adds a new element to top of Steque.
  void push(T value);

  // removes and returns top element.
  T pop() throws EmptyException;

  // adds a new element to bottom of Steque.
  void enqueue(T value);
}
```

So, you add from both ends but remove from one end. To better understand this ADT, consider the following sequence of operations and the schematic representation of steque after each operation (top of the steque is to the left):

```java
steque.push(1);     // [1]
steque.push(2);     // [2, 1]
steque.enqueue(3);  // [2, 1, 3]
steque.enqueue(4);  // [2, 1, 3, 4]
steque.pop();       // [1, 3, 4]
```






  Quack ADT

A queue-ended stack or _quack_ is a data structure that supports `push`, `pop`, and `dequeue`. Here is an example interface:

```java
// A queue-ended stack.
public interface Quack {
  // is empty?
  boolean empty();

  // adds a new element to the top of Quack.
  void push(T value);

  // removes and returns top element.
  T pop() throws EmptyException;

  // removes and returns bottom element.
  T dequeue() throws EmptyException;;
}
```

So, you add from one end but remove from both ends. To better understand this ADT, consider the following sequence of operations and the schematic representation of quack after each operation (top of the quack is to the left):

```java
quack.push(1);   // [1]
quack.push(2);   // [2, 1]
quack.push(3);   // [3, 2, 1]
quack.push(4);   // [4, 3, 2, 1]
quack.pop();     // [3, 2, 1]
quack.dequeue(); // [3, 2]
```





Deque ADT

Double Ended Queue (Deque) is a limited access data structure that allows insertion/removal at either end in $\Omicron(1)$.







* Deque is a stack if you always add/remove at one end.
* Deque is a queue if you always add to one end and remove from other.



Here is an example interface:

```java
// A double-ended queue.
public interface Deque {
    boolean empty();
    T front() throws EmptyException;
    T back() throws EmptyException;
    void insertFront(T t);
    void insertBack(T t);
    void removeFront() throws EmptyException;
    void removeBack() throws EmptyException;
}
```



The data structures above are not as widely used as Stack and Queue (especially the first two). However, many programming languages include an implementation of Deque as it can operate as a Stack or a Queue. We leave it to you as an (unsolved) exercise to implement their operations.
# Positional List

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Define what a **Doubly Linked List** is.
* Enumerate the advantages & disadvantages of a doubly linked list vs. a singly linked list.
* Trace the basic operations of a doubly linked list.
* Understand the basic operations of a doubly linked list well enough to implement them.
* Describe the role of **Position** abstraction.
* Explain how Position is different from the Node (inner) class.
* Explain and trace the core operations of **List ADT**.
* Describe the difference between similar operations (e.g., `front` vs. `removeFront`).
* Implement the core operations of List efficiently.
* Analyze alternative implementation approaches' time/space efficiency (e.g., array vs. linked structure).
* Explain the advantages of using **sentinel node-based implementation**.

> [Starter code](../../zip/chap14-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap14-solution.zip) for this chapter.


# Introduction to Doubly Linked List



- Define what a Doubly Linked List is.
- Enumerate the advantages & disadvantages of a double linked list vs. a singly linked list.



The linked list data structure we have studied in earlier chapters is described in the literature as the **singly** linked list (SLL). It is called "singly" because the nodes are chained together in a *single* direction. Each node contains a pointer to the *next* node.





Image is taken from Educative entry on "[What is a doubly linked list](https://www.educative.io/edpresso/what-is-a-doubly-linked-list)."




You can traverse a singly linked list in one direction (from "head" to "tail"). In contrast, there is the **doubly** linked list (DLL) data structure where you can traverse the list in opposite directions. In a DLL, each node contains a pointer to the next node and the previous node.





Image is taken from Educative entry on "[What is a doubly linked list](https://www.educative.io/edpresso/what-is-a-doubly-linked-list)."




Here is a minimal scaffolding of a DLL data structure. 

```java
public class DoublyLinkedList {
  private Node head; 
  // we could have a tail pointer, too!

  private static class Node {
    E data;
    Node next;
    Node prev;
  }

  // Methods are not shown to save space.
}
```

The advantage of DLL over SLL, in addition to bi-directional traversal, is that we can insert/remove **before** a *given node*. The disadvantages are that (1) we need more memory (to store the extra pointers) and (2) we must maintain and update the extra pointer as we perform various operations.  


Resources

* Wikipedia's entry on [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list).



# DLL Operation: Prepend



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.



Suppose we have the following DLL, and we want to add a new node to its front. 





Exercise Complete the implementation of the `addFirst` method that creates a node and adds it to the front of the list.

```java
public void addFirst (T data) {
  // TODO Implement Me!
}
```

Hint: Use the following visualization as a guidance:







Solution

```java
public void addFirst(T data) {
  Node node = new Node<>(data);
  node.next = head;
  head.prev = node;
  head = node;
}
```

**Caution**: the implementation above fails to account for edge cases!




# DLL Operation: Append



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.




Suppose we have the following DLL, and we want to append (add to the end) a new node.





Exercise Complete the implementation of the `addLast` method that creates a node and add it to the back of the list.

```java
public void addLast(T data) {
  // TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void addLast(T data) {
  Node node = new Node<>(data);
  node.prev = tail;
  tail.next = node;
  tail = node;
}
```

**Caution**: the implementation above fails to account for edge cases!




# DLL Operation: Get



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.



Suppose we have a DLL with $n$ elements (nodes), and we want to get the data stored in the $k^{th}$ node (at index $k-1$). 





Exercise Complete the implementation of the `get` method which returns data stored at a given index.

```java
public T get(int index) {
  return null; // TODO Implement me!
}
```

Hint: you cannot directly jump to $K^{th}$ node. You need to start from the `head` or the `tail` and follow the pointers to _get there_!

For this exercise, traverse the list from the `tail` pointer. Assume there is a `numElements` attribute that correctly represents the number of nodes in this DLL.


Solution

```java
public T get(int index) {
  Node target = tail;
  for (int i = numElements - 1; i > index; i--) {
    target = target.prev;
  }
  return target.data;
}
```

**Caution**: the implementation above fails to account for an edge case!


# DLL Operation: Delete



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.



Suppose we want to delete the node pointed to by the reference variable `target`:





Exercise Complete the implementation of `delete`.

```java
public void delete(Node target) {
	// TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void delete(Node target) {
  Node prevNode = target.prev;
  Node nextNode = target.next;

  prevNode.next = nextNode;
  nextNode.prev = prevNode;

  // no need for these!
  target.next = null;
  target.prev = null;
}
```

**Caution**: the implementation above fails to account for edge cases!


# DLL Operation: Insert After



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.



Suppose we want to insert a node *after* the one pointed to by the reference variable `target`:





Exercise Complete the implementation of `insertAfter`.

```java
public void insertAfter(Node target, T data) {
	// TODO Implement me!
}
```

Hint: Use the following visualization as guidance.






Solution

```java
public void insertAfter(Node target, T data) {
  Node nodeToInsert = new Node<>(data);
  Node nextNode = target.next;

  target.next = nodeToInsert;
  nodeToInsert.prev = target;

  nodeToInsert.next = nextNode;
  nextNode.prev = nodeToInsert;
}
```

**Caution**: the implementation above fails to account for edge cases!


# DLL Operation: Insert Before



- Trace the basic operations of a (doubly) linked-list implementation.
- Understand the basic operations of a (doubly) linked list well enough to implement them.




Suppose we want to insert a note *before* the one pointed to by the reference variable `target`:





Exercise Complete the implementation of `insertBefore`.

```java
public void insertBefore(Node target, T data) {
	// TODO Implement me!
}
```

Hint: Use the following visualization as guidance.







Solution

```java
public void insertBefore(Node target, T data) {
  Node nodeToInsert = new Node<>(data);
  Node prevNode = target.prev;

  target.prev = nodeToInsert;
  nodeToInsert.next = target;

  nodeToInsert.prev = prevNode;
  prevNode.next = nodeToInsert;
}
```

**Caution**: the implementation above fails to account for edge cases!


# Exposing Node?



* Understand the potential risks of exposing the Node inner class.



Consider the following operation which we have explored earlier:

```java
public void delete(Node target) { }
public void insertAfter(Node target, T data) { }
public void insertBefore(Node target, T data) { }
```

Exercise How does one get a reference to a node in an arbitrarily position among the nodes in a DLL?


Solution

We can update the `get` method to return a reference to a node at the target position.

```diff
- public T get(int index) { }
+ public Node get(int index) { }
```

We can also update the following methods to return a reference to a newly inserted node.

```diff
- public void addFirst (T data) { }
+ public Node addFirst (T data) { }

- public void addLast(T data) { }
+ public Node addLast(T data) { }
```



Exercise Is there any benefit in, e.g., having `get` to return a Node and later pass this node to a method like `insertAfter` in contrast to having a method like `insertAfter` to recieve the target's position index?


Solution

Imagine this contrived example:

```java
Node node = get(3);
insertAfter(node, 4);
insertBefore(node, 2);
delete(node);
```

In contrast to:

```java
insertAfter(3, 4);
insertBefore(3, 2);
delete(3);
```

In the first snippet, the $$\Omicron(n)$$ search to find the node at index 3 happens once when we call `get`. On the other hand, in the secon snippet, the linear search happens for each operation.




When we use `Node` as a type in the signature of any public operations (such as `delete`), we must also make `Node` available (change it from `private` to a `public` static nested class).

Exercise What are the potential _risks_ of exposing `Node` to clients of DLL?


Solution

A deviant client can change the value stored at a node and change the `next`/`prev` pointers. The latter will corrupt the data structure.



# The Position Interface: Protecting Node!



* Describe the role of Position abstraction.



The `Node` class represents a "position" in a linked list. We can further abstract the idea of a position. For example, we can create the following interface:

```java
/** 
 * Generic position interface.
 * 
 * @param  the element type.
 */
public interface Position {
  
  /** 
   * Read element from this position.
   * 
   * @return element at this position.
   */
  T get();
}
```

Then, we can have the `Node` class to implement this interface:

```java
public class DoublyLinkedList {
  private Node head;
  private Node tail;

  private static class Node implements Position {
    E data;
    Node next;
    Node prev;

    Node(E data) {
      this.data = data;
    }

    @Override
    public E get() {
      return data;
    }
  }

  // Constructor not shown

  public Position addFirst (T data) { }
  public Position addLast(T data) { }
  public Position get(int index) { }
  public void delete(Position target) { }
  public void insertAfter(Position target, T data) { }
  public void insertBefore(Position target, T data) { }
}
```

Notice how a client of `DoublyLinkedList` works with values of type `Position` but internally, the `DoublyLinkedList`, operates on `Node`, the subtype of `Position`.

Exercise How does this strategy "protect" `Node`?


Solution

A client who receives a value of type `Position` have one operation at their disposal: `get`, which returns the data stored at that position. However, they will not directly access `data` or `next`/`prev` reference variables.



# Return a Position!



* Explain how Position is different from the Node (inner) class.



Here is the implementation of `addFirst` from an earlier exercise:

```java
public void addFirst(T data) {
  Node node = new Node<>(data);
  node.next = head;
  head.prev = node;
  head = node;
}
```

Exercise Update `addFirst` to return the newly added node (position) which contains the given data value.

```java
public Position addFirst(T data) {
  // TODO Implement Me!
  return null;
}
```


Solution

```java
public Position addFirst(T data) {
  Node node = new Node<>(data);
  node.next = head;
  head.prev = node;
  head = node;
  return node;
}
```

Notice we can directly return a value of type `Node` where a `Position` is expected. This works because of "type substitution."



# Receive a Position!



* Explain how Position is different from the Node (inner) class.



Here is the implementation of `delete` from an earlier exercise:

```java
public void delete(Node target) {
  Node prevNode = target.prev;
  Node nextNode = target.next;

  prevNode.next = nextNode;
  nextNode.prev = prevNode;
}
```

Exercise Update `delete` so that it receives a "Position" instead of a "Node".

```java
// Pre: target's actual type is Node
public void delete(Position target) {
  // TODO Implement Me!
}
```


Solution

```java
// Pre: target's "actual" type is Node
public void delete(Position target) {
  Node targetNode = (Note) target;

  Node prevNode = targetNode.prev;
  Node nextNode = targetNode.next;

  prevNode.next = nextNode;
  nextNode.prev = prevNode;
}
```

Notice we must explicitly downcast `target` to access the `next` and `prev` reference variables.



# The List ADT: Generic Positional List



- Enumerate the core operations of List ADT
- Describe the difference between similar operations (e.g., `front` vs. `removeFront`)




Here is the List ADT, which is an abstraction build on top of the doubly linked list:

```java
/**
 * Generic positional list abstraction.
 *
 * @param  Element type.
 */
public interface List extends Iterable {
  /**
   * Number of elements in this list.
   *
   * @return Number of elements.
   */
  int length();

  /**
   * List is empty or not.
   *
   * @return True if empty, false otherwise.
   */
  boolean empty();

  /**
   * Insert at the front of this list.
   *
   * @param data Element to insert.
   * @return Position created to hold element.
   */
  Position insertFront(T data);

  /**
   * Insert at the back of this list.
   *
   * @param data Element to insert.
   * @return Position created to hold element.
   */
  Position insertBack(T data);

  /**
   * Insert before a position.
   *
   * @param p Position to insert before.
   * @param data Element to insert.
   * @return Position created to hold element.
   * @throws PositionException If p is invalid for this list.
   */
  Position insertBefore(Position p, T data) throws PositionException;

  /**
   * Insert after a position.
   *
   * @param p Position to insert after.
   * @param data Element to insert.
   * @return Position created to hold element.
   * @throws PositionException If p is invalid for this list.
   */
  Position insertAfter(Position p, T data) throws PositionException;

  /**
   * Remove a position.
   *
   * @param p Position to remove.
   * @throws PositionException If p is invalid for this list.
   */
  void remove(Position p) throws PositionException;

  /**
   * Remove from the front of this list.
   *
   * @throws EmptyException If this list is empty.
   */
  void removeFront() throws EmptyException;

  /**
   * Remove from the back of this list.
   *
   * @throws EmptyException If this list is empty.
   */
  void removeBack() throws EmptyException;

  /**
   * First position.
   *
   * @return First position.
   * @throws EmptyException If this list is empty.
   */
  Position front() throws EmptyException;

  /**
   * Last position.
   *
   * @return Last position.
   * @throws EmptyException If this list is empty.
   */
  Position back() throws EmptyException;

  /**
   * This position is the first position or not.
   *
   * @param p Position to examine.
   * @return True if p is the first position, false otherwise.
   * @throws PositionException If p is invalid for this list.
   */
  boolean first(Position p) throws PositionException;

  /**
   * This position is the last position or not.
   *
   * @param p Position to examine.
   * @return True if p is the last position, false otherwise.
   * @throws PositionException If p is invalid for this list.
   */
  boolean last(Position p) throws PositionException;

  /**
   * Next position.
   *
   * @param p Position to examine.
   * @return Position after p.
   * @throws PositionException If p is invalid or the last position.
   */
  Position next(Position p) throws PositionException;

  /**
   * Previous position.
   *
   * @param p Position to examine.
   * @return Position before p.
   * @throws PositionException If p is invalid or the first position.
   */
  Position previous(Position p) throws PositionException;

  /**
   * Returns an iterator "going forward" over list elements of type T.
   * The standard iterator() method creates one of these.
   *
   * @return Iterator going from front to back.
   */
  Iterator forward();

  /**
   * Returns an iterator "going backward" over list elements of type T.
   *
   * @return Iterator going from back to front.
   */
  Iterator backward();
}
```

Notes:

* You can insert/remove from either end.
* You can insert/remove in between given a "position."
* There are several "getter" methods to access, e.g., front, back, and the next or previous positions given a "position."
* There are three iterators: forward, backward, and a default one inherited from Iterable.
# Position ADT



* Describe the role of Position abstraction.




Let's revisit the `Position` interface:

```java
/**
 * Generic position interface.
 *
 * @param  the element type.
 */
public interface Position {

  /**
   * Read element from this position.
   *
   * @return element at this position.
   */
  T get();
}
```

Recall we have created this abstraction to _protect_ the `Node` class. 
All operations of the List ADT receive/return Position, but internally, a position is _cast_ into a node.
# Position Exception



* Describe the role of PositionException.



Data structures that work with `Position` throw `PositionException` for a variety of reasons:

* when a given position is `null`;
* when a given position refers to a different, unrelated instance, e.g., passing a position from List A into List B;
* when a given position refers to a different data structure, e.g., passing a Graph position to a List.

```java
/**
 * Exception for position-based data structures.
 *
 * Data structures that use Position interface throw PositionException
 * if the position provided to them is null or otherwise invalid.
 */
public class PositionException extends RuntimeException {
  /**
   * Constructs a new PositionException.
   */
  public PositionException() {

  }

  /**
   * Constructs a new PositionException with the specified detail message.
   *
   * @param message the detail message. The detail message is saved for
   *                later retrieval by the getMessage() method.
   */
  public PositionException(String message) {
    super(message);
  }
}
```
# Node is-a Position!



* Explain how Position is different from the Node (inner) class.




Here is how we declare the `Node` class:

```java
private static class Node implements Position {
  Node next;
  Node prev;
  E data;
  List owner;

  Node(E data, List owner) {
    this.data = data;
    this.owner = owner;
  }

  @Override
  public E get() {
    return data;
  }
}
```

Notice the attribute `owner`. We use this attribute to check whether a "position" belongs to this data structure or not. 

For example, when we insert to the front of the List:

```java
public Position insertFront(T data) {
  Node newFront = new Node(data, this);
 
  // update current front, head, tail, numElements, etc.

  return newFront;
}
```

By passing `this` to the constructor of `Node`, we set "this data structure" to be the owner of `newFront`.

We can then check if a given position belongs to this data structure:

```java
public boolean first(Position position) throws PositionException {
  Node node = (Node) position;
  if (node.owner != this) {
    throw new PositionException();
  }

  return head == node;
}
```
# The convert method!



- Explain how Position is different from the Node (inner) class.
- Describe the role of the convert method.




Open the `LinkedList.java` file in the starter code and find the helper method `convert`:

```java
// Convert a Position back into a Node.
// Guards against null positions, positions from other data structures,
// and positions that belong to other LinkedList objects.
private Node convert(Position position) throws PositionException {
  try {
    Node node = (Node) position;
    if (node.owner != this) {
      throw new PositionException();
    }
    return node;
  } catch (NullPointerException | ClassCastException e) {
    throw new PositionException();
  }
}
```

The `convert` method takes care of any situation that may require throwing the `PositionException`. You should be using this method when receiving a position as an argument. For example:

```java
public boolean first(Position position) throws PositionException {
  Node node = convert(position);
  return head == node;
}
```
# List ADT: Efficient Implementation



* Analyze the time/space efficiency of alternative implementation approaches (e.g., array vs. linked structure).



We want to implement the List ADT so that all core operations are in $\Omicron(1)$. We claim this can only be done using a doubly linked list (with a TAIL pointer). 

Exercise Give an example operation of the List ADT which cannot be implemented in constant time using an array-based or singly linked list implementation. 


Solution

The `insertBefore` operation will be at the best linear time:

* In an array-based implementation, we need to shift all the elements at the given position to the right to make room for the given value to be inserted before the given position.
  
* In a singly linked list, we need to traverse the list from the HEAD node to reach the node (position) before the given position. We must do this search because there is no "previous" pointer to get hold of a node before a given position quickly.



# Revisit `delete`



* Implement the core operations of List efficiently.



Open the `DoublyLinkedList.java` file and find the `delete` method.

```java
public void delete(Position target) {
  Node targetNode = convert(target);
  Node beforeTarget = targetNode.prev;
  Node afterTarget = targetNode.next;

  beforeTarget.next = afterTarget;
  afterTarget.prev = beforeTarget;
}
```

Exercise The implementation above fails to account for some edge cases! Identify those edge cases. 


Solution

Here are some edge cases:

* If `target` is the first element, we will not have a `beforeTarget`.
* If `target` is the last element, we will not have a `afterTarget`.
* If `target` is the only element, we will not have a `beforeTarget` nor a `afterTarget`.

In every case above, we need to treat with care and also make sure to update the `head` and/or `tail` pointers.


# Using Sentinel Nodes



* Explain the advantages of using sentinel node-based implementation.




A common practice when implementing a link list is to use _sentinel_ nodes. 





* Sentinel nodes are _dummy_ nodes that you add to both ends of your linked list. 
* You will have the `head` and the `tail` pointer to always point to sentinel nodes.
The constructor will construct the sentinel nodes; they will never be removed nor updated.
* You should not count the sentinel nodes as elements of the data structure. 
* The sentinel nodes should not be considered when iterating over the elements of the data structure.

Here is the constructor of `LinkedList` building the sentinel nodes:

```java
public LinkedList() {
  head = new Node<>(null, this);
  tail = new Node<>(null, this);
  head.next = tail;
  tail.prev = head;
  numElements = 0;
}
```



Using sentinel nodes eliminates many of the _edge cases_ that arise in implementing linked list operations.




Exercise Update the implementation of `delete` (in `DoublyLinkedList.java`) to account for all edge cases. Consider `head` and `tail` point to sentinel nodes. 



Solution

Well, we don't need to make any changes to the implementation of `delete` because with the addition of sentinel nodes, all those edge cases are accountd for!

```java
public void delete(Position target) {
  Node targetNode = convert(target);
  Node beforeTarget = targetNode.prev;
  Node afterTarget = targetNode.next;

  beforeTarget.next = afterTarget;
  afterTarget.prev = beforeTarget;
}
```




Resources

* Wikipedia's entry on [Sentinel Node](https://en.wikipedia.org/wiki/Sentinel_node).



# Implement `LinkedList`



* Implement the core operations of List efficiently.



Exercise Open the starter code and complete the implementation of `LinkedList`. We want all core operations of List ADT to run in $\Omicron(1)$. Use sentinel node based implementation. 

**Hint:** The unit tests in `ListTest` are going to be very helpful.


Solution

Please check the posted solution. 


# Set ADT

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the core operations of **Set ADT**.
* Implement the core operations of Set efficiently using array/linked structure.
* Analyze the time/space efficiency of alternative implementation approaches (e.g., array vs. linked structure)
* Understand "move-to-front" and "transpose sequential search" *heuristics* well enough to implement them.
* Implement **Fail-Fast** iterators for the Set ADT.
* Explain the difference between the Set and the **OrderedSet ADT**.
* Implement the core operations of OrderedSet efficiently using array/linked structure.
* Describe set-theoretical operations *union*, *intersection*, and *set difference*.

> [Starter code](../../zip/chap15-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap15-solution.zip) for this chapter.


# Set ADT: The Interface



* Explain and trace the core operations of Set ADT.



A set is an *iterable* collection of **unique** elements. A set has no particular ordering of elements (neither by position nor by value).


```java
/**
 * Sets of arbitrary values (not necessarily Comparable).
 * Iteration order is undefined.
 *
 * @param  Element type.
 */
public interface Set extends Iterable {
  /**
   * Insert a value.
   * Set doesn't change if we try to insert an existing value.
   * Post: has(t) == true.
   *
   * @param t Value to insert.
   */
  void insert(T t);

  /**
   * Remove a value.
   * Set doesn't change if we try to remove a non-existent value.
   * Post: has(t) == false.
   *
   * @param t Value to remove.
   */
  void remove(T t);

  /**
   * Test membership of a value.
   *
   * @param t Value to test.
   * @return True if t is in the set, false otherwise.
   */
  boolean has(T t);

  /**
   * Number of values.
   *
   * @return Number of values in the set, always greater equal to 0.
   */
  int size();
}
```

# Linked Implementation of Set



- Implement the core operations of Set efficiently (linked base).
- Analyze the time/space efficiency of linked implementation.




We want to _efficiently_ implement the Set ADT with an underlying linked list. (Go for the most straightforward choice, *singly linked list*, unless efficiently demands more complex structures.)

Exercise Complete the following table. 

| Operation | How? | Runtime   |
| --------- | ---- | :-------: |
| `has`     |      |           |
| `insert`  |      |           |
| `remove`  |      |           |
| `size`    |      |           |


Solution

Except for `size`, all operations require a helper `find` method to check if an element exists. We cannot do better than Linear Search for `find`.

| Operation | How?                                        | Runtime   |
| --------- | ------------------------------------------- | :-------: |
| `has`     | `return find(t) != null;`                   |   $\Omicron(n)$  |
| `insert`  | `if (find(t) == null), prepend(t);`         |   $\Omicron(n)$  |
| `remove`  | `remove(find(t));`                          |   $\Omicron(n)$  |
| `size`    | `return numElements;`                       |   $\Omicron(1)$  |
| `find`    | Linear search                               |   $\Omicron(n)$  |

We can use a doubly linked list so once the "node to be removed" is found, we can remove it in constant time (we need access to the previous node). Or we can have a `findPrevious` method to get hold of the node before the one "to be removed" in a singly linked list, in linear time, and then remove the "next" node (the target node) in constant time.


# `LinkedSet`



* Implement the core operations of Set efficiently (linked base).



Exercise Open the starter code and complete the implementation of `LinkedSet`. 


Solution

Please check the posted solution. 



# Hacking the $\Omicron(n)$ Find



* Understand move-to-front heuristic well enough to implement it.



Consider the following implementation of `find`:

```java
private Node find(T t) {
  for (Node n = head; n != null; n = n.next) {
    if (n.data.equals(t)) {
      return n;
    }
  }
  return null;
}
```

Exercise Update the implementation of `find` to employ the ["move-to-front heuristic"](https://xlinux.nist.gov/dads/HTML/movefront.html) as it is described in the ["Dictionary of Algorithms and Data Structures"](https://xlinux.nist.gov/dads/).


Solution

Assuming helper methods `remove` and `prepend` exist, you can remove the target node and then prepend it to the list.





# Array Implementation of Set



- Implement the core operations of Set efficiently (array base).
- Analyze the time/space efficiency of array-based implementation.




We want to _efficiently_ implement the Set ADT with an array as the underlying data structure.

Exercise Complete the following table. 

| Operation | How? | Runtime   |
| --------- | ---- | :-------: |
| `has`     |      |           |
| `insert`  |      |           |
| `remove`  |      |           |
| `size`    |      |           |


Solution

Except for `size`, all operations require a helper `find` method to check if an element exists. We cannot keep the underlying data sorted to perform Binary Search (why?). We will, however, explore this option for implementing an array-based OrderedSet ADT. Let's keep the underlying data unsorted and perform Linear Search in `find`.

| Operation | How?                                        | Runtime   |
| --------- | ------------------------------------------- | :-------: |
| `has`     | `return find(t) != -1;`                     |   $\Omicron(n)$  |
| `insert`  | `if (fint(t) == -1) data[numElement++] = t;`  |   $\Omicron(n)$  |
| `remove`  | Find the element, swap with last, `numElements--` |   $\Omicron(n)$  |
| `size`    | `return numElements;`                       |   $\Omicron(1)$  |
| `find`    | Linear search                               |   $\Omicron(n)$  |

Notice the `remove` strategy, which allows us to spend constant time after the element is found.




# `ArraySet`



* Implement the core operations of Set efficiently (array base).



Exercise Open the starter code and complete the implementation of `ArraySet`.


Solution

Please check the posted solution. 



# Hacking the $\Omicron(n)$ Find



* Understand the "transpose sequential search" heuristic well enough to implement it.



Recall we can use a *heuristic* that moves the target of a search to the head of a list, so it is found faster next time. This technique is called ["move-to-front heuristic"](https://xlinux.nist.gov/dads/HTML/movefront.html). It speeds up linear search performance in the linked list if the target item is likely to be searched again soon.

Exercise Can we apply the *move-to-front* heuristic to speed up linear search in an array?


Solution

Maybe! Moving the search target to the front of an array requires shifting all the other elements to the right. This is an additional linear time operation (in addition to the linear search).

**Aside:** It would not be a good idea to implement the "move-to-front" heuristic in an array by swapping the target with the front value (instead of "moving" the target element to the front). (Think why?)

A more common strategy is ["transpose sequential search"](https://xlinux.nist.gov/dads/HTML/transposeSeqSearch.html) heuristic. Look up Wikipedia's entry on [Techniques for rearranging nodes in Self-organizing list](https://en.wikipedia.org/wiki/Self-organizing_list#Techniques_for_rearranging_nodes) for more information.



# Set Iterator



* Implement an iterator for the Set ADT.



We know Set is an *unordered* collection.

Exercise How should we implement the iterator for Set ADT?

A) Iterate in the same order the elements have been added. \
B) Iterate in the order that corresponds to the natural ordering of the elements (e.g., smallest first, largest last). \
C) To ensure this is an _unordered_ collection, we must iterate over the elements in random order. \
D) Iterate over the elements from `head` to `tail` in `LinkSet` and from index `0` to `numElements - 1` in `ArraySet`.


Solution

The correct answer is **D**. It is the cheapest strategy. 

The statement that "set is an _unordered_ collection" implies that a client shall **not** expect the iteration is done in any particular order. However, we don't need to go out of our way to ensure an un-orderly iteration!



# Fail-Fast Iterators



* Implement fail-fast iterators for the Set ADT.



Have you ever thought about what will happen if you *structurally* modify a data structure while iterating over it? 



A structural modification is any operation that adds or deletes elements or explicitly resizes the data structure.



Assume you are iterating over an ArraySet. While the iteration is going on, an element you've already visited (iterated over) is removed. This could happen in a _concurrent_ program, but here is a contrived example to showcase this scenario:

```java
for (int num: myArraySet) {
  // do something with num
  if (feelingLucky()) {
    myArraySet.remove(num);
  }
}
```

Exercise Can you anticipate any issues with iteration?


Solution

It depends on the implementation of the iterator and the remove method. In general, the results of the iteration are undefined under a structural modification.

Suppose we assume the removal strategy is the one we have discussed earlier. In that case, the last element of the array will be swapped with the element to be removed. Thus, effectively we will end the iteration by not visiting (not knowing about) the last element before removal.





In Java's Collection Framework, it is (generally) prohibited for one thread (program, process) to modify a Collection while it (or another thread) is iterating over it. 



When that happens, the iterator will throw [ConcurrentModificationException](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ConcurrentModificationException.html). 

> Iterators that throw an exception to signal the collection was structurally modified during an iteration are known as **fail-fast** iterators. They fail quickly and cleanly, rather than risking arbitrary, non-deterministic behavior at an undetermined time in the future.

Exercise How can you make an iterator "fail-fast"?

**Hint:** To make an iterator "fail-fast," we need to be able to tell that the data structure has been modified since the iteration started.


Solution
 
Here is one strategy: use a **version number** in the data structure class to achieve this. 

* The number starts at `0` and is incremented whenever a structural modification is performed. 
* Each iterator also "remembers" the version number it was created for. 
* We can then check for modifications by comparing version numbers in the Iterator operations: We raise an exception if we notice a mismatch.

We have implemented this feature in the `LinkedSet`. So, make sure to study it when you get the solution code. Then, try to implement it for `ArraySet`.





# Ordered Set



* Explain the difference between the Set ADT and the OrderedSet ADT.



An ordered-set is an *iterable* collection of **ordered** *unique* elements. The elements are expected to be iterated over in order, based on their values.


```java
/**
 * Ordered set of arbitrary values.
 * Iteration order is based on the values.
 *
 * @param  Element type.
 */
public interface OrderedSet> extends Set {
  // Same operations as the Set ADT
}
```

Notice we must use _bounded generics_ to ensure the elements are comparable (otherwise, we cannot put them in order).
# Linked Implementation of OrderedSet



* Implement the core operations of Ordered Set efficiently (linked base).



We want to _efficiently_ implement the OrderedSet ADT with an underlying linked list. 

Exercise Complete the following table. 

| Operation | How? | Runtime   |
| --------- | ---- | :-------: |
| `has`     |      |           |
| `insert`  |      |           |
| `remove`  |      |           |
| `size`    |      |           |


Solution

Except for `size`, all operations require a helper `find` method to check if an element exists. Thus, we cannot do better than Linear Search for `find`. (Performing Binary Search on a linked list is futile as its cost is $\Omicron(n)$ while its implementation is more complex than Linear Search.)

| Operation | How?                                        | Runtime   |
| --------- | ------------------------------------------- | :-------: |
| `has`     | `return find(t) != null;`                   |   $\Omicron(n)$  |
| `insert`  | Find where to insert, then insert!          |   $\Omicron(n)$  |
| `remove`  | `remove(find(t));`                          |   $\Omicron(n)$  |
| `size`    | `return numElements;`                       |   $\Omicron(1)$  |
| `find`    | Linear search                               |   $\Omicron(n)$  |

We can come up with a clever implementation so the `find` method would return the previous node of the target (instead of the target itself). This will make the `insert` method easier to use the same `find` operation like the one used by other operations. However, we leave this as an (unsolved) exercise to you to implement the functions of OrderedSet using a linked list.


# Array Implementation of OrderedSet



* Implement the core operations of OrderedSet efficiently (array base).



We want to _efficiently_ implement the OrderedSet ADT with an array as the underlying data structure.

Exercise Complete the following table. 

| Operation | How? | Runtime   |
| --------- | ---- | :-------: |
| `has`     |      |           |
| `insert`  |      |           |
| `remove`  |      |           |
| `size`    |      |           |


Solution

Except for `size`, all operations require a helper `find` method to check if an element exists. Then, we can perform Binary Search, so `find` and `has` will cost $\Omicron(\lg n)$. The `insert` and `remove` operations will remain linear since we must shift the elements around to keep the values in order.

| Operation | How?                                        | Runtime   |
| --------- | ------------------------------------------- | :-------: |
| `has`     | `return find(t) != -1;`                     |   $\Omicron(\lg n)$  |
| `insert`  | Find where to insert, then shift elements to make room.  |   $\Omicron(n)$  |
| `remove`  | Find the element, shift all element after it to left.   |   $\Omicron(n)$  |
| `size`    | `return numElements;`                       |   $\Omicron(1)$  |
| `find`    | Binary search                               |   $\Omicron(\lg n)$  |

We leave this as an (unsolved) exercise to you: implement the operations of an array-based OrderedSet.




# Set-theoretical Operations



* Describe set-theoretical operations union, intersection, and set difference.



In Mathematics, two sets can be "added" together. Thus, for example, the **union** of $A$ and $B$, denoted by $A \cup B$, is the set of all elements of either $A$ or $B$.

We can include this operation in the Set ADT:

```java
/**
 * Constructing a new set with elements that are in this set or
 * in the other set.
 *
 * @param other set.
 * @return all elements that are in this set or the other set.
 */
Set union(Set other);
```

The **intersection** of $A$ and $B$, denoted by $A \cap B$, is the set of elements that are members of both $A$ and $B$. 


```java
/**
 * Constructing a new set with elements that are in this set and
 * in the other set.
 *
 * @param other set.
 * @return the elements this set and other set have in common.
 */
Set intersect(Set other);
```

The **set difference** of $A$ and $B$, denoted by $A - B$, is the set of all elements that are in $A$ but not in $B$. 

```java
/**
 * Constructing a new set with elements that are in this set but not
 * in the other set.
 *
 * @param other set.
 * @return the elements in this set but not in the other set.
 */
Set subtract(Set other);
``` 

These operations can be defined for OrderedSet ADT as well:

```java
OrderedSet union(OrderedSet other); 
OrderedSet intersect(OrderedSet other);  
OrderedSet subtract(OrderedSet other);  
```

We leave it to you as a *challenge* exercise to implement these operations _efficiently_.


# Binary Search Tree

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Illustrate the **decision tree** corresponding to the binary search process.
* Identify the various components of a (rooted) **tree** structure.
* Differentiate general (rooted) tree from a **binary tree**.
* Recognize the *recursive* nature of the tree structure.
* Differentiate binary trees from binary search trees based on the **complete ordering property**.
* Recognize a tree as a **non-linear** *linked* structure.
* Explain and trace the core operations of a Binary Search Tree.
* Implement the core operations of an OrderedSet efficiently with a Binary Search Tree.

> [Starter code](../../zip/chap16-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap16-solution.zip) for this chapter.


# Preface



* Setting the stage for Binary Search Tree



We will learn about a new data structure called **Binary Search Tree** (hereafter abbreviated as **BST**). We will use BST to implement the **OrderedSet ADT**, and we claim this implementation can, potentially, be more efficient than the alternatives we have explored.

The idea behind the new data structure is to create **non-linear** *linked nodes*. By doing so, we can perform a binary search (with the expected efficiency, as in a sorted array). However, _insertion_ and _removeal_ are also efficient (as we don't need to shift elements).





Go to the next lesson for an explanation of the illustration above!


# The Decision Tree



* Illustrate the decision tree corresponding to the binary search process.



Consider the following ordered sequence of numbers:

$$
2, 4, 5, 7, 8, 9, 12, 14, 17, 20, 21, 25
$$

Imagine we are searching for the value $x$, and we perform a binary search. The following illustration shows the [decision tree](https://en.wikipedia.org/wiki/Decision_tree) (or comparison tree or search tree) corresponding to the binary search process.







A decision tree is a flowchart-like structure that models the decisions (conditional statements in an algorithm) and their possible outcomes (consequences).







Irrespective of the value of $x$, we start the search by comparing it to the value of the element in the middle of the sequence, $9$. If $x = 9$ then the search ends. Otherwise, there are two possible outcomes:
1. $x>9$, in which case we will explore the right half of the sequence (the next comparison will be against $17$). 
2. $x<9$, in which case we will explore the left half of the sequence (the next comparison will be against $5$).

This process is repeated until the search succeeds or fails (no more values are left to compare). 
# Is it a tree?



* Connect the notion of a tree structure with the tree in nature!



Let's focus on this structure:





This is a **tree**! Imagine an upside-down tree where the root (and trunk) is at the top, and the branches are growing downward! 






Resources

In the context of Graph Theory (Discrete Math), the structure above is called a "rooted" tree. Read the Wikipedia entry on [Tree, Rooted tree](https://en.wikipedia.org/wiki/Tree_(graph_theory)#Rooted_tree) for more information.

If you are interested in a detailed theoretical (mathematical) description of a tree, visit Chapter 4, Section 2 of [Discrete Mathematics: An Open Introduction, 3rd edition](http://discrete.openmathbooks.org/dmoi3/sec_trees.html).


# Tree Components



* Identify the various components of a rooted tree structure.



In the tree structure below, each circle that contains a value is called a **node**.





The first node from where the tree originates is called the **root** node.

The tree creates a hierarchical structure where except for the root node, every other node has a **parent** and *zero or more* **children**. 

The nodes with no children are called **leaves**.

Exercise Given the tree structure above, complete the table below:

|                                             | Answer |
|:--------------------------------------------|:------:|
| The **root** is                             |        |
| The **leaves** are                          |        |
| The **parent** of node with value $21$ is   |        |
| The **children** of node with value $5$ are |        |



Solution

|                                             |       Answer       |
|:--------------------------------------------|:------------------:|
| The **root** is                             |        $9$         |
| The **leaves** are                          | $4, 8, 14, 20, 25$ |
| The **parent** of node with value $21$ is   |        $17$        |
| The **children** of node with value $5$ are |       $2, 7$       |



The nodes which belong to the same "parent" are called *siblings*! 

The _parent-child_ relationship between nodes can be extended to _ancestors_ and _descendants_. 

For example, 

* The nodes $21$, $17$, and $9$ are ancestors of $25$.
* The nodes $2$, $7$, $4$, and $8$ are descendants of $5$.

The root is an ancestor of every node in a tree, and every node is a descendant of the root.


Resources

* Wikipedia's entry on [Tree (data structure), Terminology](https://en.wikipedia.org/wiki/Tree_(data_structure)#Terminology).



# Binary Tree



* Differentiate general (rooted) tree from a binary tree.



The tree illustrated below is an instance of a **binary tree**.







In a binary tree, each node has at most **two** children (hence called *binary*).



Exercise Which of the following trees are binary tree?






Solution

All correct!




Resources

* Wikipedia's entry on [Binary Tree](https://en.wikipedia.org/wiki/Binary_tree).



# Binary Tree: A recursive structure



* Recognize the recursive nature of the tree structure.



A binary tree is a *recursive* structure. You can imagine there is a left and a right **subtree** to the sides of the root, each with their own root and left/right subtrees, each with their own root and $\dots$







In a tree $T$, a node $n$ together with all of its descendants, if any, is called a subtree of $T$. Node $n$ is the root of its subtree.



The recursive structure of the tree lends itself to a recursive implementation of operations on it.


Resources

* Definition on Dictionary of Algorithms and Data Structures: [recursive data structure](https://xlinux.nist.gov/dads/HTML/recursivstrc.html).



# Binary Search Tree



* Differentiate binary trees from binary search trees based on the complete ordering property.



The tree illustrated below is a special *binary tree* called **binary search tree**







In a binary search tree, for a value stored at a node, 
1. All the values in its left subtree are smaller than it. 
2. All the values in its right subtree are greater than it.



The property described above is often referred to as the **complete ordering property** of BSTs.

Exercise Which of the following trees are Binary Search Tree?






Solution

(B) is the correct answer!









Resources

* Wikipedia's entry on [Binary Search Tree](https://en.wikipedia.org/wiki/Binary_search_tree).



# BST Node



* Recognize a tree as a non-linear linked structure.



A tree is a **non-linear** _linked_ structure. 
We can easily repurpose the `Node` class to implement a _binary_ tree.

```java
public class BstOrderedSet> 
    implements OrderedSet {
  
  private Node root;

  private static class Node {
    E data;
    Node left;
    Node right;

    Node(E data) {
      this.data = data;
    }
  }

  // Other operations not shown here.
}
```

In `Node`, the `left` reference variable points to the _root_ of the left subtree, i.e., the left child of the node. Likewise, the `right` reference variable points to the _root_ of the right subtree, i.e., the right child of the node.
# BST Operation: Find



* Implement the core operations of an OrderedSet efficiently with a Binary Search Tree.



The `has` operation in a BST implementation of OrderedSet ADT can perform a binary search.





Exercise Complete the implementation of `has`.

```java
@Override
public boolean has(T t) {
  // TODO Implement me!
  return false;
}
```

**Suggestion**: Make use of a private helper `find` method.


Solution

Here is the implementation of `has`:

```java
@Override
public boolean has(T t) {
  Node target = find(t);
  return target != null;
}
```

And this is the implementation of the `find` helper method:

```java
private Node find(T data) {
  Node curr = root;
  while (curr != null) {
    if (curr.data.compareTo(data) > 0) {
      curr = curr.left;
    } else if (curr.data.compareTo(data) < 0) {
      curr = curr.right;
    } else {
      return curr;
    }
  }
  return curr;
}
```

We could also implement `find` recursively:

```java
private Node find(Node root, T data) {
  if (root == null) {
    return null;
  }
  if (root.data.compareTo(data) > 0) {
    return find(root.left, data);
  } else if (root.data.compareTo(data) < 0) {
    return find(root.right, data);
  } else {
    return root;
  }
}
```

Here is the same but more _polished_ recursive implementation!

```java
private Node find(Node root, T data) {
  if (root == null || root.data.equals(data)) {
    return root;
  }
  
  if (root.data.compareTo(data) > 0) {
    return find(root.left, data);
  }
  
  return find(root.right, data);
}
```


# BST Operation: Insert (How it works)



* Explain and trace the core operations of a Binary Search Tree.



In a BST implementation of OrderedSet ADT, the' insert' operation must (partially) traverse the tree to find where to insert the new element, so the BST *remains* valid.





Exercise Insert $6$ to the following BST.






Solution








# Insert (Tracing)



* Explain and trace the core operations of a Binary Search Tree.



Exercise Draw the binary search tree resulting from inserting these values, in this order:  

$$
7,  2,  13,  4,  5,  15,  10,  17,  8,  14,  11
$$


Solution








Resources

* You can use [this interactive BST visualizer](http://btv.melezinek.cz/binary-search-tree.html) to solve this exercise.


# Insert (Implementation)



* Implement the core operations of an OrderedSet efficiently with a Binary Search Tree.



Exercise Complete the implementation of `insert`.


```java
@Override
public void insert(T t) {
  // TODO Implement Me!
}
```


Solution

Here is an iterative implementation:

```java
public void insert(T t) {
  if (root == null) { // edge case!
    root = new Node<>(t);
    return;
  }

  Node curr = root;
  while (curr != null) {
    if (curr.data.compareTo(t) > 0) {
      if (curr.left == null) { // reached a leaf
        // insert the new node
        curr.left = new Node<>(t);
        curr = null; // break out of the loop
      } else {
        // explore the left subtree
        curr = curr.left;
      }
    } else if (curr.data.compareTo(t) < 0) {
      if (curr.right == null) { // reached a leaf
        // insert the new node
        curr.right = new Node<>(t);
        curr = null; // break out of the loop
      } else {
        // explore the right subtree
        curr = curr.right;
      }
    } else {
      // duplicate is not allowed!
      return;
    }
  }
}
```

Here is a recursive implementation of `insert`

```java
@Override
public void insert(T t) {
  root = insert(root, t);
}

/* inserts given value into subtree rooted at given node
    and returns changed subtree with new node added. */
private Node insert (Node root, T data) {
  if (root == null) {
    /* If the tree is empty, return a new node */
    return new Node<>(data);
  }

  /* Otherwise, recur down the tree */
  if (root.data.compareTo(data) > 0) {
    root.left = insert(root.left, data);
  } else if (root.data.compareTo(data) < 0) {
    root.right = insert(root.right, data);
  }

  /* return the (unchanged) node pointer */
  return root;
}
```

Note the use of recursion for insert — using the value returned to update the current node. This may take a moment to understand! 



# BST Operation: Remove (How it works)



* Explain and trace the core operations of a Binary Search Tree.



The `remove` operation in a BST implementation of OrderedSet ADT is a somewhat involved process! We will break it down into three scenarios.


- ***Node to be removed is a leaf:*** Simply remove from the tree!

  ```text
                50                             50
              /     \         remove(20)      /   \
            30      70       --------->    30     70 
            /  \    /  \                     \    /  \ 
          20   40  60   80                   40  60   80
  ```

- ***Node to be removed has only one child:*** Copy the child value into the node and delete the child node.

  ```text 
                50                             50
              /     \         remove(30)      /   \
            30      70       --------->    40     70 
              \    /  \                          /  \ 
              40  60   80                       60   80
  ```

- ***Node to be removed has two children:***
    1. Find the smallest value in the node's right subtree (*in-order* successor). 
    2. Copy the value key of the in-order successor to the target node and delete the in-order successor. 

  ```text
                50                           60
              /    \         remove(50)     /   \
            40      70       --------->    40    70 
                    /  \                            \ 
                  60   80                           80
  ```

    - Note that the largest value in the left subtree (in-order predecessor) can also be used.


Exercise Remove $17$ from the following BST.






Solution

Either replace $17$ with the smallest value in its right subtree:





Or replace $17$ with the largest value in its left subtree:








# Remove (Tracing)



* Explain and trace the core operations of a Binary Search Tree.



Consider this BST





Exercise Starting with the above binary search tree, draw it after deleting each of these in turn: $5$, $2$, $13$.



Solution

`remove(5)`





`remove(2)`





`remove(13)`







# Remove (Implementation)



* Implement the core operations of an OrderedSet efficiently with a Binary Search Tree.



Exercise Complete the implementation of `remove`.

```java
@Override
public void remove(T t) {
  // TODO Implement Me!
}
```

**Hint:** Go for a recursive implementation. An iterative one without having a `parent` pointer in the `Node` class will be tough to carry out.


Solution

```java
@Override
public void remove(T t) {
  // Uses a recursive (private) helper insert
  root = remove(root, t);
}
```

```java
/* removes node with given value in the subtree rooted
   at given node and returns modified subtree. */
private Node remove(Node node, T t) {
  if (node == null) { // base case
    return node;
  }
  // find the node that contains "t"
  if (node.data.compareTo(t) > 0) {
    node.left = remove(node.left, t);
  } else if (node.data.compareTo(t) < 0) {
    node.right = remove(node.right, t);
  } else { // found it; let's remove it!
    // zero or one child
    if (node.right == null) {
      return node.left;
    } else if (node.left == null) {
      return node.right;
    }
    // two children
    Node next = findSmallest(node.right);
    node.data = next.data;
    node.right = remove(node.right, next.data);
    numElements--;
  }

  return node;
}
```

```java
// find the smallest value in subtree rooted at node
// Pre: node != null
private Node findSmallest(Node node) {
  Node small = node;
  while (small.left != null) {
    // go left as far as we can
    small = small.left;
  }
  return small;
}
```


# Tree & Tree Traversal

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace (binary) Tree Traversals (level-order, in-order, pre-order, and post-order).
* Implement various tree traversals.
* Define tree-related terminologies: *Path*, *Height*, *Depth*, *Level*.
* Explain why the height of a **full binary tree** is $\Omicron(\lg N)$.
* Distinguish between *best-case* vs. *worst-case* running time of operations on BST.
* Differentiate general (rooted) tree from a binary tree
* Recognize applications of general-purpose tree
* Recognize "Array of References" and "Leftmost-Child–Right-Sibling" representations of the general-purpose tree.

> [Starter code](../../zip/chap17-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap17-solution.zip) for this chapter.


# Tree Traversal



* Enumerate the (binary) tree traversals.



How can one iterate over all the elements of this BST? 





There are four traversal strategies for iterating over the elements of a _binary_ tree:

* Level-order traversal
* In-order traversal
* Pre-order traversal
* Post-order traversal

We will explore each.
# Level-order Traversal



* Explain and trace level-order traversal.



In level-order traversal, nodes are visited *level by level* from *left to right*, starting from the root.





Following the level-order traversal will generate this sequence:

$$
9, 5, 17, 2, 7, 12, 21, 4, 8, 14, 20, 25
$$

Exercise Carry out the level-order traversal for the following BST. 






Solution

$$
7, 2, 13, 4, 10, 15, 5, 8, 11, 14, 17
$$



# In-order Traversal: Left, Root, Right!



* Explain in-order traversal.



In a BST, in-order traversal will produce the sequence in order (duh!). In a way, we recreate the ordered sequence where this binary (decision) tree was generated from:





This is the strategy:



For every node, visit the left subtree, then the node, then the right subtree.







Following the strategy above will generate this sequence:

$$
2, 4, 5, 7, 8, 9, 12, 14, 17, 20, 21, 25
$$


Explanation

We start with the root, $9$, but the strategy demands us to visit all the nodes in the left subtree first. So we move to the subtree rooted at $5$. However, before we visit $5$, we must visit all the nodes in its left subtree. So we move to the subtree rooted at $2$. Since there is no subtree to the left of $2$, we can visit $2$. Thus the first node that we will iterate over is going to be $2$. 





According to the strategy, when a node is visited, we must visit all the nodes in its right subtree. So we move to node $4$. We must visit all the nodes in the subtree to the left of $4$, but there are none, so we can process $4$ itself. Therefore, $4$ will be the second node we will iterate over. We must now visit all the nodes to the right subtree of $4$, but there is none. Therefore, we conclude our visit of all the nodes in the right subtree of $2$, and by proxy, our visit to all the nodes in the left subtree of $5$ is done too. We must now process $5$ itself. Thus, $5$ will be the third node we will iterate over. Then we move to the right subtree of $5$ and $\dots$



Here is a *recursive definition* of in-order traversal: for each node, recursively perform in-order traversal of the subtree rooted at its left child, then visit the root (the note itself), then recursively perform in-order traversal of the subtree rooted at its right child.


Resources

* [In-order tree traversal in 3 minutes](https://youtu.be/5dySuyZf9Qg) on YouTube.




# In-order Traversal: Exercise



* Trace in-order traversal.



Consider this BST





Exercise Carry out the in-order traversal. 

**Hint:** In-order strategy states for every node, visit left subtree, then the node, then right subtree.


Solution

$$
2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17
$$



# Pre-order Traversal: Root, Left, Right!



* Explain pre-order traversal.



This is the strategy for pre-order traversal:



For every node, visit it, then visit its left subtree, then visit its right subtree.







Following the strategy above will generate this sequence:

$$
9, 5, 2, 4, 7, 8, 17, 12, 14, 21, 20, 25
$$


Explanation

We start with the root, $9$; then we visit all the nodes in its left subtree. So we move to the subtree rooted at $5$. Next, we visit $5$; then we visit all the nodes in its left subtree. So, we move to $2$. First, we visit $2$, and then we visit all the nodes in its left subtree. But $2$ has no subtree to its left. So we move to visit all the nodes in the right subtree of $2$. 





So we move to node $4$. We visit $4$. Then, we must visit all the nodes in the subtree to the left of $4$, but there is none. So, we visit all the nodes to the right subtree of $4$, but there are none. Therefore, we conclude our visit of all the nodes in the right subtree of $2$, and by proxy, our visit to all the nodes in the left subtree of $5$ is done too. We must now move to the right subtree of $5$ and $\dots$



Here is a *recursive definition* of pre-order traversal: for each node, visit it, then recursively perform pre-order traversal of its children (the subtrees rooted at its left and right child, in that order).


Resources

* [Pre-order tree traversal in 3 minutes](https://youtu.be/1WxLM2hwL-U) on YouTube.







# Pre-order Traversal: Exercise



* Trace pre-order traversal.



Consider this BST





Exercise Carry out the pre-order traversal. 

**Hint:** Pre-order strategy states for every node, visit it, then visit its left subtree, then visit its right subtree.


Solution

$$
7, 2, 4, 5, 13, 10, 8, 11, 15, 14, 17
$$



# Post-order Traversal: Left, Right, Root!



* Explain post-order traversal.



This is the strategy for post-order traversal:



For every node, visit its left subtree, then visit its right subtree, then visit the node.







Following the strategy above will generate this sequence:

$$
4, 2, 8, 7, 5, 14, 12, 20, 25, 21, 17, 9
$$


Explanation

We start with the root, $9$, but we must first visit its left subtree. So we move to $5$, but we must first visit its left subtree. So we move to $2$. We must visit the left subtree of $2$, but there is none. So we move to visit the right subtree of $2$, which is rooted at $4$. We must, however, first visit the left subtree of $4$. Since there is no left subtree to $4$, we move to visit its right subtree. There is, however, no subtree to the right of $4$. So we can visit $4$ itself. Now we are done with the right subtree of $2$, so we can visit $2$ itself. 





Now we are done with the left subtree of $5$. We must visit the right subtree of $5$ now. So we move to $7$, but we must first visit its left subtree. There is no subtree to the left of $7$, though. So we can move to the right subtree of $7$. Therefore, we move to $8$. We must now visit the left subtree of $8$, but there is none. So we move to visit the right subtree of $8$, but there is none. We can, therefore, visit $8$ itself. Once $8$ is visited, we are done with the right subtree of $7$, so we can visit $7$ itself. Once we have visited $7$, we are done with the right subtree of $5$, so we can visit $5$ itself. Once we have visited $5$, we are done with the left subtree of $9$, so we must move to visit the right subtree of $9$ $\dots$



Here is a *recursive definition* of post-order traversal: for each node, recursively perform post-order traversal of its children (the subtrees rooted at its left and right child, in that order), then visit the node itself.


Resources

* [Post-order tree traversal in 2 minutes](https://youtu.be/4zVdfkpcT6U) on YouTube.


# Post-order Traversal: Exercise



* Trace post-order traversal.



Consider this BST





Exercise Carry out the post-order traversal. 

**Hint:** Post-order strategy states for every node, visit its left subtree, then visit its right subtree, then visit the node.


Solution

$$
5, 4, 2, 8, 11, 10, 14, 17, 15, 13, 7
$$



# In-order Traversal: Recursive Implementation



* Implement in-order traversal.



For BST implementation of OrderedSet ADT, we must iterate over the nodes using in-order traversal, so the outcome is "ordered" (sorted).

We will try to implement a recursive in-order traversal first before implementing the iterator since the recursive implementation is easier to understand and implement.

Assume the `Node` class was public, and we have the reference variable `root` pointing to the root of a BST. The following statement must print the values in the BST "in order":

```java
printInOrder(root);
```

Exercise Complete the implementation of `printInOrder`.

```java
public static void printInOrder(Node node) {
  // TODO Implement me!
  System.out.print(node.data + " ");
}
```


Solution

```java
public static void printInOrder(Node node) {
  if (node == null) {
    return;
  }
  printInOrder(node.left);
  System.out.print(node.data + " ");
  printInOrder(node.right);
}
```



# In-order Traversal: Iterative Implementation



* Implement in-order traversal.



Exercise Open the starter code and complete the implementation of `BstOrderedSet.iterator`.

**Hint 1:** You cannot do this recursively (in Java). The iterator needs to "pause," so to speak, and wait for a call to `next` to generate (retrieve) the next element. 

**Hint 2:** You will need an auxiliary data structure in the iterator class to keep track of some of the nodes as you traverse the tree so that you can visit those nodes later. This auxiliary data structure will make a less efficient iterator (space complexity greater than $\Omicron(1)$).  


Solution

Please refer to the posted solution code.




Resources

You may find this article on Medium useful: [Binary Search Tree Iterator](https://medium.com/algorithm-problems/binary-search-tree-iterator-19615ec585a).

The approach posted in the solution (and the article above) needs an auxiliary data structure. To perform the iteration in $\Omicron(1)$ space, you should perform Morris traversal and implement [Threaded binary tree](https://en.wikipedia.org/wiki/Threaded_binary_tree).


# Tree Terminology: Path



* Define tree-related terminologies.



Suppose $n_1, n_2, \dots, n_k$ is a sequence of nodes in a tree such that $n_1$ is the parent of $n_2$, which is the parent of $n_3$, and so on, down to $n_{k−1}$, which is the parent of $n_k$. Then $n_1, n_2, \dots, n_k$ is called a **path** from $n_1$ to $n_k$. 





For example, the sequence $9, 17, 21, 20$ is a path from root, $9$, to leaf, $20$.



The **length** of a path is $k − 1$, where $k$ is the number of nodes on it.



A path may consist of a single node ($k = 1$), in which case the length of the path is $0$.
# Tree Terminology: Height



* Define tree-related terminologies.



The **height** of a node $n$ is the _length_ of a _longest path_ from $n$ to a leaf. 







The height of a tree is the height of its root.



We can _recursively_ define the height of a node:

$$
\text{height}(n)=\left \\{ \begin{matrix} 0 & n = \text{leaf} \\\ 1 + \max(\text{height of children}) & n \neq \text{leaf} \end{matrix} \right \\}
$$

**Aside**: In some references, the height of a node includes the node.  In that case, the height of a leaf is 1.
# Tree Terminology: Depth



* Define tree-related terminologies.



The **depth**, or *level*, of a node $n$, is the length of the path from the root to $n$.







The height of a tree is the depth of its *deepest* node.



We can _recursively_ define the depth of a node:

$$
\text{depth}(n)=\left \\{ \begin{matrix} 0 & n = \text{root} \\\ 1 + \text{depth}(\text{parent}(n)) & n \neq \text{root} \end{matrix} \right \\}
$$

**Aside**: In some references, the depth of a node includes the node. In that case, the depth of the root is $1$. Some references keep the depth of the root at $0$ but define $\text{level} = \text{depth} + 1$ so the level of the root is $1$.
# Tree Terminology: Exercise I



* Define tree-related terminologies.




Consider the following tree:





Exercise Complete the table below:

|                                              |  Answer |
| :------------------------------------------- | :-----: |
| The heigh of the tree is                     |         |
| The height of node with value $4$ is         |         |
| The depth of root is                         |         |
| The depth of node with value $10$ is         |         |


Solution

|                                              |  Answer |
| :------------------------------------------- | :-----: |
| The heigh of the tree is                     |    3    |
| The height of node with value $4$ is         |    0    |
| The depth of root is                         |    0    |
| The depth of node with value $10$ is         |    2    |



# Tree Terminology: Exercise II



* Define tree-related terminologies.



Consider the following trees:

















Exercise Complete the table below:

| Tree  | Height  | is BST? |
| :---: | :-----: | :-----: | 
| **A** |         |         |
| **B** |         |         |
| **C** |         |         |


Solution

| Tree  | Height  | is BST? |
| :---: | :-----: | :-----: | 
| **A** |   6     |   Yes   |
| **B** |   2     |   Yes   |
| **C** |   6     |   Yes   |



# Perfect Binary Tree



* Explain why the height of a complete binary tree is $\Omicron(\lg N)$.



A perfect binary tree is a binary tree in which all interior nodes have two children, and all leaves have the same depth or same level







The height of a perfect binary tree is $\Omicron(\lg n)$, where $n$ is the number of nodes. 



You can verify this by looking at the example above: a perfect binary tree has $1$ node at level (depth) $0$, $2$ nodes at level $1$, $4$ nodes at level $2$, and so on. Thus, it will have $2^d$ nodes at level $d$. Adding these quantities, the total number of nodes $n$ for a perfect binary tree with depth $d$ is:

$$
n = 2^0 + 2^1 + 2^2 + \dots + 2^d = 2^{d+1} − 1
$$

For example, the perfect binary tree of depth $2$ above has $2^3 – 1 = 7$ nodes. Now, consider the formula above for the number of nodes in a perfect binary search tree:

$$
n = 2^{d+1} − 1
$$

Solving for $d$, we get:

$$
n+1 = 2^{d+1}
$$

$$
\lg(n+1) = \lg(2^{d+1})
$$

$$
\lg(n+1) = (d+1)\lg(2)
$$

$$
\lg(n+1) = (d+1)
$$

$$
d = \lg(n+1) - 1
$$

We know the height of the tree is the depth of the *deepest* node. So the height of a perfect binary tree is at most $\lg(n+1) - 1$ or $\Omicron(\lg n)$.


Resources

* [How to Sum Consecutive Powers of 2](https://jarednielsen.com/sum-consecutive-powers-2/).
* Wikipedia's entry on [Types of Binary Tree](https://en.wikipedia.org/wiki/Binary_tree#Types_of_binary_trees).
* Wikipedia's entry on [Properties of Binary Trees](https://en.wikipedia.org/wiki/Binary_tree#Properties_of_binary_trees).



# BST Analysis



* Distinguish between best-case vs. worst-case running time of operations on BST.



The BST provides a *reasonably* fast implementation of an OrderedSet ADT. 

Each of the operations `insert`, `remove`, and `has` (search) start at the root and follow a path down to, in the worst case, the leaf at the lowest level. The time complexity of each operation is, therefore, $\Omicron(h)$ where $h$ is the height of the tree.

## The Worst Case

In the worst case, all the nodes in the BST will be arranged in a single path. 





Such a tree would result, for example, from taking a list of $n$ elements in sorted order and inserting them one at a time into an initially empty tree. 

The height of a $n$-node tree like the one above is clearly $n - 1$. Therefore, the core operations of OrderedSet ADT will take $\Omicron(n)$ time on a collection of $n$ elements.

## The Best Case

In the best case, the BST is a perfect binary tree. 





The height of a $n$-node perfect binary tree like the one above is $\Omicron(\lg n)$. Therefore, the core operations of OrderedSet ADT will take $\Omicron(\lg n)$ time on a collection of $n$ elements.

**Aside:** The average running time is difficult to establish because it is not clear that all BSTs are equally likely, but it has been shown to be similar to the best case. It is beyond the scope of this course to explore the average case.



We will later learn about "Balanced Binary Search Trees" that automatically keep their height within (or close to) $\Omicron(\lg n)$ in the face of arbitrary item insertions and deletions.




Resources

* [Binary Search Tree (BST) Worst Case](https://youtu.be/-lpJhqKl6eo) on YouTube.



# Tree ADT



- Differentiate general (rooted) tree from a binary tree
- Recognize "Array of References" and "Leftmost-Child–Right-Sibling" representations of a general tree.



A general (or unrestricted) tree is one where each node has an unlimited number of children nodes.

A Tree ADT to represent a general tree may look like this:

```java
public interface Tree extends Iterable {
  int size();
  boolean empty();
  Position insertRoot(T t) throws InsertionException;
  boolean root(Position p) throws PositionException;
  Position root() throws EmptyException;
  Position insertChild(Position p, T t) throws PositionException;
  boolean leaf(Position p) throws PositionException;
  Position parent(Position p) throws PositionException;
  T remove(Position p) throws PositionException, RemovalException;
  Iterator iterator();
  Iterable> children(Position p) throws PositionException;
}
```

A Tree ADT has utility in any application that requires hierarchical representation of data:
* File directories
* Inheritance hierarchy
* Website page structure
* Book outline
* Etc

The Tree ADT can be implemented using a linked structure such as the one we have used to represent the binary search tree. The static nested `Node` class can be updated according to the following tree representations:

1. Array of References Representation

  ```java
  private static class Node {
    T data;
    ArrayList> children;
  }
  ```

2. Leftmost-Child–Right-Sibling Representation

  ```java
  private static class Node {
    T data;
    Node child;
    Node sibling;
  }
  ```
# Map ADT & BBST

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the core operations of **Map**.
* Describe the difference between similar operations (`put` and `insert`).
* Differentiate between similar data structures: Map, Dictionary, OrderedMap.
* Implement Map using an ArrayList internally.
* Implement OrderedMap using a BST (without balancing strategies).
* Describe what **balance property** is (in the context of BST structure).
* Identify the height and **balance factor** of binary search tree nodes.
* Differentiate binary trees, binary search trees, and **balanced binary search trees** based on the structure (balance) and ordering properties.

> [Starter code](../../zip/chap18-starter.zip) for this chapter.


Solution code

[Solution code](../../zip/chap18-solution.zip) for this chapter.


# Map ADT: The Abstraction



* Explain the core operations of Map ADT.



A Map is a collection of `` pairs. Keys must be **unique** and **immutable**.

Maps are also known as "dictionaries," or "associative arrays," or "symbol tables" in other contexts. 
Some references distinguish between Map and Dictionary by defining the latter as a Map in which duplicate keys are allowed.



The core operations of a Map involve "insertion," "removal," and "update" of `` pairs, as well as the _lookup_ of a value associated with a particular key.



Maps are a very popular abstraction. To understand the versatility of Maps, consider that Arrays can be seen as Maps where keys are the array indices. In a way, a Map is a (fast) "key lookup" data structure that offers a flexible means of indexing into its element. Some programming languages, such as Go and Python, have "built-in" Maps.


Resources

* Wikipedia entry on [Associative Arrays](https://en.wikipedia.org/wiki/Associative_array).
* Wikipedia entry on [Immutable Object](https://en.wikipedia.org/wiki/Immutable_object).
* [How to create Immutable class in Java?](https://www.geeksforgeeks.org/create-immutable-class-java/)
* [Sets and Maps](https://youtu.be/gmIb-qZhTDQ) short video by Udacity on YouTube.




# Map ADT: The Interface



* Explain and trace the core operations of Map ADT.




```java
/**
 * Map ADT.
 *
 * @param  Type for keys.
 * @param  Type for values.
 */
public interface Map extends Iterable {
  /**
   * Insert a new key/value pair.
   *
   * @param k The key.
   * @param v The value to be associated with k.
   * @throws IllegalArgumentException If k is null or already mapped.
   */
  void insert(K k, V v) throws IllegalArgumentException;

  /**
   * Remove an existing key/value pair.
   *
   * @param k The key.
   * @return The value that was associated with k.
   * @throws IllegalArgumentException If k is null or not mapped.
   */
  V remove(K k) throws IllegalArgumentException;

  /**
   * Update the value associated with a key.
   *
   * @param k The key.
   * @param v The value to be associated with k.
   * @throws IllegalArgumentException If k is null or not mapped.
   */
  void put(K k, V v) throws IllegalArgumentException;

  /**
   * Get the value associated with a key.
   *
   * @param k The key.
   * @return The value associated with k.
   * @throws IllegalArgumentException If k is null or not mapped.
   */
  V get(K k) throws IllegalArgumentException;

  /**
   * Check the existence of a key.
   *
   * @param k The key.
   * @return True if k is mapped, false otherwise (even for null!).
   */
  boolean has(K k);

  /**
   * Number of mappings.
   *
   * @return Number of key/value pairs in the map.
   */
  int size();
}
```

Notes:

* Map uses two generic variables, `K` and `V`.
* Keys must be unique, not null, and should be immutable; values have
no constraints. 
* We use Java's `IllegalArgumentException` instead of providing our specific exception classes; we do this mainly to show you yet another style of interface design.
* There is a subtle difference between `put` and `insert` where the former updates existing key/value pairs.
* The interface extends `Iterable`, which means the iterator will iterate over the keys.
# Array-based Implementation



* Implement Map using an ArrayList internally.



Open `ArrayMap` in the starter code. This implementation uses Java's ArrayList internally. It is inefficient; all operations take $\Omicron(n)$ time. The `iterator()` method makes a copy of all keys, so it takes $\Omicron(n)$ extra time and space to initiate the iteration (although `next` and `hasNext` are $\Omicron(1)$). 

Notice the `ArrayMap` class uses the following structure:

```java
// Entry to store a key and a value pair.
private static class Entry {
  K key;
  V value;

  Entry(K k, V v) {
    this.key = k;
    this.value = v;
  }
}
```

We could have used two arrays in parallel and manage insert/remove ourselves (instead of delegating it to ArrayList). 

```java
private K[] keys;
private V[] values;
```

You should probably think about this alternative implementation as it makes for a great exam question! 

Use `ArrayMapTest` to test any alternative implementation of `ArrayMap`. The `ArrayMapTest` extends `MapTest`. Take a moment and review the tests which we have provided in the starter code.



Efficient implementations of Map are done either as a "*hash table*" or a "*search tree*." 



We will explore these implementations in the several following chapters. 
# OrderedMap ADT: The Interface



* Explain and trace the core operations of OrderedMap ADT.



A binary search tree implementation of Map uses the "keys" to organize the entries in a tree structure. Therefore, the "keys" must be **comparable**. Like Set and OrderedSet, we define the OrderedMap ADT along with the Map ADT.


```java
/**
 * OrderedMpp ADT.
 *
 * @param  Type for keys.
 * @param  Type for values.
 */
public interface OrderedMap, V>
    extends Map {
}
```

In an OrderedMap, its iterator is expected to go over the keys *in order* (according to the keys' natural ordering). This expectation is specified in `OrderedMapTest` (see the starter code).
# `BinarySearchTreeMap`



* Implement OrderedMap using a BST (without balancing strategies).



Exercise Complete the implementation of `BinarySearchTreeMap`.

**Suggestion:** Refer to the BST implementation of OrderedSet ADT from previous lectures and use it as a starting point. 


Solution

See the posted solution code.



# Efficiency



* Recall efficiency of BST operation.



In earlier chapters, when we analyzed the performance of OrderedSet operations implemented as a Binary Search Tree, we established:



Each of the operations starts at the root and follows a path down to potentially the leaf at the deepest level. Therefore, the time complexity of each operation is $\Omicron(h)$, where $h$ is the height of the tree.



In the **worst-case**, all the nodes in the BST will be arranged in a single path. Therefore, the core operations take $\Omicron(n)$ time on a collection of $n$ elements.

In the **best-case**, the BST is a full binary tree with the height of $\Omicron(\lg n)$. Therefore, the core operations take $\Omicron(\lg n)$ time on a collection of $n$ elements.
# Balanced BST



* Describe what balance property is in the context of BST structure.



For efficiency, we shall restrict the height of a BST so that it is $\Omicron(\lg n)$ instead of the worst-case $\Omicron(n)$.

**Balancing to the rescue:** The tree structure is *adjusted* as necessary to maintain a better balance and resulting height.

A BST is balanced when it has the following property:



For any node, the heights of the node's left and right subtrees are either equal or differ by at most $1$.



The above is often referred to as the **balance property**. Thus, a BST that maintains the balance property is a balanced BST or simply a **BBST**. 

# Balance Factor



* Identify the height and balance factor of binary search tree nodes.



To assist in establishing _balance property_, we define the following measure:



Each node's **balance factor** is the height of the left subtree minus the height of the right subtree.



More specifically

```text
bf(node) = height(node.left) – height(node.right)
```

For the calculation of _balance factor_, we define "height" as follows:

$$
\text{height}(n)=\left \\{ \begin{matrix} -1 & n = \text{null} \\\ 0 & n = \text{leaf} \\\ 1 + \max(\text{height of children}) & n \neq \text{leaf} \end{matrix} \right \\}
$$



A BST is balanced when, for any node, its balance factor is $1$, $0$, or $-1$.



Here is an example





The above BST is not balanced because $\text{bf}(\\{ 10, 5 \\})\notin \\{0, 1, -1 \\}$.
# Exercise I



* Differentiate binary trees, binary search trees, and balanced binary search trees based on the structure (balance) and ordering properties.



Consider the following tree:

```text
             20
            /  \
          10    30
               /  \
              5   40
```

Exercise Is this a binary tree, BST, or BBST? Why?


Solution

This tree is a binary tree. The $5$ violates the BST property because it is smaller than $20$ (root), yet it is on the right subtree. All values in the right subtree should be greater than the current node (root in this case). 

If it were a BST, it would have been a BBST because, for any node, its balance factor is $1$, $0$, or $-1$.



Exercise Using the following numbers $\\{4, 1, 3, 9, 29, 30, 27, 21\\}$, construct a binary tree (that is not a BST or BBST), a BST (that is not a BBST), and a BBST. 


Solution

Answers may vary. 

Binary tree example: should only satisfy the property that each node has at most two children.

```text
         1 
        / \
       3   29
      / \
     4  21
    / \
   30  9
      /
     27
```

BST example: should satisfy the binary tree property and the BST property.

```text
         21
      /      \
     1       27
      \        \
       3       29
        \       \
         4       30
          \
           9
```

BBST example: needs to satisfy the binary tree property, BST property, and BBST height property.

```text
        9
     /     \
   3        27
 /   \     /   \
1     4   21   29
                 \
                  30 
```



# Exercise II



* Differentiate binary trees, binary search trees, and balanced binary search trees based on the structure (balance) and ordering properties.




Exercise Could this tree be a BBST? If not, list every instance of all BBST violations by indicating roots of all non-BBST compliant subtrees.






Solution

A violation of the order property can be seen through an in-order traversal: \
A, C, B, D, E, F, G, M, I, J, M, L, N, K, O, P

A violation of the balance property exists in nodes D, A, and G: 






# Height is $\Omicron(\lg N)$



* Show the height of BST that maintains balance property is $\Omicron(\lg N)$.






This section is an optional reading!



In a BBST, unlike a regular BST, it is *guaranteed* that the tree's height is $\Omicron(\lg N)$. This claim can be easily observed for a perfect BST, where each internal node has two children and all the leaves are at the same level. 





Let's observe the perfect BSTs above

| Height | Nodes | Binary Log Calculation |
|:------:|:-----:|:----------------------:|
|  $0$   |  $1$  |     $\log_2 1 = 0$     |
|  $1$   |  $3$  |     $\log_2 3 = 1$     |
|  $2$   |  $7$  |     $\log_2 7 = 2$     |
|  $3$   | $15$  |    $\log_2 15 = 3$     |

Given these perfect BSTs, one can intuit how this holds more generally. 

First, convince yourself that the worst height of a BST is when it has the minimum number of nodes concerning its height. Then, let's call $N(h)$ the minimum number of nodes given height. 

> The worst case $h \in \Omicron(n)$ is when $N(h) = h+1$.

If we find the upper bound of $N(h)$ for BBST, then we've bounded the height of all BBSTs. 

Let's consider a BBST of height $h \geq 3$ and the minimum number of nodes $n=N(h)$. This tree is composed of a root and two subtrees. Since the whole tree has the minimum number of nodes for its height, so do the subtrees. For the big tree to be of height $h$, one of the subtrees must be of height $h-1$. To get the minimum number of nodes, the other subtree is of height $h-2$.



Why can't the other subtree be of height $h-3$ or $h-4$?


Because the balance property requires the height of siblings to differ by at most $1$. 




So far, we established for $h \geq 3$,

$$
N(h) = 1 + N(h-1) + N(h-2)
$$

We know

$$
N(h) > N (h-1)​ \space \text{and} \space N(h-1) > N(h-2)​
$$

therefore

$$
N(h) > N(h-1) + N(h-2) > 2\times N(h-2)
$$

If $N(h) > 2\times N(h-2)$ then $N(h-2) > 2\times N(h-4)$, therefore

$$
N(h) > 2\times N(h-2) > 4 \times N(h-4)
$$

We can keep doing this

$$
N(h) > \dots > 4\times N(h-4) > 8\times N(h-8) > \dots
$$


What does the above expression demonstrate? Why is that significant?

This expression demonstrates that the number of nodes at least doubles each time the height increases by a factor of $2$. This observation is important because $N(h)$ is exponential in $h$. Therefore $h$ is logarithmic in the number of nodes.



Continuing the proof, we see that this can be written as: 

$$
N(h) > 2^i \times N(h - 2i)
$$

The largest $i$ we can put in there is $i = \frac{h}{2} - 1$:



Why is this value of $i$ chosen as $\frac{h}{2} - 1$?


This allows the expression to be equal to either $1$ or $2$, depending on if $h$ is even or odd. 
  * Even $h$: $h - 2i = h - (h - 2) = 2$
  * Odd $h$: $h - 2i = h - (2(\frac{h}{2}) - 2) = h - ((h + 1)-2) = 1$



So we have:

$$
N(h) > 2^i \times (h-2i) > 2^{\frac{h}{2}} \times N(h - h) > 2^{\frac{h}{2}}
$$



So we showed the minimum number of nodes, $n=N(h)$ in a BBST of height $h$, is **at least** $2^{\frac{h}{2}}$:  

$$
n > 2^{\frac{h}{2}}
$$



What does this say about height, $h$? First, let's take the logarithm of both sides:

$$
\lg n > \lg 2^{\frac{h}{2}} \Rightarrow h < 2\lg(n)
$$

So, $h \in \Omicron(\lg n)$.
# AVL Tree

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Elaborate on the purpose of **structural rotation**.
* Describe, trace and implement structural rotations: **single right, single left, double right-left, and double left-right**.
* Explain and trace the balancing operations of an **AVL tree**.
* Select the appropriate rotation type to rebalance an AVL tree after an insertion/removal operation is performed.
* Implement the core operations of OrderedMap efficiently with an AVL tree.
* Analyze the time/space efficiency of an AVL tree.

> This chapter does not have a starter/solution code because a homework is about implementing an AVL tree.
# Structural Rotation: Definition



* Elaborate on the purpose of structural rotation.



Consider these numbers: $3, 5, 7$. The following are valid binary search trees made up of the given numbers. However, only one is "balanced."









The four unbalanced trees can be _transformed_ to the balanced one using **structural rotation**. (Notice the balanced one can be generated from the sequence $5, 3, 7$ or $5, 7, 3$.)



A structural rotation is an operation on a binary tree that changes the tree's structure without affecting the order of the elements (as read in by an in-order traversal).



A structural rotation is often employed to balance two branches of different depths. 

In a structural rotation, one node is shifted up, another is shifted down, and some of the remaining nodes may be shifted to ensure the tree is still a binary tree (each node only has two branches). 



Structural rotations change the height of the tree by moving up larger subtrees.




# Single Right Rotation



* Describe Single Right Rotation.



Consider inserting the values $7, 5, 3$ in that order. 

We start by inserting $7$:





We have a BST with a single node; it is balanced! 

Next, we insert $5$.





Now our BST has two nodes. Notice the height and balance factor of the root has changed (and it is still balanced).

Next, we insert $3$.





Our BST has three nodes now. Notice the heights and balance factors of the parent and _grandparent_ of $3$ have changed. In particular, the grandparent (the root) is not balanced anymore! 

However, if we were to push $7$ to the **right** of $5$, we would restore balance:





This is called a (single) **right rotation**:





Notice the violation of balance property occurred in the grandparent of the newly inserted node. From the perspective of the grandparent node, the problem was in the **left child's left subtree**. The solution is a (single) right rotation to bring the parent node (_median_ value) above the grandparent (_high_ value). 
# Right Rotation: Exercise I



* Trace Single Right Rotation.



Consider the following BST:





Exercise Insert the value $4$ and apply a structural rotation to balance the BST if needed.


Solution

Let's observe the original BST is balanced:





Here is the BST after insertion:





Notice the violation of balance property occurs in the grandparent of the newly inserted node. From the perspective of the grandparent node, the problem is in the **left child's left subtree**. This is the pattern that requires (single) right rotation to bring the parent node (_median_ value) above the grandparent (_high_ value).  






# Right Rotation: Exercise II



* Trace Single Right Rotation.



Consider the following BST:





Exercise Remove the value $11$ and apply a structural rotation to balance the BST if needed.


Solution

Let's observe the original BST is balanced:





Here is the BST after removal:





Notice the violation of the balance property occurs in the parent of the deleted node. From the perspective of the parent node, the problem is in the **left child's left subtree**. This is the pattern that requires a (single) right rotation to bring the left child (_median_ value) above the parent (_high_ value).  






# Right Rotation: Exercise III



* Trace Single Right Rotation.



Consider the following BST:





Exercise Insert the value $3$ and apply a structural rotation to balance the BST if needed.


Solution

Let's observe the original BST is balanced:





Here is the BST after insertion:





Notice the violation of balance property occurs in the great-grandparent of the inserted node. From the perspective of the great-grandparent node, the problem is in the **left child's left subtree**. This is the pattern that requires a (single) right rotation. **However, this case is a bit tricky:**

> When you bring $7$ to be the parent of $14$, the latter needs to go to the right of $7$. However, $11$ is already to the right of $7$. So we need to attach $11$ to the left of $14$ (where $7$ used to be).






# Right Rotation: Exercise IV



* Implement Single Right Rotation.



Consider the schematic representation of the pattern that leads to a (single) right rotation:





The nodes with dashed lines are roots of their own subtree (they could be null too). After the application of a right rotation, we get the following:





Exercise Complete the implementation of `rightRotation` method:

```java
Node rightRotation (Node root) {
  // TODO Implement me!
}
```

**Note:** The argument to `rightRotation` is the root of a subtree (not necessarily the root of the BST). The `rightRotation` must return the updated (new) root. Assume each node has access to its left/right subtrees.


Solution

```java
Node rightRotation (Node root) {
  Node child = root.left;
  root.left = child.right;
  child.right = root;
  return child;
}
```



Exercise What is the time complexity of your implementation of `rightRotation` method?


Solution

The `rightRotation` involves a constant number of assignments. Therefore, its time complexity is $\Omicron(1)$








# Single Left Rotation



* Describe Single Left Rotation.



Consider inserting the values $3, 5, 7$ in that order. 

We start by inserting $3$:





We have a BST with a single node; it is balanced! 

Next, we insert $5$.





Now our BST has two nodes. Notice the height and balance factor of the root has changed (and it is still balanced).

Next, we insert $7$.





Our BST has three nodes now. Notice the heights and balance factors of the parent and _grandparent_ of $7$ have changed. In particular, the grandparent (the root) is not balanced anymore! 

However, if we were to push $3$ to the **left** of $5$, we would restore balance:





This is called a (single) **left rotation**:





Notice the violation of balance property occurred in the grandparent of the newly inserted node. From the perspective of the grandparent node, the problem was in the **right child's right subtree**. The solution is a (single) left rotation to bring the parent node (_median_ value) above the grandparent (_high_ value). 
# Left Rotation: Exercise V



* Trace Single Left Rotation.



Consider the following BST:





Exercise Start from the leaf (6) and visit the parent nodes back to the root. As soon as you see an imbalanced node, apply rotation to fix it.


Solution

Let's note the heights and balance factors in the original BST:





We can perform a left rotation to bring $4$ to the left of $5$:





We then perform another left rotation to bring $3$ to the left of $5$: 






# Double Rotation: Right-Left



* Describe Double Right-Left Rotation.



Consider inserting the values $3, 7, 5$ in that order. 

We start by inserting $3$:





We have a BST with a single node; it is balanced! 

Next, we insert $7$.





Now our BST has two nodes. Notice the height and balance factor of the root has changed (and it is still balanced).

Next, we insert $5$.





Our BST has three nodes now. Notice the heights and balance factors of the parent and _grandparent_ of $5$ have changed. In particular, the grandparent (the root) is not balanced anymore! 

A single rotation will not restore the balance! However, if we were to push $7$ to the **right** of $5$, we would _transform_ the structure to a pattern we have seen before:






The above is a (single) left rotation away from being balanced!







So we perform a **left** rotation:





As you have noticed, we needed two rotations, first a right and then a left rotation. This is called a (double) **right-left rotation**.





Notice the violation of balance property occurred in the grandparent of the newly inserted node. From the perspective of the grandparent node, the problem was in the **right child's left subtree**. The solution is a (double) right-left rotation to bring the _median_ value above the high and low values. 
# Double Rotation: Left-Right



* Describe Double Left-Right Rotation.



Consider inserting the values $7, 5, 3$ in that order. 

We start by inserting $7$:





We have a BST with a single node; it is balanced! 

Next, we insert $3$.





Now our BST has two nodes. Notice the height, and balance factor of the root has changed (and it is still balanced).

Next, we insert $5$.





Our BST has three nodes now. Notice the heights and balance factors of the parent and _grandparent_ of $5$ have changed. In particular, the grandparent (the root) is not balanced anymore! 

A single rotation will not restore the balance! However, if we were to push $3$ to the **left** of $5$, we would _transform_ the structure to a pattern we have seen before:






The above is a (single) right rotation away from being balanced!







So we perform a **right** rotation:





As you have noticed, we needed two rotations, first a left and then a right rotation. This is called a (double) **left-right rotation**.





Notice the violation of balance property occurred in the grandparent of the newly inserted node. From the perspective of the grandparent node, the problem was caused in **left child's right subtree**. The solution is a (double) left-right rotation to bring the _median_ value above the high and low values. 
# Double Rotation: Exercise VI



* Trace Double Right-Left Rotation.



Consider the following BST:





Exercise Insert the value $23$ and apply a structural rotation to balance the BST if needed.


Solution

Let's observe the original BST is balanced:





Here is the BST after insertion:





Notice the violation of balance property occurs in the great-grandparent of the newly inserted node. From the perspective of the grandparent node, the problem is in the **right child's left subtree** (the highlighted nodes). This is the pattern that requires (double) right-left rotation to rebalance.  

So we perform a right rotation to bring $22$ above $25$ (to bring the median value on a level between the low and high values):





And then a left rotation to bring $22$ above $20$ (to bring the median value on a level above the low and high values):






# Double Rotation: Exercise VII



* Trace Double Left-Right Rotation.



Consider the following BST:





Exercise Remove the value $13$ and apply a structural rotation to balance the BST if needed.


Solution

Let's observe the original BST is balanced:





Here is the BST after removal:





Notice the violation of balance property occurs in the parent of the node which was removed. From the perspective of the parent node, the problem is in the **left child's right subtree**. (the highlighted nodes) This is the pattern that requires (double) left-right rotation to rebalance.  

So we perform a left rotation to bring $10$ above $9$ (to bring the median value on a level between the low and high values):





And then a right rotation to bring $10$ above $12$ (to bring the median value on a level above the low and high values):






# Putting it all together!



* Select the appropriate rotation type to rebalance a BST after an insertion/removal operation is performed.



This is called a (single) **right rotation**:







Cause of imbalance: **left child's left subtree**.



This is called a (single) **left rotation**:







Cause of imbalance: **right child's right subtree**.



This is called a (double) **right-left rotation**:







Cause of imbalance: **right child's left subtree**.



This is called a (double) **left-right rotation**:







Cause of imbalance: **left child's right subtree**.



Exercise Complete the following table which assists in determining the type of rotation.

| Imbalance Node  | Child's `bf = -1`  (right heavy) | Child's `bf = 1`  (left heavy) |
| :-------------: | :----------------------------------: | :-------------------------------:  |
| **`bf = -2`  (right heavy)** |   |   |
| **`bf = 2`  (left heavy)** |   |   |



Solution

| Imbalance Node  | Child's `bf = -1`  (right heavy) | Child's `bf = 1`  (left heavy) |
| :-------------: | :----------------------------------: | :-------------------------------:  |
| **`bf = -2`  (right heavy)** |  Left |  Right-Left |
| **`bf = 2`  (left heavy)** |  Left-Right |  Right |

**Caution** The table above does not account for an edge case where the child's balance factor is $0$. 




Schematic representation of tree rotations








Resources

- Wikipedia's entry on [AVL Tree: Rebalancing](https://en.wikipedia.org/wiki/AVL_tree#Rebalancing).
- Wikipedia's entry on [Tree Rotation](https://en.wikipedia.org/wiki/Tree_rotation).



# Structural Rotation: Exercise



* Trace the balancing operations of an AVL tree.



Exercise Starting from an empty BST, perform the following operations in order. Apply structural rotations, if needed, to keep the BST balanced.

```text
insert(15)
insert(20)
insert(24)
insert(10)
insert(13)
insert(7)
insert(30)
insert(36)
insert(25)
remove(24)
remove(20)
```


Solution

Visit [this interactive visualizer](https://www.cs.usfca.edu/~galles/visualization/AVLtree.html) and carry out the operations.

The final BST should look like this:







# AVL Tree



* Explain the core operations of the AVL tree.





AVL Tree is named after its inventors [Adelson-Velskii](https://en.wikipedia.org/wiki/Georgy_Adelson-Velsky), and [Landis](https://en.wikipedia.org/wiki/Evgenii_Landis). It is a **self-balancing** binary search tree that maintains the height balance property (in addition to the BST order property) using structural rotations.



The insertion and deletion operations for AVL trees begin similarly to the corresponding procedures for (regular) binary search trees. It then performs *post-processing* structural rotations to restore the balance of any portions of the tree that are adversely affected by the insertion/removal.

**Insertion**
- Insert an item as in a (regular) binary search tree.
- Starting with the new node, go up to update heights.
- If you find a node that has become unbalanced, do a rotation to rebalance it.
- Rebalancing reduces the height of a subtree by $1$, so there is no need to propagate up! That is, only one adjustment is needed at most because the tree was previously balanced before the insert.
- So, in total, it takes $\Omicron(\lg N)$ to insert and $\Omicron(1)$ to rebalance.

**Removal**
- Remove an item as in a (regular) binary search tree.
- Starting with the parent of the deleted node, go up to update heights.
  - By "deleted node," I mean the _actual_ deleted node (which is going to be a leaf), not the node whose value was replaced with in-order predecessor/successor.
- As you go up from the deleted node to update the heights of its ancestors, rebalance them as needed (perform rotation).
- Rotations may reduce the height of a subtree, causing further imbalances for ancestors, so you may need to perform more than one rebalancing operation.
- Since you travel from the deleted node upwards to the root, at most, you will perform $\Omicron(\lg n)$ rotations.
- So, in total, $\Omicron(\lg n)$ to remove and $\Omicron(\lg n)$ to rebalance.

**Recommendation**: Implement insertion/removal (with structural rotations) recursively!



For the implementation of the AVL Tree, you must store the height of each node.



As you insert/remove, you must travel upward toward the root to update the ancestors' height. 


Resources

* Wikipedia's entry on [AVL Tree](https://en.wikipedia.org/wiki/AVL_tree).
* [VisualAlgo interactive demo for BST and AVL Tree](https://visualgo.net/bn/bst).



# Priority Queue

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Describe what Priority Queue is and how it is different from standard Queue.
* Describe the core operations of Priority Queue.
* Contrast the efficiency of alternative implementation approaches (e.g., sorted/unsorted sequence vs. binary heap).
* Enumerate the structure and order properties of a **binary heap**.
* Differentiate binary search trees from binary heaps.
* Explain how a binary heap can be represented using a (ranked) array.
* Explain and trace the core operations of Priority Queue with Binary Heap implementation.
* Understand the operations of Binary Heap well enough to implement it. 
* Explain the role of the overloaded non-default constructor of PriorityQueue.
* Describe how the "natural ordering" of objects is established in Java.
* Explain the difference between `Comparable` and `Comparator` interfaces.

> [Starter code](../../zip/chap20-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap20-solution.zip) for this chapter.


# PriorityQueue ADT



* Describe what Priority Queue is and how it is different from standard Queue.



A Priority Queue is a variation of a standard queue where instead of being a "first-in-first-out" data structure, values come out in order of highest priority. 

The "priority" by default is based on the natural ordering of the elements. In some implementations, if priority is the same for two elements, they are removed (dequeued) based on "first come, first serve." 





Image is taken from ["Can flipping the queue spare you time?"](https://www.networkpages.nl/can-flipping-the-queue-spare-you-time/).




In this chapter, we will explore a very interesting data structure called **binary heap** that enables us to efficiently implement PriorityQueue ADT.

# PriorityQueue Interface



* Describe the core operations of Priority Queue.



Here is an interface for the PriorityQueue ADT.

```java
/**
 * Priority Queue of ordered values.
 *
 * @param  Element type.
 */
public interface PriorityQueue> {
  /**
   * Insert a value.
   *
   * @param t Value to insert.
   */
  void insert(T t);

  /**
   * Remove best value.
   *
   * @throws EmptyException If queue is empty.
   */
  void remove() throws EmptyException;

  /**
   * Return best value.
   *
   * @return best value in the queue.
   * @throws EmptyException If queue is empty.
   */
  T best() throws EmptyException;

  /**
   * Check if no elements present.
   *
   * @return True if queue is empty, false otherwise.
   */
  boolean empty();
}
```

Notice the elements of PriorityQueue must be Comparable. 



By default, a PriorityQueue will order (prioritize) the elements based on their _natural ordering_. 




What is "natural" ordering?



Natural ordering, here, means the default ordering of objects of a specific type when they are sorted in an array or a collection. 



For example, the natural ordering of `String` objects is alphabetic order. On the other hand, the natural ordering of `Date` objects is chronological order. 

Java allows you to define the natural ordering of objects of a specific type by implementing `Comparable` interface.



A class that implements the `Comparable` interface is said to have a natural ordering. And, the `compareTo()` method is called the natural comparison method.






Where are the "priorities"?

We are taking the natural ordering of elements to indicate their priority! For example, the smallest element has the least priority and the largest element has the highest priority. 

We could provide the priorities separately. Then, the interface would look like this:

```java
/**
 * Priority Queue of ordered values.
 *
 * @param  Priority type.
 * @param  Element type.
 */
public interface PriorityQueue, T> {
  /**
   * Insert a value.
   * @param t Priority associated with the value.
   * @param t Value to insert.
   */
  void insert(P p, T t);

  // Other operations are not shown!
```

In the case above, the priorities are associated with values just like "keys" are associated with values in an ordered dictionary (why not an ordered map?). However, we don't have a method to get a value given its priority! The priority queue is more of a "restricted access" data structure like a queue and stack. 



The `best` can be defined to return the largest (maximum) or smallest (minimum) value. By a contract, we may define `best` to return the largest value.

As a further clarifying point, recall a queue is not a set, so duplicate values are allowed.
# PriorityQueue Implementation



Contrast the efficiency of alternative implementation approaches.



Suppose we want to implement the operations of PriorityQueue using a linked list or an array-based implementation. 

Exercise Provide an example of core operations, for each underlying data structure, where the operation cannot be implemented better than $\Omicron(n)$. 

**Hint**: We have, generally, two choices: keeping the underlying data ordered (sorted) or not.


Solution

Keeping the underlying data sorted will enable us to perform `remove` and `best` in constant time. However, the `insert` will be $\Omicron(n)$ since an array implementation will require shifting elements around. Even a linked list implementation will require a linear search to find where to insert. 

Keeping the underlying data unordered (unsorted) will require us to perform a linear search for finding/removing the `best` (whether we use an array or a linked list implementation).



# PriorityQueue: Tree Implementation



* Enumerate the structure and order properties of the binary heap.



We are going to explore a tree-based implementation of PriorityQueue. This implementation is called a **Binary Heap** (or simply a Heap).

A heap has the following properties:

* **Structure (shape) property**: heap is a *complete* binary tree.



The structure property implies the height of the tree is $\Omicron(\lg n)$.1



* **Order (heap) property**: the element stored at each node has a higher priority (is considered "better") than its children.



The order property implies the "best" element is always at the root.



---



1 We know the height of a *perfect* binary tree is $\Omicron(\lg n)$. We should prove the complete binary tree also has a logarithmic height. We'll leave that to you to justify this. The proof is not difficult but it is beyond the scope of this course.  


# Heap Structure Property



* Describe the structure property of a binary heap.



A **complete binary tree** is one where the tree is "full" on all levels except possibly the last, and the last level has all its nodes to the left side. (Contrast this with a _perfect_ binary tree that you have learned about in an earlier chapter.)

Exercise Which of the following is a complete binary tree?






Solution

The two on the top are complete binary trees, but the bottom two are not. The bottom left one does not have all its nodes at the last level to the left side. In the bottom right one, level 2 is not full!




# Heap Order Property



* Describe the order property of a binary heap.



The ordering can be one of two types:

* The **min-heap** property: the value of each node is less than or equal to the value of its children, with the minimum-value element at the root.\
(Here, "best" means "smallest.")

* The **max-heap** property: the value of each node is greater than or equal to the value of its children, with the maximum-value element at the root.\
(Here, "best" means "largest.")

Exercise Which of the following is a valid binary **max** heap?






Solution

The two on the top are valid binary max heaps, but the bottom two are not. The bottom left is a min-heap. The bottom right violates the heap order property (200 should be at the root).


# Ranked Array Representation



* Explain how a binary heap can be represented using a (ranked) array.



A complete binary tree can be uniquely represented by storing its level order traversal in an array.





The root is the second item in the array. We skip the index zero cell of the array for the convenience of implementation. Consider $k^{\text(th)}$ element of the array, 

* its left child is located at `2 * k` index
* its right child is located at `2 * k + 1` index
* its parent is located at `k / 2` index

Exercise Do we have to skip index zero?


Solution

No! We could start at index zero. However, the formulas for getting children and parents would be different (how?).



# Best Operation



* Trace the best operation of Priority Queue with Binary Heap implementation.



As stated earlier, the "best" element is always at the root of a binary heap implementation of PriorityQueue ADT. 





Therefore `best()` is $\Omicron(1)$ operation returning the value stored at root.

# Insert Operation



* Trace the insert operation of Priority Queue with Binary Heap implementation.



Consider we have the following _min_-heap:





To insert a new element, such as $7$, we initially appended it to the end of the heap. (So it goes to the last level, after the last element from left to right.)





We may have to "repair" the heap property by percolating the new node to its proper position. This process is often called **swim-up**. It involves comparing the added element with its parent and moving it up a level (swapping with the parent) if needed. 





The comparison is repeated until the parent is smaller than or equal to the percolating (swimming) element.





The worst-case runtime of the algorithm is $\Omicron(\lg n)$, since we need at most one swap on each level of a heap on the path from the inserted node to the root.
# Remove Operation



* Trace the removal operation of Priority Queue with Binary Heap implementation.



Consider we have the following _min_-heap:





To remove the best, that is, to remove the minimum element, we can replace the root with the last element of the heap.





Then, we will delete the last element.





Finally, we restore the heap property by percolating the new root down. This process is often called **sink-down**. It involves comparing the added element with its children and moving it down a level (swapping with the smaller of the two children) if needed. 





The process is repeated until the children are smaller than or equal to the percolating (sinking) element.





Or until the percolating (sinking) element reaches the deepest level.





Similar to insertion, the worst-case runtime for removal is $\Omicron(\lg n)$ since we need at most one swap on each heap level on the path from the root node to the deepest level.
# Heap Operations: Exercise



* Trace the core operations of Priority Queue with Binary Heap implementation.



Exercise Consider an empty _min_-heap. Show (draw) the resulting heap in tree form after each of the following operations:

```text
insert(10)
insert(5)
insert(8)
insert(1)
insert(14)
remove()
insert(12)
insert(3)
insert(7)
remove()
```


Solution







# Overloaded Constructors



* Explain the role of the overloaded non-default constructor of PriorityQueue.





Any implementation of `PriorityQueue` *must* provide two constructors: a default constructor (with no argument) and a non-default one which allows a Comparator to be provided to _overwrite_ the natural ordering of the element types. 



Consider the following example!

```java
/**
 * Priority queue implemented as a binary heap.
 *
 * @param  Element type.
 */
public class BinaryHeapPriorityQueue>
    implements PriorityQueue {

  private Comparator cmp;

  /**
   * A binary heap using the "natural" ordering of T.
   */
  public BinaryHeapPriorityQueue() {
    this(new DefaultComparator<>());
  }

  /**
   * A binary heap using the given comparator for T.
   *
   * @param cmp Comparator to use.
   */
  public BinaryHeapPriorityQueue(Comparator cmp) {
    this.cmp = cmp;
    // TODO
  }

  @Override
  public void insert(T t) {
    // TODO
  }

  @Override
  public void remove() throws EmptyException {
    // TODO
  }

  @Override
  public T best() throws EmptyException {
    // TODO
    return null;
  }

  @Override
  public boolean empty() {
    // TODO
    return false;
  }

  // The default comparator uses the "natural" ordering.
  private static class DefaultComparator>
      implements Comparator {
    public int compare(T t1, T t2) {
      return t1.compareTo(t2);
    }
  }

}
```

Notice the use of `Comparator`! Java's `Comparator` is an interface that defines a `compare(T o1, T o2)` method with two arguments that represent compared objects.

In the next sections, we will work with a contrived example to clarify how the *Comparator* works and how it is different from *Comparable*.
# The Submission Class



* Motivate the need for defining the natural ordering of types.



Open `Submission.java` in the `demo` package of the starter code. The `Submission` class represents a (homework) submission made by a student. Among its attributes are:

```java
private int positionInQueue;
private String student;
private int numPriorSubmission;
```

All submissions are positioned one after another in the order they are received. Thus, submissions are *comparable* to one another, based on the order they were received (their position in the submission queue).

```java
@Override
public int compareTo(Submission other) {
  return this.positionInQueue - other.positionInQueue;
}
```

We will use the `Submission` class to motivate the idea of a priority queue.

# Sorting Submissions



* Describe how the "natural ordering" of objects is established in Java.



Suppose we have a collection of submissions. We want to sort the collection according to the "natural ordering" of its elements. 



Natural ordering, as noted before, means the default ordering of objects of a specific type when they are sorted in an array or a collection. 



For example, the natural ordering of `String` objects is alphabetic order. On the other hand, the natural ordering of `Date` objects is chronological order. 

Java allows you to define the natural ordering of objects of a specific type by implementing `Comparable` interface.



A class that implements the `Comparable` interface is said to have a natural ordering. And, the `compareTo()` method is called the natural comparison method.



So the natural ordering of `Submission` is based on the order they were received, as it is defined in the `Submission.compareTo` method.

Suppose we instantiate several objects of type `Submission` and store them in a Java Collection such as a `List`. In that case, we can use `Collections.sort` to order (sort) the submissions based on their natural ordering.

An example is provided in the `Main.java` in the `demo` package of the starter code.

```java
List students = getStudents();
List submissions = getSubmissions(students);

System.out.print("Submission sorted (natural ordering):");
Collections.sort(submissions);
System.out.println(submissions);
```
# Prioritizing Submissions



* Motivate the need for overriding the natural ordering of types.



A submission that was made earlier will be graded earlier. This behavior is an example of a first-in-first-out situation. Therefore, a queue will be a natural choice to store submissions.

Imagine we want to *prioritize* the grading of submissions on AutoGrader. We want to consider the number of prior submissions by a student, in addition, or instead of, the order in which the submission was received. For the sake of this contrived example, assume the fewer prior submissions you've made, the higher the priority of your submission to get AutoGrader processing time.

Based on the number of prior submissions, we can sort (order) the submissions using `Collections.sort`, but we need to _overwrite_ the natural ordering of `Submission` objects. In Java, you can do this by providing a `Comparator` to the `Collections.sort`. An example is provided in the `Main.java` in the `demo` package of the starter code.

```java
List students = getStudents();
List submissions = getSubmissions(students);

System.out.print("Submission sorted (priority ordering):");
Collections.sort(submissions, new LessSubmissionHigherPriority());
System.out.println(submissions);
```

where `LessSubmissionHigherPriority` is defined as:

```java
private static class LessSubmissionHigherPriority 
  implements Comparator {

  @Override
  public int compare(Submission s1, Submission s2) {
    return s1.getNumPriorSubmission() - s2.getNumPriorSubmission();
  }
}
```
# Java's Comparator



* Explain the difference between Comparable and Comparator interfaces.



Java's `Comparator` is an interface that defines a `compare(T o1, T o2)` method with two arguments that represent compared objects.

The `compare` method works similarly to the `Comparable.compareTo()` method:

* return $0$ when two objects `o1` and `o2` are equal.
* return a value $> 0$ when the first object is greater than the second one.
* return a value $< 0$ when the first object is less than the second one.

So to give a higher "order" to a student with fewer prior submissions, we must create the following Comparator:

```java
private static class LessSubmissionHigherPriority 
  implements Comparator {

  @Override
  public int compare(Submission s1, Submission s2) {
    return s1.getNumPriorSubmission() - s2.getNumPriorSubmission();
  }
}
```

We can (just to have fun) create a Comparator to reverse the natural ordering of submissions:

```java
private static class ReverseNaturalOrdering 
  implements Comparator {

  @Override
  public int compare(Submission s1, Submission s2) {
    // notice s2 is compared to s1, the order matters!
    return s2.compareTo(s1);
  }
}
```

The following Comparator would meet the natural ordering of submissions:

```java
private static class NaturalOrdering 
  implements Comparator {

  @Override
  public int compare(Submission s1, Submission s2) {
    return s1.compareTo(s2);
  }
}
```

I encourage you to use the "demo" (`Main.java` in the `demo` package of the starter code) to experiment with Comparators.


Resources

* Baeldung's article on [Comparator and Comparable in Java](https://www.baeldung.com/java-comparator-comparable).



# Linked Tree Representation



* Implement binary heap with linked tree representation.



Exercise Suppose that a binary heap with $n$ nodes is implemented as a linked tree structure (not a ranked array). How can you get (find) the node (position) where the next leaf should be added in $\Omicron(\lg n)$ time?


Solution

We can employ a clever trick based on binary numbers in conjunction with the ranked representation. Suppose we have already $10$ nodes in the binary heap, and we want to insert the $11^{\text{th}}$ node. We know the binary representation of $11$ is $1011$:

$$
11_{10} = 1011_{2}
$$

Now take the first digit (from left) of the binary representation to mean "start at the root" and for each remaining digit (from left to right) in the binary representation of the $k^{\text{th}}$ node, go *left* if the digit is $0$ and go *right* if it is $1$:








Exercise Here are a few  unsolved (challenging) exercises:

* You might want to add another operation to your PriorityQueue ADT to update (change) the priorities (values). Can you think of an approach to implement the `update` efficiently? 

* You might want to take two priority queues and merge (combine) them. (Some references include a `meld` operation that does this). Can you think of an approach to implement this process efficiently? 


**Aside:** Binary Heap is a clever implementation of PriorityQueue ADT. There are other variations of the heap data structure, namely the [Binomial](https://en.wikipedia.org/wiki/Binomial_heap)[ heap](https://en.wikipedia.org/wiki/Binomial_heap) and [Fibonacci heap](https://en.wikipedia.org/wiki/Fibonacci_heap). These implementations are beyond the scope of this course!
# Treap

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the balancing operations of a Treap.
* Explain the role of the priorities in rebalancing and the importance of being random.
* Select the appropriate rotation type to rebalance a Treap after performing an insertion or removal.
* Implement the core operations of an OrderedMap efficiently with a Treap.
* Analyze the time/space efficiency of a Treap implementation approach for a Map.

> [Starter code](../../zip/chap21-starter.zip) for this chapter


Solution code

[Solution code](../../zip/chap21-solution.zip) for this chapter.


# Random BST: Shuffling the Keys



* Explain the effect of shuffling the keys before insertion to a BST on its height.



Consider inserting the numbers $1, 2, 3, 4, 5, 6$ into a Binary Search Tree in that order. The resulting BST will have the worst height: $h=\Omicron(n)$.





Here is the same input sequence **randomly** shuffled: $4, 3, 5, 1, 6, 2$. The BST resulting from inserting these values (in order) will have a height closer to the best case: $h\approx \Omicron(\lg n)$.





In fact, there are 720 permutations of the sequence $1, 2, 3, 4, 5, 6$. Among these permutations, $32$ will result in $\Omicron(n)$ height, $80$ will result in $\Omicron(\lg n)$ height and the remaining will be $\Omicron(\lg n) < h \ll \Omicron(n)$. (Download the starter code and run the `Demo.main` method.)

Suppose we have, *a priori*, all the keys that will be inserted in a (regular) BST. We know that if the keys are sorted, our BST will degenerate into a linked list. However, it can be proven (see [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition), 3rd edition, theorem 12.4) that a simple shuffle of the keys is sufficient to make the BST balanced. 

The **Treap** data structure is designed around this observation. It attempts to create the effects of shuffling the keys _as they are inserted (or removed)_.
# Random BST: Treap



* Describe what a Treap data structure is.





Treap is a data structure that combines binary search tree and binary heap (hence the name: Tree + Heap ⇒ Treap).



* Each entry (key-value pair) is assigned a **random** *priority*. 
* The BST ordering property is based on keys. 
* The priorities are used to create a (_min_ or _max_) heap.

### Insertion

* Generate random priority for the entry (key-value pair).
* Insert the entry as you would in BST (based on the "key" and ignoring priorities)
* If priorities (inserted node and its parent) are not in the desired order (based on whether we maintain a max- or min-heap), **rotate node up and parent down** (instead of swim-up).    
* Repeat the last step until all priorities are in the desired order. 


### Removal

* Find the target following a "lookup" in BST (on keys, ignoring priorities). 
* Change the priority of the entry to be removed to a value that results in the entry sinking. For example, if the priorities are non-negative, set the target's priority to $-1$.
* Rotate down the target until it cannot sink any further (it becomes a leaf), then remove it.



After any sequence of insertions and deletions, the *height* of the tree is, **with high probability**, *proportional to* the **logarithm** of the number of entries.





Resources

- Wikipedia's entry on [Treap](https://en.wikipedia.org/wiki/Treap)
- [A Visual Introduction to Treap Data Structure (Part I: The Basics)](https://medium.com/carpanese/a-visual-introduction-to-treap-data-structure-part-1-6196d6cc12ee) on Medium.




# Treap: Insertion



- Explain and trace the balancing operations of a Treap.
- Explain the role of the priorities in rebalancing and the importance of being random.



Let's insert the values $1, 2, 3, 4, 5, 6$ into a Treap (in that order).

As you insert each value, assign a random priority to it. For example, below, the priorities are generated from the range $[0, 1]$ and shown under each node.





The insertion process starts as the insertion process of a regular BST, ignoring the priorities. Therefore, the value $2$ goes to the right of $1$.





Assuming we maintain a max-heap for this example, the priorities (inserted node and its parent) are not in the desired order. We, therefore, perform a **rotate left** to bring the node up and parent down.






Why we didn't "swip up" the node to replace its parent?

Rotations preserve the BST order property of the keys, but swapping entries (like a heap percolate-up/down) would not!



Let's insert the next value:





The BST order (over keys) and heap order (over priorities) are maintained. So we insert the next value:





We have a violation of heap order property (over the priorities). So we need to **rotate left** to bring the inserted node above the parent node to fix the heap order property while maintaining the BST order property.






How do we decide to rotate left/right?



If the child node that violates the heap order property is to the parent's right, then you need to rotate left. If the child is to the left of the parent, then you need to rotate right.





We must follow up with another rotate left operation:





The BST order (over keys) and heap order (over priorities) are maintained. So we insert the next value:





The BST order (over keys) and heap order (over priorities) are maintained. So we insert the next value:





We have a violation of heap order property (over the priorities). Since the inserted (child) node is to the right of its parent, we need to **rotate left** to bring the inserted node above the parent node.





The rotation fixes the heap order property while maintaining the BST order property.



Notice that the resulting BST is balanced even though we inserted $1$ to $6$ in sorted order. 


# Treap Insertion: Exercise



* Select the appropriate rotation type to rebalance a Treap after performing an insertion.



Consider the following (max-heap) treap, where the keys are the letters and the priorities are the integers:





Exercise Show the result of inserting the key **M**, including any necessary rotations. Assume the priority generated for the key **M** is 15.


Solution

We insert **M** following the insertion process of a regular BST (ignoring priorities):





We must now apply tree rotations to fix the max-heap order property. Since **M** is to the left of **P**, we apply a right rotation to bring **M** above **P** and **P** to the right of **M**:





Since the priority of **M** is larger than the priority of **T**** we must apply tree rotation again to bring **M** above **T**:





We must apply a rotation one last time; this time, however, we apply a left rotation since **M** is to the right of **J**:





Notice the BST order property is maintained over the keys (letters) and the max-heap order property is maintained over the priorities (integers).


# Treap: Deletion



- Explain and trace the balancing operations of a Treap.
- Explain the role of the priorities in rebalancing and the importance of being random.



Consider the following (max-heap) treap:





Let's remove the node with key $2$. 

We find the entry to be removed (look-up as in BST, ignoring priorities). Since we have a "max-heap" treap and the priorities are non-negative, we set the priority of the entry to be removed to $-1$:





The max-heap order property is violated. To fix it, we need to bring the child node with key $3$ above the node with key $2$ (because $3$ has a higher priority between the children of $2$).

Since $3$ is to the right of $2$, we perform a left rotation:





The max-heap order property is still violated. To fix it, we need to bring the child node with key $1$ above the node with key $2$. Since $1$ is to the left of $2$, we perform a right rotation:





We can easily remove the node with key $2$ as it is a leaf now.





Notice the resulting treap is a BST over the keys and a max-heap over the priorities, and its height is $\Omicron(\lg n)$.


# Treap Removal: Exercise



* Select the appropriate rotation type to rebalance a Treap after performing a removal.




Consider the following (max-heap) treap, where the keys are the letters and the priorities are the integers:





Exercise Show the result of removing the key **T**, including any necessary rotations. 


Solution

After finding the node with key **T** we set its priority to $-1$:





We must now apply tree rotations to fix the max-heap order property. Among the children of **T**, the node with key **Y** has the highest priority. So we must apply a _left_ rotation to bring **Y** above **T**:





Among the children of **T**, the node with key **P** has the highest priority. So we must apply a _right_ rotation to bring **P** above **T**:





Notice at this point we can simply remove **T** (since it has only one child). If we were to follow the process completely, we would have to apply another _left_ rotation to bring **X** above **T**:





We can easily remove the node with key **T** as it is a leaf now.






# Treap: Analysis



* Analyze the time/space efficiency of a Treap implementation approach for a Map.



Operation costs:
* `find` is $\Omicron(h)$ where $h$ is the height of the tree (look-up in BST over keys, ignoring priorities).
* `insert` is $\Omicron(h)$ 
  * at most $\Omicron(h)$ to find where the key must be inserted, 
  * at most $\Omicron(h)$ rotation to bring newly inserted node to the root (to fix heap order property)
* `remove` is $\Omicron(h)$ 
  * at most $\Omicron(h)$ to find the target, 
  * at most $\Omicron(h)$ rotation to bring the target to level $0$ (make it a leaf so it can easily be removed)

So what is the height of the treap? 
* In the *best case*, it is $\Omicron(\lg n)$.
* In the *worst case*, it is $\Omicron(n)$.
* In the **average case**, it can be shown the height is $\Omicron(\lg n)$.



The [original paper](http://faculty.washington.edu/aragon/pubs/rst89.pdf) by Aragon and Siedel in 1989 showed that in a treap where **the priorities are independently and uniformly distributed continuous random variables**, the **expected** depth of any node is $\Omicron(\lg n)$. Consequently, it follows that the **expected** running time for any of the core operations is $\Omicron(\lg n)$.



The takeaway: when inserting a new entry, generate a random real number between $0$ and $1$ and use that number as the priority of the new node. Using "real numbers" implies the probability of two nodes having the same priority is (almost) zero. In practice, we can use random integers from a large range, like $0$ to $2^{31} − 1$ (which is generated by Java's `Random.nextInt` method), or random floating-point numbers. 

In contrast to Treap, AVL tree _guarantees_ $\Omicron(\lg n)$ height. However, there is no need to track nodes' height or calculate balance factors in a treap. Moreover, there are fewer rotation types, and the direction of rotations is easier to determine. In benchmark experiments, treap has shown to have good performance and often outperforms the AVL tree.
# Hash Table

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Identify the steps of **hashing** (convert to hash code and compression).
* Enumerate the properties of a _good_ hash function.
* Describe the job of Java's `hashCode` method.
* Define what a hash table is.
* Explain what **collision** (in the context of hashing) is and when it happens.
* Describe Open Addressing with Linear Probing as a collision resolution.
* Understand and apply the **tombstone mechanism** when removing an entry from a Hash Table with open addressing schemes.
* Explain what is meant by *contamination* of open address hash tables.
* Understand **rehashing** well enough to implement it.
* Compute the **load factor** of a hash table.
* Determine table size and when to rehash.
* Analyze the efficiency of "open address" hash tables.
* Describe primary (and secondary) **clustering effect** of linear probing.
* Describe other probing strategies (quadratic, double hashing, $\dots$, for open address hash table.
* Differentiate **chaining** collision resolution from open addressing.
* Analyze the efficiency of "separate chaining" hash tables.

> This chapter does not have a starter/solution code because a homework is about implementing hash tables.
# Motivation



* A hash table is an implementation of Set/Map aiming for average (expected) constant-time operations. 



Suppose we have an array:





And we want to store a bunch of elements in it:

$$
\text{cat}, \space \text{bat}, \space \text{tap}, \space \text{mad}, \text{dam}, \space \text{nap}, \space \text{pat}
$$

What will we do?

Well, we most likely store them sequentially, one after another.





Here, insertion is fast (constant time), and searching is slow (linear time).

We can get a logarithmic-time search (via binary search) if we keep the data sorted, but that means extra work for insertion (linear time).





We could organize the data in a (balanced) binary search tree to get logarithmic-time insert/remove and find.





But is there any way to get **constant time** for all the core operations?

> A **hash table** is a data structure aiming for average (expected) constant-time operations.

# The Big Picture



* Describe the general idea behind the hash table.





The fundamental idea behind Hash Table is to map each key (entry) to a position (an array index) based on the key itself.



Imagine I have this mystery function that can map keys to array indices:





So when I give it "cat," it would give me $1$, and when I give it "dam," it would give me $9$, $\dots$:





Using this mystery function, I know _exactly_ where to insert each element (and therefore where _exactly_ to look for it). The cost of insertion and search will essentially be the cost of running the mystery function.

Note that we access an entry based on its key (associative retrieval), not its location (so no need to, e.g., search for the key in a tree).


Resources

- Wikipedia's entry on [Hash Table](https://en.wikipedia.org/wiki/Hash_table).
- Robert Nystrom's book "Crafting Interpreter" has a chapter on [Hash Tables](https://www.craftinginterpreters.com/hash-tables.html).



# Hash Function



- Identify the steps of **hashing** (i.e., convert to hash code and compression).
- Enumerate the properties of a _good_ hash function.



Imagine the array where we store the keys has the capacity $M$. The job of the mystery function is to map "keys" to "indices" in the range $[0, M-1]$.



The process of mapping the keys to array indices is called **hashing**.



The mystery function is a **hash function**.





A hash function performs the following two steps:

1. convert a key to an integer (**hash code**) in a range like $[0, N)$.

2. map the hash code into the smaller range $[0, M - 1]$ where $M \ll N$.

A **good** hash function is

* Uniform: maps keys to array indices as evenly as possible (ideally with equal likelihood of generating each index value).
  
* Deterministic: a given key is always mapped to the same index (so we can look it up after insertion).
  
* Cheap: a constant-time operation that is simple and fast to compute.


Aside: What is "hashing," again?!

A speaker may use the word "hashing" to only mean "generating hash code for any given key," or to the "design of hash functions," or to refer to "hash tables" (the entire process of building these data structures with collision resolution, etc.)! **Case in point:** when you speak with "others," do not assume they have the same definition of "hashing" as what is described above here.


  

Resources

* Wikipedia's entry on [Hash Functions](https://en.wikipedia.org/wiki/Hash_function).



# Java's `hashCode()`



- Define what a hash table is.
- Describe the job of Java's `hashCode` method.



In this course, we are primarily interested in implementing hash tables but we take for granted that we already have a good hash function!



A **Hash Table** is a data structure that organizes data using hash functions to support quick insertion/removal and search. 



The responsibility of a hash function is typically divided between the element and the data structure:

* The first step, which converts the key to an integer (hash code), is done by the element (key) itself.

* The second step, which reduces (compresses) the hash code to the range of array indices, is part of a general hash table implementation.

In Java, [every "type" inherits `hashCode()` method from the `Object` class](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#hashCode--). The `hashCode` method returns a hash code value (an integer) for an object. 

* When using built-in types, you can simply call their `hashCode` method to get the hash code value. 

* When making your custom types, you must implement `hashCode` (override it). 

As noted earlier, in this class (i.e., when doing your homework) we take for granted that we can always ask for the hash code of an object by invoking the `hashCode` method on it.

Once we have the hash code for a given key, mapping that value to array indices can be as simple as follows:

```java
// Compress hashcode to index range [0, M-1]
index = key.hashCode() % M; 
```


Resources

* [What is hash code in Java](https://www.educative.io/edpresso/what-is-a-hash-code-in-java) by Educative.
* [Guide to hashCode() in Java](https://www.baeldung.com/java-hashcode) by Baeldung.
* [Java equals() and hashCode() Contracts](https://www.baeldung.com/java-equals-hashcode-contracts) by Baeldung.




# The Challenge



* Explain what **collision** (in the context of hashing) is and when it happens.




The Set/Map ADT can be (efficiently) implemented using a **Hash Table**.


* `has`/`get(key)`: compute `getIndex(key)`, check the corresponding array element for an entry with a matching key.

* `insert`/`put(key, value)`: compute `getIndex(key)`, add or update the corresponding array element with the entry for the matching key.

* `remove(key)`: compute `getIndex(key)`, find the entry with a matching key in the corresponding array collection.

The performance of a hash table depends on how _good_ the hashing procedure is and how efficiently and effectively the **collisions** are handled.

> A *collision* occurs when two different keys $x$ and $y$ map to the same position (array index) in the hash table, i.e., `getIndex(x) == getIndex(y)`.
# Collisions



* Explain what **collision** (in the context of hashing) is and when it happens.



A *collision* occurs when more than one key is mapped to the same array index.

 




Collisions are rare events if they are the results of a well-designed hash function. But, they are inevitable as the set of possible keys is usually vastly larger than the capacity of the hash table (range of array indices). 






Example





There are two main collision handling techniques:

* **Open addressing** – locate the next available (open) position.
* **Chaining** – store multiple entries in each position.




Resources

To understand why collisions are inevitable, consult these references:

* [Counting hash collisions with the birthday paradox](http://matt.might.net/articles/counting-hash-collisions/) by Matt Might.
* [The Pigeonhole Principle: how do pigeons and pigeonholes relate to cryptographic hashing functions and the number of hairs on people's heads?](https://steemit.com/technology/@jiansoo/the-pigeonhole-principle-how-do-pigeons-and-pigeonholes-relate-to-cryptographic-hashing-functions-and-the-number-of-hairs-on) by SteemIt!
* [Hashtables, Pigeonholes, and Birthdays](https://blog.codinghorror.com/hashtables-pigeonholes-and-birthdays/) by Coding Horror!





# Open Addressing



* Describe "Open Addressing with Linear Probing" as a collision resolution.





Open addressing allows elements to overflow out of their target position into other "open" (unoccupied) positions.









The process of locating an open location in the hash table is called **probing**, and various probing techniques are available.




Resources

* Wikipedia's entry on [Open Addressing](https://en.wikipedia.org/wiki/Open_addressing).



# Linear Probing



* Describe "Open Addressing with Linear Probing" as a collision resolution.



Suppose the calculated index for an item's key points to a position occupied by another item. In that case, we increment the index by a constant step size (usually $1$). Then, we keep incrementing the index (modulo the table length to allow for table wraparound) until we find an empty position to insert the key.

```java
int index = getIndex(key);
// if table[index] is occupied, then
for(int i = 0; i < M; i++) { 
  index = (getIndex(key) + i) % M; 
  // stop the loop when table[index] is available!
}
// if we get here and haven't inserted the key, the table is full! 
```


Example



# Linear Probing: Exercise I



* Describe "Open Addressing with Linear Probing" as a collision resolution.



Suppose we have a hash table with capacity $M=7$, and we aim to insert integer keys. Further, assume the `hashCode()` is defined as the sum of the digits of `key`. 

Exercise Complete the table after each of the following operations, assuming collision resolution using **linear probing** (with step size of $1$). For the `find()` operation, _output_ the *indices* of the positions visited during the search to find the element.

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | Output |
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: |
| `insert(1111)` |       |       |       |       |       |       |       |        |
| `insert(5005)` |       |       |       |       |       |       |       |        |
| `insert(86)`   |       |       |       |       |       |       |       |        |
| `find(5557)`   |       |       |       |       |       |       |       |        |
| `insert(2332)` |       |       |       |       |       |       |       |        |
| `insert(8333)` |       |       |       |       |       |       |       |        |
| `find(2332)`   |       |       |       |       |       |       |       |        |
| `insert(700)`  |       |       |       |       |       |       |       |        |



Solution

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | Output |
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: |
| `insert(1111)` |       |       |       |       | 1111  |       |       |        |
| `insert(5005)` |       |       |       | 5005  | 1111  |       |       |        |
| `insert(86)`   | 86    |       |       | 5005  | 1111  |       |       |        |
| `find(5557)`   | 86    |       |       | 5005  | 1111  |       |       | 1: NOT_FOUND |
| `insert(2332)` | 86    |       |       | 5005  | 1111  | 2332  |       |        |
| `insert(8333)` | 86    |       |       | 5005  | 1111  | 2332  | 8333  |        |
| `find(2332)`   | 86    |       |       | 5005  | 1111  | 2332  | 8333  | 3,4,5: FOUND |
| `insert(700)`  | 86    | 700   |       | 5005  | 1111  | 2332  | 8333  |        |

$$
(1 + 1 + 1 + 1) \bmod 7 = 4
$$

Therefore, insert $1111$ at index $4$.

$$
(5 + 0 + 0 + 5) \bmod 7 = 3
$$

Therefore, insert $5005$ at index $3$.

$$
(8 + 6) \bmod 7 = 0
$$

Therefore, insert $86$ at index $0$.

$$
(5 + 5 + 5 + 7) \bmod 7 = 1
$$

Lookup for $5557$ at index $1$. There is no element there! Therefore, return `NOT_FOUND`.

$$
(2 + 3 + 3 + 2) \bmod 7 = 3
$$

Try to insert $2332$ at index $3$. The position is occupied. Linear probing will take us to index $4$ and then index $5$ where the element can be inserted. Therefore, insert $2332$ at index $5$.

$$
(8 + 3 + 3 + 3) \bmod 7 = 3
$$

Try to insert $8333$ at index $3$. The position is occupied, but the occupant is not the target! Linear probing will take us to index $4$ and then index $5$ and finally index $6$ where the element can be inserted. Therefore, insert $8333$ at index $6$.

$$
(2 + 3 + 3 + 2) \bmod 7 = 3
$$

Lookup $2332$ at index $3$. The position is occupied, but the occupant is not the target! Linear probing will take us to index $4$ and then index $5$ where the element can be found. 

$$
(7 + 0 + 0) \bmod 7 = 0
$$

Try to insert $700$ at index $0$. The position is occupied. Linear probing will take us to index $1$ where the element can be inserted. Therefore, insert $700$ at index $1$.


# Linear Probing: Exercise II



* Describe "Open Addressing with Linear Probing" as a collision resolution.



Consider the state of the hash table at the end of the previous exercise; we want to perform two more operations:

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | Output |
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: |
| Current State  | 86    | 700   |       | 5005  | 1111  | 2332  | 8333  | |
| `remove(2332)` |       |       |       |       |       |       |       | |
| `find(8333)`   |       |       |       |       |       |       |       | |

Exercise Complete the table above as you carry out the operations. Do you see any (potential) issues with the `remove` operation?


Solution

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | Output |
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: |
| Current State  | 86    | 700   |       | 5005  | 1111  | 2332  | 8333  | |
| `remove(2332)` | 86    | 700   |       | 5005  | 1111  |       | 8333  | |
| `find(8333)`   | 86    | 700   |       | 5005  | 1111  |       | 8333  | 3,4,5: NF |


$$
(2 + 3 + 3 + 2) \bmod 7 = 3
$$

Lookup $2332$ at index $3$. The position is occupied, but the occupant is not the target! Linear probing will take us to index $4$ and then index $5$ where the element will be found. We can now remove (delete) the element.

$$
(8 + 3 + 3 + 3) \bmod 7 = 3
$$

Lookup $8333$ at index $3$. The position is occupied, but the occupant is not the target! Linear probing will take us to index $4$ and then index $5$, which is empty. At this point, the algorithm will assume $8333$ is not in the Hash Table because if it were, it would have been inserted at index $5$. Therefore, it will return `NOT_FOUND`.



Removing an item may lead to not being able to find previously inserted items that collided with it.







# Lazy Deletion



* Understand and apply the **tombstone mechanism** when removing an entry from a Hash Table with open addressing schemes.



We learned from the previous exercise that if we delete an item (i.e., set its table entry to `null`), then when might prematurely stop a search for another one. This could happen for a target item that had collided with the deleted one. We may incorrectly conclude that the target is not in the table (because we will have stopped our search prematurely.)


The issue with removal

Suppose we insert APPLE, BANANA, CAT, and ANT in the table below. Notice ANT must be inserted at index 0. However, it colides with APPLE. Linear probing will take ANT to index 3. 

Suppose further we delete CAT in the table below.





If we search for ANT now, we won't find it! We start the search at index 0 where APPLE is. However, we stop the search at index 2 as it is empty. 



The solution is **lazy deletion**: flag the item as deleted (instead of deleting it). This strategy is also known as placing a **tombstone** ⚰️ over the deleted item!





Source: [craftinginterpreters.com](https://www.craftinginterpreters.com/hash-tables.html)





Place a tombstone!

When an item is deleted, replace the deleted item with a tombstone (to mark it as "deleted").





By storing a tombstone (a sentinel value) where an item is deleted, we force the search algorithm to keep looking until either the target is found or a free cell is located.






Future inserts can overwrite tombstones, but lookups treat them as collisions.



For the previous exercise, we insert a tombstone (`TS`) in place of the deleted entry:
 
|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | Output |
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :----: |
| Current State  | 86    | 700   |       | 5005  | 1111  | 2332  | 8333  | |
| `remove(2332)` | 86    | 700   |       | 5005  | 1111  |  TS   | 8333  | |
| `find(8333)`   | 86    | 700   |       | 5005  | 1111  |  TS   | 8333  | 3,4,5,6: F |

The `find(8333)` will go over the indices as follows:

```text
[3] --> [4] --> [5] (TOMBSTONE) --> [6] --> FOUND
```

Without inserting a tombstone in place of $2332$, `find(8333)` would not know to continue after finding an empty cell at index $5$.
# Contamination



* Explain what is meant by *contamination* of open address hash tables.



In the previous lesson, we learned about tombstones. The use of tombstones prevents search from terminating prematurely. In addition, the insert operation can reuse the positions marked by a tombstone.


Is it safe for insert to write over tombstone?

You should not simply replace a tombstone with a new item. Why? Well, suppose the item was already inserted in the table! Then, it is necessary to follow the probe sequence to ensure that the new item is not already present in the table.

So, you either must never write over tombstones or perform a search before insertion to verify that a duplicate is not in the table. Only then the new item can safely be inserted into the slot of the first tombstone encountered through the probe sequence.





Tombstones waste space and reduce search efficiency.  



In the worst-case scenario, if the table is (almost) full of tombstones as most of the items were deleted, you will have $\Omicron(n)$ performance when searching. However, there are truly only a few items remaining in the table! This phenomenon is called **contamination**. To recover from contamination, you must **rehash**!
# Rehash



* Understand **rehashing** well enough to implement it.



The likelihood of a collision is proportional to how full the table is.



When the hash table becomes _sufficiently_ full, a _larger_ table should be allocated, and the entries should be reinserted. 



Unlike growing a dynamic array, copying the values from the original collection to the new one will not work with a hash table. 


Why not?

The table size affects the index returned by the hashing procedure. For example, suppose a key was inserted at index $x$ in the smaller table. In that case, it will not necessarily be mapped to the same index in a larger table. Therefore, the search operation may not find an element in the table after the table is resized.



The solution is **rehashing**:

1. Allocate a new hash table (with twice the capacity of the original).

2. **Reinsert** each old table entry (that has not been deleted) into the new hash table.
# Load Factor



* Compute the **load factor** of a hash table.



The **load factor** ($\alpha$) for a hash table is a measure that indicates how "full" the table is. It is defined as the number of filled cells divided by table capacity.

$$
\alpha = \frac{\text{number of filled cells}}{\text{table capacity}}
$$

Notice the numerator is "number of filled cells," that is, the total number of items stored in the table plus the tombstones.



The lower the load factor, the better the performance. There is less chance of a collision when a table is sparsely populated.



When the load factor reaches a given threshold, rehashing kicks in. Since rehashing increases the capacity, it reduces the load factor.

The threshold is usually found empirically based on benchmarking experiments. For example, Java's built-in Hash Table (HashMap) maintains a load factor $\alpha \le 0.75$.

The load factor threshold is usually configurable as it offers a tradeoff between time and space costs. Lower load factor means fewer collisions but more memory overhead. 
# Table Size



* Determine table size and when to rehash.





When choosing a table size (capacity), selecting a **prime number** larger than the desired table size is recommended.



Why? There is an intuitive explanation on StackExchange (use this [link](https://cs.stackexchange.com/a/64191)). The idea generally is that we compute the index as 

```java
// Compress hashcode to index range [0, M-1]
index = key.hashCode() % M; 
```

Assume the value returned from `key.hashCode()` is $K$. Then, for every possible $K$ that shares a common factor with $M$, the key will be hashed to multiple positions of this factor. Therefore, to minimize collisions, we can reduce the number of common factors between $M$ and $K$ by choosing $M$ to have very few factors: a prime number.

In particular, when you rehash, if you double the table capacity, you get an even number. If the table capacity $M$ is an even number, and the `key.hashCode()` is even, the `index` will also be an even number. This occurrence can bias the entries in the table to only occupy the even indices. In that case, only half the table positions may be used. 

When it comes to implementing the hash table, a common practice is to tabulate primes $p_1, p_2, \dots, p_{k}$ (for some choice of $k$) and use it as a look-up table to pick the table size. Something like this:

```java
int primes[] = {2, 5, 11, 23, 47, 97, 197, 397, 797, 1597, 3203, 6421, 12853, 25717, 51437,102877, 205759, 411527, 823117, 1646237,3292489, 6584983, 13169977};
```

When this strategy is employed, the table capacity is capped at $p_k$. Alternatively, when the capacity reaches $p_k$, the algorithm switches to doubling the capacity for the following rehashes.


Resources

* [Why should the size of a hash table be a prime number?](https://www.quora.com/Why-should-the-size-of-a-hash-table-be-a-prime-number) on Quora.
* [Why Should the Length of Your Hash Table Be a Prime Number?](https://medium.com/swlh/why-should-the-length-of-your-hash-table-be-a-prime-number-760ec65a75d1) on Medium.
* [Good hashtable primes](https://planetmath.org/goodhashtableprimes) on Planet Math.


# Linear Probing: Analysis



* Analyze the efficiency of "open address" hash tables.



If there are no collisions, insert/remove and search performance is $\Omicron(1)$; this is the *best-case* scenario.

The *worst-case* scenario is when the probing sequence goes over every occupied position. This scenario leads to $\Omicron(n)$ performance where $n$ is the number of items stored in the hash table.

Given an open-address hash table with load factor $\alpha$, the **expected** number of probes in an unsuccessful search (or for inserting an element) is _at most_:

$$
\frac{1}{1 - \alpha}
$$

The statement above can be proved by assuming the hash table employs a uniform hashing function. The proof is beyond the scope of this class. Interested reader is referred to the [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition): Theorem $11.6$ and Corollary $11.7$. 

> **The take-home message**: since $\alpha < 1$ in open-address hash tables, the average (expected) performance is constant time under the assumption of uniform hashing.


Resources

* Wikipedia entry on [Hash Table: Performance](https://en.wikipedia.org/wiki/Hash_table#Performance).





# Primary Clustering



* Describe primary (and secondary) **clustering effect** of linear probing.



The problem with linear probing is that it tends to form clusters of keys in the table, resulting in longer search chains. 

The reason is that an existing cluster will act as a "net" and catch many of the new keys, which will be appended to the chain and exacerbate the problem.

Exercise Under the assumption of uniform hashing, what is the likelihood that the next key will end up in each "open" position, in the following situation:






Solution

We have ten slots. Assuming uniform hashing, if the table was empty, there is an equal likelihood the next element will be mapped to each index. That is, each position has a $10\\%$ chance to home the next element to be inserted.

Given indices $2$, $3$, and $4$ are occupied, if an element is mapped to any of these, it will end up in position $5$. So the chances $2$, $3$ and $4$ will home the next element is $0$. On the other hand, position $5$ has a net chance of $40\\%$ to get the next element.





This simple illustration shows how a cluster of elements acts as a net to catch the next element to be inserted.  



Each new collision expands the cluster by one element, thereby increasing the length of the search chain for each element in that cluster. In other words, long chains get longer and longer, which is bad for performance since the number of positions scanned during insert/search increases. This phenomenon is called primary clustering (or simply, clustering) issue.

Other probing strategies exist to mitigate the undesired clustering effect of linear probing.

# Quadratic Probing



* Describe other probing strategies (quadratic, double hashing, ... for open address hash table.



In quadratic probing, unlike in linear probing where the strides are constant size, the strides are increments form a quadratic series ($1^2, 2^2, 3^2, \dots$). Thus, the next value of index is calculated as:

```java
int index = getIndex(key);
// if table[index] is occupied, then
for(int i = 0; i < M; i++) { 
  index = (getIndex(key) + i * i) % M; 
  // stop the loop when table[index] is available!
} 
```

This probing creates larger and larger gaps in the search sequence and avoids primary clustering.





A potential issue with quadratic probing is that not all positions are examined, so it is possible that an item can't be inserted even when the table is not full. 


Secondary clustering effect

If a key is mapped to the same index as another key, the second key's prob sequence will follow the first one's footsteps. If this happens repeatedly (for example, due to a poorly implemented hash function), long chains will still form and cause performance to degrade. The phenomenon is called secondary clustering.


# Quadratic Probing: Exercise III



* Describe other probing strategies (quadratic, double hashing, ... for open address hash table.



Suppose we have a hash table with capacity $M=7$, and we aim to insert integer keys. Further, assume the `hashCode()` is defined as the sum of the digits of `key`. 

Exercise Complete the table after each of the following operations, assuming collision resolution using **quadratic probing**.

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | 
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| `insert(5005)` |       |       |       |       |       |       |       |       
| `insert(6374)` |       |       |       |       |       |       |       |      
| `insert(2637)` |       |       |       |       |       |       |       |       
| `insert(7897)` |       |       |       |       |       |       |       |       
| `insert(3453)` |       |       |       |       |       |       |       |       
| `insert(2703)` |       |       |       |       |       |       |       |       
| `insert(7151)` |       |       |       |       |       |       |       |       



Solution

|                | [ 0 ] | [ 1 ] | [ 2 ] | [ 3 ] | [ 4 ] | [ 5 ] | [ 6 ] | 
| -------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| `insert(5005)` |       |       |       |  5005 |       |       |       |       
| `insert(6374)` |       |       |       |  5005 |       |       | 6374  |      
| `insert(2637)` |       |       |       |  5005 | 2637  |       | 6374  |       
| `insert(7897)` | 7897  |       |       |  5005 | 2637  |       | 6374  |       
| `insert(3453)` | 7897  |  3453 |       |  5005 | 2637  |       | 6374  |       
| `insert(2703)` | 7897  |  3453 |       |  5005 | 2637  |  2703 | 6374  |       
| `insert(7151)` | 7897  |  3453 |  7151 |  5005 | 2637  |  2703 | 6374  |

Here are the calculations:

$$
5 + 0 + 0 + 5 = 10 \implies 10 \space \\% \space 7 = 3
$$

$$
6 + 3 + 7 + 4 = 20 \implies 20 \space \\% \space 7 = 6
$$

$$
2 + 6 + 3 + 7 = 18 \implies 18 \space \\% \space 7 = 4
$$

$$
7 + 8 + 9 + 7 = 31 \implies 31 \space \\% \space 7 = 3
$$

The position `[3]`  is already occupied; the quadratic probe would explore the sequence:
* $(3 + 1) \\% 7 = 4$ OCCUPIED
* $(3 + 4) \\% 7 = 0$ Bingo! 

$$
3 + 4 + 5 + 3 = 15 \implies 15 \space \\% \space 7 = 1
$$

$$
2 + 7 + 0 + 3 = 12 \implies 12 \space \\% \space 7 = 5
$$

$$
7 + 1 + 5 + 1 = 14 \implies 14 \space \\% \space 7 = 0
$$

The position `[0]`  is already occupied; the quadratic probe would explore the sequence:
* $(0 + 1) \\% 7 = 1$ OCCUPIED
* $(0 + 4) \\% 7 = 4$ OCCUPIED
* $(0 + 9) \\% 7 = 2$ Bingo!


# Other Probing Strategies



* Describe other probing strategies (quadratic, double hashing, ... for open address hash table.



Another probing strategy is **double hashing**, where the interval between probes is computed by a second hash function:

```java
int index = getIndex(key);
// if table[index] is occupied, then
for(int i = 0; i < M; i++) { 
  index = (getIndex(key) + hash(key) * i) % M; 
  // stop the loop when table[index] is available!
} 
```

Here, `hash(key) != key.hashCode()` and `hash(key) != 0`. (And, of course, the second hash function must also be a "good" hash function.)


How is the second hash function implemented?

In practice, the second hash function usually uses the first one (`key.hashCode()`) to produce another hash value.

The general form for this practice is as follows: $\text{index} = [h_1(k) + j\times h_2(k)] \space \text{mod} M$ looping over $j \in [1,M)$ until a position is found.

Let’s assume that the keys are already hash keys (i.e. $k$ is `key.hashCode`) then $h_1$ is what I defined earlier as `getIndex`: $h_1(k) = k \text{mod} M$ and the second hash function is $h_2(k) = q - (k \text{mod} q)$ where $q


The advantage of double hashing is that the probe sequence depends on the "key" (rather than a fixed pattern). Double hashing avoids (both primary and secondary) clustering.

There are many, more sophisticated, techniques based on open addressing. Examples include:

* [Cuckoo Hashing](https://en.wikipedia.org/wiki/Cuckoo_hashing)
* [Coalesced Hashing](https://en.wikipedia.org/wiki/Coalesced_hashing)
* [Robin Hood Hashing](https://en.wikipedia.org/wiki/Hash_table#Robin_Hood_hashing)
* [Hopscotch Hashing](https://en.wikipedia.org/wiki/Hopscotch_hashing)


Cuckoo Hashing Demo

This section should be considered as an optional reading; we will go over it if time allows!





# Chaining



* Differentiate "chaining" collision resolution from "open addressing."





Store all colliding elements in an auxiliary data structure like a linked list.









Each position in the hash table can be seen as a **bucket** that stores multiple entries. Thus, this approach is sometimes called **bucket hashing**.



Other names for chaining include "separate chaining" (as in collisions are dealt with using *separate* data structures), "open hashing," "close addressing" (as opposed to open addressing).


Example




Resources

* Wikipedia's entry on [Hash Table: Separate Chaining](https://en.wikipedia.org/wiki/Hash_table#Separate_chaining).



# Chaining vs. Open Addressing



* Differentiate "chaining" collision resolution from "open addressing."



Once there is a collision, instead of probing for an open (unoccupied) position, you traverse the auxiliary data structure referenced by the table element at `index = key.hashCode() % M`. Once you have determined that an item is not present, you can insert it into the auxiliary data structure. 

To delete an item, you remove it from the auxiliary data structure. In contrast to open addressing, removing an item actually deletes it, so it will not be part of future search chains.

Another contrast to open addressing is that only items with the same value for `key.hashCode() % M` will be examined during the search. Search chains can overlap in open addressing. For example, a search chain may include items in the table with different starting index values.

You can store more elements in the table than its capacity allows (which is not the case for open addressing, unless you rehash and grow the table). Although in chaining you will not run out of space, you can end up with a load factor $\alpha > 1$, which means higher chances of collisions. Therefore, you should consider rehashing in chaining too (to keep the load factor small).
# Chaining: Analysis



* Analyze the efficiency of "separate chaining" hash tables.



In chaining, insert, remove, and lookup, run proportional to the number of keys in the given chain. So, to analyze the complexity, we need to examine the length of the chains.

**Best case**: if we are lucky, the number of elements inserted is smaller than the capacity of the table, and we have a stellar hashing procedure with no collision! So the performance for insert, remove, and search are $\Omicron(1)$. 

**Worst case**: if we're unlucky with the keys we encounter or have a poorly implemented hashing procedure, all keys may get mapped to the same bucket.





In the worst-case, the performance is $\Omicron(N)$ where $N$ is the number of items stored in the hash table. (Assuming the auxiliary is a linked list).

**Average case**: under the assumption of [uniform hashing](https://en.wikipedia.org/wiki/SUHA_(computer_science)), we _expect_ each bucket to be of size $\frac{N}{M}$ where $N$ is the number of items stored in the hash table, and $M$ is its size (number of array/table slots). Notice this ratio is the load factor. Therefore, the performance is $\Omicron(\alpha)$.





If we keep the $\alpha$ bounded by a small constant (preferably below $1$), we get (expected) constant runtime operations (in the average case analysis).



Notice that the performance also depends on the choice of the auxiliary data structures. The most common choices follow.

| Chaining in each bucket | `find` | `delete` | `insert` |
| :---------------------- | :----: | :------: | :------: |
| (Unordered) linked list | $\Omicron(\alpha)$ | $\Omicron(\alpha)$ | $\Omicron(\alpha)$ |
| (Ordered) dynamic array | $\Omicron(\lg \alpha)$ | $\Omicron(\alpha)$ | $\Omicron(\alpha)$ |
| (Self-balancing) binary search tree | $\Omicron(\lg \alpha)$ | $\Omicron(\lg \alpha)$ | $\Omicron(\lg \alpha)$ |

In Java, the HashMap is implemented using a chaining strategy with a linked list auxiliary data structure. Java's HashMap maintains a load factor $\alpha \le 0.75$. When the number of elements is reached a certain limit, it switches to a BST for the auxiliary data structure (the keys must also be "comparable" otherwise you wouldn't be able to build BSTs).


Resources

* Wikipedia entry on [Hash Table: Performance](https://en.wikipedia.org/wiki/Hash_table#Performance).



# Heapsort

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain how a PriorityQueue can be used to sort a collection.
* Describe the efficiency of PriorityQueue based sort using Big-Oh notation.
* Define **heapsort** and explain how it relates to (and differs from) selection sort.
* Trace the operation of Floyd's **heapify** method, which builds the heap from the bottom up.
* Explain why we can ignore the leaves in the heapify process.
* Show there are ⌈ n/2 ⌉ leaves in a complete binary tree.
* Show Floyd's **heapify** is a linear-time operation.
* Explain and trace the **in-place** sorting stage of heapsort.
* Implement heapsort efficiently in Java.
* Use PriorityQueue to solve selection problems. 

> [Starter code](../../zip/chap23-starter.zip) for this chapter.


Solution code

[Solution code](../../zip/chap23-solution.zip) for this chapter.


# Sort using PriorityQueue



* Explain how a PriorityQueue can be used to sort a collection.



A PriorityQueue can be used for sorting a collection:

* Insert every element from the collection into the PriorityQueue.
* Remove elements from the PriorityQueue and place them in order, one after another, in the collection.

Exercise Complete the implementation of `Heap.sort` in the starter code.

```java
private static void sort(Integer[] data) {
  // TODO Implement me
}
```

**Suggestion**: Use Java's built-in [PriorityQueue](https://docs.oracle.com/javase/8/docs/api/java/util/PriorityQueue.html ).


Solution

```java
private static void sort(Integer[] data) {
  PriorityQueue pq = new PriorityQueue<>();
  for (int i = 0; i < data.length; i++) {
    pq.add(data[i]);
  }

  for (int i = 0; i < data.length; i++) {
    data[i] = pq.remove();
  }
}
```


# Efficiency



* Describe the efficiency of PriorityQueue based sort using Big-Oh notation.



Consider your implementation of `Heap.sort` from the previous section. Assume Java's `PriorityQueue` is implemented as Binary Heap, hence insertion/removal are $\Omicron(\lg n)$.

Exercise Complete the following table. Use Big-Oh notation to asymptotically describe time/space complexities.

|             | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| ----------- | --------------- | ---------------- | ----------- | --------------- |
| `Heap.sort` |                 |                  |             |                 |



Solution

* We need a PriorityQueue: $\Omicron(n)$ auxiliary space.
* $n$ inserts into PriorityQueue, each $\Omicron(\lg n)$, adds up to $\Omicron(n \lg n)$
* $n$ removes from PriorityQueue, each $\Omicron(\lg n)$, adds up to $\Omicron(n \lg n)$

|             | Time Complexity | Space Complexity | Input Space | Auxiliary Space |
| ----------- | :-------------: | :--------------: | :---------: | :-------------: |
| `Heap.sort` | $\Omicron(n \lg n)$    | $\Omicron(n)$           | $\Omicron(n)$      | $\Omicron(n)$          |



# Heapsort



* Define heapsort and explain how it relates to (and differs from) selection sort.




**Heapsort** is using a (binary) Heap-based PriorityQueue for sorting. It has $\Omicron(n \lg n)$ performance. This performance is *substantially* better than all the other (quadratic) sorting algorithms we have studied.  



Heapsort is, in a way, similar to selection sort; it repeatedly finds the smallest/largest item and moves it to the front/back of the collection. 

The main difference is that instead of scanning through the entire collection to find the smallest/largest item, it uses a heap to find the "best" (max or min) element in sub-linear $\Omicron(\lg n)$ time.



In the earlier example, we started with an empty heap (PriorityQueue), then successively inserted each element. This process is an $\Omicron(n \lg n)$ operation. It turns out building the heap can be done in linear time! Although the asymptotic efficiency of heapsort remains $\Omicron(n \lg n)$.

This alternative method starts by arbitrarily putting the elements in a ranked array representation of a binary heap, thus respecting the shape property and repairing the heap (order) property in linear time.

**Aside-1:** This latter approach was invented by [Robert W Floyd](https://en.wikipedia.org/wiki/Robert_W._Floyd), computer scientist and recipient of [Turing Award](https://en.wikipedia.org/wiki/Turing_Award) in 1978.  

**Aside-2:** The original $\Omicron(n \lg n)$ approach is called Williams' method after [J. W. J. Williams](https://en.wikipedia.org/wiki/J._W._J._Williams) the inventor of binary heaps. Williams invented binary heap for heapsort (not for implementing PriorityQueue).


Resources

* Wikipedia's entry on [Heapsort](https://en.wikipedia.org/wiki/Heapsort).



# Floyd's Heapify



* Trace the operation of Floyd's heapify method, which builds the heap from the bottom up.



Let's revisit how William's (original) method builds the heap:


Demo



Let's now see the Floyd's **heapify** algorithm which builds the heap from bottom up:


Demo


# Exercise



* Trace the operation of Floyd's heapify method, which builds the heap from the bottom up.



Exercise Heapify the following collection (sequence of numbers) using Floyds method. Draw the binary heap and corresponding rank array representation as in the previous demo.

$$
8, 3, 2, 7, 9, 4
$$


Solution




Resources

* The exercise is taken from [Heapsort Algorithm](https://www.interviewcake.com/concept/java/heapsort) on InterviewCake.



# `#Leaves` in a Complete Binary Tree



- Explain why we can ignore the leaves in the heapify process.
- Show there are ⌈ n/2 ⌉ leaves in a complete binary tree.



Floyd's heapify algorithm works from the bottom of the tree upwards: compare each node to its children and move nodes down (sink) so that parent nodes are always larger than their children.



However, the leaf nodes don't have any children; they can be ignored! 



Exercise How many leaves are in a complete binary tree that has $n$ nodes. Formulate this as an exact function of $n$ (not Big-Oh notation).


Solution

In a complete binary tree, there are $\left \lceil n/2 \right \rceil$ leaves and $\left \lfloor n/2 \right \rfloor$ non-leaf nodes (internal nodes and a root).

Let's build an intuition for this. The complete binary tree can be seen as the structure of a [single-elimination tournament](https://en.wikipedia.org/wiki/Single-elimination_tournament) with $k$ teams corresponding to the leaves. As a simplifying condition, imagine $k$ is a power of 2. Therefore, we have a _perfect_ (complete & full) binary tree:





Each non-leaf (internal node) is a _game_ in which the loser is eliminated, and the winner goes up to the next round. Finally, there is one final match, which corresponds to the root.  

Since there will be only one winner, there must be $(k-1)$ games (non-leaf nodes) since every other $(k-1)$ team loses exactly once. Therefore, we have $k+(k-1)=2k−1$ nodes altogether. And, we can verify that

$$
\left \lceil \frac{n}{2} \right \rceil = \left \lceil \frac{2k-1}{2} \right \rceil = k \\; \\; \text{leaves}
$$

A similar analogy can be made for the case where $k$ is not a power of 2. You can imagine one lucky team rests in the first round of the tournaments and directly goes to the next round. Eventually, $k-1$ teams must be eliminated, and that takes $k-1$ games (non-leaf nodes).

**Aside:** the "proper" approach to proving the formula above for the number of leaves is through [Mathematical induction](https://en.wikipedia.org/wiki/Mathematical_induction).

The implication of this observation is to simplify the implementation of the heapify process:

```java
private static void heapify(Integer[] data) {
  for (int i = numNodes / 2; i >= 1; i--) {
    sink(data, i);
  }
}
```


# Heapify: A Linear-Time Operation



* Show Floyd's heapify is a linear-time operation.



Each node, in Floyd's algorithm, in the worst case, sinks to become a leaf. The _sink_ operation involves repeatedly (recursively) _swapping_ the value with one of the children. Let's make the following observation:



Each node at height $h$ makes at most $h$ swaps (down).



So if you are a leaf, at height $h=0$, you make no swaps down. If you are one level higher, at height $h=1$, you make at most one swaps down. And so on. Let's make the following observation:



In a complete binary tree with $n$ nodes, there are $\left \lceil \frac{n}{2} \right \rceil$ nodes at height $h=0$ (leaves), $\left \lceil \frac{n}{4} \right \rceil$ nodes at height $h=1$, $\left \lceil \frac{n}{8} \right \rceil$ nodes at height $h=2$, $\dots$



In general, there are at most $\left \lceil \frac{n}{2^{h+1}} \right \rceil$ nodes of height $h$, so the cost of building a heap is

$$
\sum_{h=0}^{\left \lfloor \lg n  \right \rfloor} \left \lceil \frac{n}{2^{h+1}} \right \rceil \Omicron(h)
$$

We can rewrite the above expression as

$$
O \left ( \sum_{h=0}^{\left \lfloor \lg n  \right \rfloor} \left \lceil \frac{n}{2^{h+1}} \right \rceil h \right )
$$

Since $n$ does not depend on the value of $h$, it can be moved out of the fraction 

$$
O \left ( n \sum_{h=0}^{\left \lfloor \lg n  \right \rfloor} \left \lceil \frac{h}{2^{h+1}} \right \rceil  \right )
$$

Further observe that $\left \lceil \frac{h}{2^{h}\times 2} \right \rceil < \frac{h}{2^{h}}$ and we can use the upper-bound within big-O notation.

$$
O \left ( n \sum_{h=0}^{\left \lfloor \lg n  \right \rfloor} \frac{h}{2^{h}} \right )
$$

Moreover, 

$$
\sum_{h=0}^{\left \lfloor \lg n  \right \rfloor} \frac{h}{2^{h}} < \sum_{h=0}^{\infty} \frac{h}{2^{h}}
$$ 

And, we can use the upper-bound within big-O notation:
​​
$$
O \left ( n \sum_{h=0}^{\infty} \frac{h}{2^{h}} \right )
$$ 
​
It turns out the summation converges to $2$: (It is beyond the scope of this course to prove this, but you can [check it out on Wolfram Alpha](https://www.wolframalpha.com/input/?i=%5Csum_%7Bh%3D0%7D%5E%7Binf%7D+h%2F%282%5Eh%29).)

$$
\sum_{h=0}^{\infty} \frac{h}{2^{h}} = 2
$$ 

Therefore, we get the following which means the heapify operation is linear time.

$$
O \left ( n \times 2 \right ) = O \left ( n \right )
$$ 



A more intuitive analysis

Let's consider a perfect binary tree; here is a less accurate but more intuitive analysis:

| Height | #nodes   | worst-case #swaps       |  Total work |
| :----: | :------: | :---------------------: | :---------: |
| $0$    | $n/2$    | $n/2 \times 0 = 0$      |  $0$        | 
| $1$    | $n/4$    | $n/4 \times 1 = n/4$    |  $n/4$      |
| $2$    | $n/8$    | $n/8 \times 2 = n/4$    |  $n/2$      |
| $3$    | $n/16$   | $n/16 \times 3 < n/8$   |  $n/2 + n/8$ |
| $4$    | $n/32$   | $n/32 \times 4 = n/8$   |  $n/2 + n/4$ |
| $5$    | $n/64$   | $n/64 \times 5 < n/16$  |  $n/2 + n/4 + n/16$ |
| $6$    | $n/128$  | $n/128 \times 6 < n/16$ |  $n/2 + n/4 + n/8$ |
| $\vdots$    | $\dots$  | $\dots$ |  $n/2 + n/4 + n/8 + \dots$ |

The summation for total work is in $\Omicron(n)$.








Resources

For a more detailed analysis of Floyd's algorithm, refer to Page 157 (Chapter 6, Section 3) of [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition).






# Heapsort: In-place Sorting



* Explain and trace the in-place sorting stage of heapsort.



Each time we remove an element from the heap, it's the largest item in the underlying array. So, in sorted order, it belongs at the end of the collection. As we'll see, removing an item from the heap conveniently frees up space at the end of the underlying array, where we can put the removed item


Demo



This approach reduces the $\Omicron(n)$ auxiliary space to $\Omicron(1)$.
# Heapsort: Putting it together!



* Implement heapsort efficiently in Java.



Exercise Complete the implementation of `HeapSort.sort` so it builds the head from bottom up (Floyd's method) and then sorts the collection _in-place_.

```java
private static void sort(Integer[] data) {
  // TODO Implement me
}
```

**Note**: Assume the following helper methods are correctly implemented:

```java
private static void sink(Integer[] data, int index, int numNodes) {
  // percolate down data[index] to restore heap order
  // the percolation is bounded to go down no further than data[numNodes]
}

private static void swap(Integer[] data, int i, int j) {
  // swap values between data[i] and data[j]
}
```


Solution

```java
private static void sort(Integer[] data) {
  // heapify
  for (int i = numNodes / 2; i >= 1; i--) {
    sink(data, i, numNodes);
  }

  // sort
  for (int i = numNodes; i >= 1; i--) {
    swap(data, 1, numNodes--); // remove best
    sink(data, 1, numNodes); // restore order
  }
}
```


# PriorityQueue Aside: Selection Problem

Consider the following programming challenge:

> You are given an array of $n$ integers in no particular order. Write a program that prints out the $k^{\text{th}}$ best (max or min) value where $k \le n$.

Students came up with the following strategies to solve this problem (assuming "best" means "max"):

**Strategy 1:** Heapify the array using Floyd's algorithm to build a _max_-heap. Then, perform $k$ remove.

**Strategy 2:** Heapify the first $k$ element using Floyd's algorithm to build a _min_-heap. Then, for the remaining $n-k$ elements, compare each to best (min) in the heap; if the element is _larger_, remove the best (min) and insert the (larger) element instead. When done, the best element in the _min_-hep is the  $k^{\text{th}}$ best value in the collection!

Exercise For each strategy, perform asymptotic analysis for it time complexity.


Solution

**Strategy 1:**
* $\Omicron(n)$ heapify using Floyd's algorithm
* $k \times \Omicron(\lg n)$ remove.
* Total: $\Omicron(n + k\lg n)$

**Strategy 2:**
* $\Omicron(k)$ heapify the first $k$ elements using Floyd's algorithm
* for the other $n-k$ elements:
  * compare each to best (min) in heap: $\Omicron(1)$
  * if it is larger, 
    * remove the best (min): $\Omicron(\lg k)$
    * insert the larger element instead: $\Omicron(\lg k)$
* Total: $\Omicron(k) + (n-k) \times \Omicron(\lg k) \in \Omicron(k + n \lg k)$





# Merge Sort

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the operations of **MergeSort** on a particular data sequence.
* Implement MergeSort efficiently (allowing $\Omicron(n)$ auxiliary space).
* Analyze the best- and worst-case space and time efficiency of merge phase and MergeSort overall.
* Determine how to optimize the merge phase to be $\Omicron(1)$ for sorted subarrays and why this results in $\Omicron(n)$ for already sorted starting sequences. 


> [Starter code](../../zip/chap24-starter.zip) for this chapter.


Solution code

[Solution code](../../zip/chap24-solution.zip) for this chapter.


# The sorting algorithms we already know!



* Recall the process and efficiency of bubble, selection, insertion, and heap sort.



| Sort | Description | Worst Case|   
| :----: | :---------- | :-------: |
| Bubble |Compare two elements at a time and swap if the second element is larger than the first. Repeat until sorted.| $\Omicron(n^2)$ |   
| Selection | Find largest, swap with the last element then, find the second largest, swap with the second last element, and so on (repeat for each position).  | $\Omicron(n^2)$ |  
| Insertion | Divide the sequence into sorted and unsorted portions. The sorted portion is empty at the start. Remove the first element from unsorted and place it into the sorted portion such that it remains sorted. Repeat until the unsorted portion is empty.  | $\Omicron(n^2)$ |  
| Heap |Build a max-heap from the sequence (bottom-up heapify). Remove the max and swap it to the end of the collection where the "leaf" was removed. Repeat the last step $n-1$ times.|  $\Omicron(n \lg n)$ | 
# Merge Sort: The Big Picture!



* Explain and trace the merge sort algorithm on a particular data sequence.



The merge sort applies the [divide-and-conquer strategy](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm) to sort a sequence:

1. Divide the sequence into subsequences of singletons. (A singleton sequence consists of one element, and it is considered to be already sorted.)

2. Successively merge the subsequences pairwise until a single sequence is reformed. Each merge preserves order, so each merged subsequence (and the final merged sequence) are sorted.






Demo




Resources

* Wikipedia's entry on [Merge sort](https://en.wikipedia.org/wiki/Merge_sort).
* HackerEarth's [Merge Sort Tutorial](https://www.hackerearth.com/practice/algorithms/sorting/merge-sort/tutorial/) with visualizer and coding questions.
* InterviewBit's [Merge Sort Tutorial](https://www.interviewbit.com/tutorial/merge-sort-algorithm/) with pictorial examples and a video explanation.
* Toptal's page on [Merge Sort](https://www.toptal.com/developers/sorting-algorithms/merge-sort) with animation, code, analysis, and discussion.
* [xSortLab demo](http://math.hws.edu/eck/js/sorting/xSortLab.html) on Merge Sort (Along other sorts).


# Recursive Implementation



* Implement the MergeSort recursively.



Exercise Open the starter code, the `Demo.java` file and complete the implementation of `mergesort`. 
Assume the `merge` method is correctly implemented, and implement `mergesort` recursively.

**Hint:** The merge sort is naturally recursive:
* Split the sequence into two subsequences (of roughly equal size).
* Sort each subsequence recursively.
* Merge the two (sorted) sub-sequences back together.


Solution

Please visit the posted solution.


# The Merge Process



* Explain and trace the merge sort algorithm on a particular data sequence.



Consider the following two sorted list of numbers, $A = \\{1, 3, 5 \\}$  and $B = \\{0, 2, 4, 8, 9 \\}$. 
We are interested in combining (merging) these two lists, such that the resulting merged list remains sorted. 

Here is the naive approach:

| Step  | Details  | Runtime |
| :---: | :------- | :-----: |
| I     | Make a list, $C$, large enough to hold all elements of $A$ & $B$ | $\Omicron(1)^{*}$ |
| II    | Copy elements of $A$ to $C$  | $\Omicron(n)$ |
| III   | Copy elements of $B$ to $C$  | $\Omicron(m)$ |
| VI    | Sort $C$  | $\Omicron(\text{sort})^{**}$ |



$^{*}$ $\Omicron(m+n)$ depending on the language/data structure cost to construct the auxiliary space.\
$^{**}$ The $\Omicron(\text{sort})$ will be _linearithmic_ at best (for comparison-based sorting).



This solution does not make any use of the knowledge the two inputs were sorted, to begin with. So naturally, the question is: can we do better? And the answer is yes; with a bit of creativity, we can copy the numbers in $A$ and $B$ to $C$, one at a time, and keep them in sorted order. 


Linear-time merge



# Implement Merge



* Implement the Merge process. 



Exercise Open the starter code, the `Demo.java` file and complete the implementation of `merge`. Your implementation must have time complexity of $\Omicron(n)$. 

**Hint:** It is possible to implement merge with $\Omicron(1)$ auxiliary space (so the whole merge sort process would be _in-place_). However, it adds unnecessary complexity to the implementation, which gets the essential idea behind the linear-time merge process. So, feel free to implement `merge` with $\Omicron(n)$ auxiliary space. 


Solution

Please visit the posted solution.


# Merge Sort: Analysis



* Analyze the best and worst-case time efficiency of MergeSort.





The merge sort runs in $\Omicron(n \lg n)$ time.



**Justification:**

* The number of times the merge sort divides a sequence is the number of times $n$ can be halved: $\Omicron(\lg n)$. Therefore, the divide part has $\Omicron(\lg n)$ levels. At each level $l$, we perform $2^l$ _divide_ (which itself is a constant time). So the total work is $2^0 + 2^1 + \dots + 2^{\lg n} \in \Omicron(n)$.





* The number of times merge sort merges the subsequences is equal to the number of sub-sequences. Therefore, the merging part also has $\Omicron(\lg n)$ levels. Consider at each level, we perform $k\times \Omicron(n/k) \in \Omicron(n)$ time to merge the sub-arrays. So the total running time for the merge sort algorithm is  $\Omicron(n \lg n)$, 






Formal Proof

A formal proof can be constructed by writing the runtime of merge sort as a **recurrence relation** 
$T(n) = 2T(n/2) + \theta(n)$ and showing $T(n) \in \theta(n \lg n)$. 

If you want to look this up, search for "the master theorem for divide-and-conquer recurrences" and look up "case 2". This is, however, beyond the scope of this course.




Resources

* [Analysis of Merge Sort](https://www.khanacademy.org/computing/computer-science/algorithms/merge-sort/a/analysis-of-merge-sort) on Khan Academy.
* Wikipedia's entry on [Merge sort: Analysis](https://en.wikipedia.org/wiki/Merge_sort#Analysis).
* Wikipedia's entry on [Master Theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)).




# Merge Sort: Optimization?



* Determine how to optimize the merge phase to be O(1) for sorted subarrays and why this results in $\Omicron(n)$ for already sorted starting sequences.



You can optimize bubble sort to stop early when the input is sorted. The optimized version of bubble sort runs in $\Omicron(n)$ for the case when the input sequence is already sorted. 



The merge sort algorithm can also be optimized to run in $\Omicron(n)$ when the input sequence is already sorted.



Exercise Can you come up with the optimization strategy and justify it runs in $\Omicron(n)$ when the input sequence is already sorted.


Solution

Since the (in-place) merge operation assumes the two sub-arrays (to be merged) are already sorted, we can check if the last element of the first subarray is less than or equal to the first element of the second sub-array. In that case we can *escape* merging:

```java 
if (a[mid - 1] > a[mid]) { // escape comparision 
	merge(a, left, mid, right)
}
```

The runtime will be $\Omicron(n)$ because the "escape comparison" is done for every pair of subarrays level by level as you merge $n$ subarrays to $n/2$ to $n/4$ to $\dots$ to $1$ array. So the total number of comparisons would be $n/2 + n/4 + n/8 + \dots + 1$ which is $\Omicron(n)$


# Quicksort

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Explain and trace the operations of **QuickSort** on a particular data sequence.
* Implement QuickSort efficiently.
* Analyze the best- and worst-case space and time efficiency of **partitioning phase** and QuickSort overall.
* Recognize that the average-case time efficiency for QuickSort is the same as the best case.
* Compare the advantages and disadvantages of various **pivot** choices when implementing the general QuickSort algorithm.
* Trace the operations of QuickSort on a particular data sequence using **the median of three** pivot choices.
* Explain what **stable sorting** means and determine which sorting algorithms (that we learned so far) are stable.

> [Starter code](../../zip/chap25-starter.zip) for this chapter.


Solution code

[Solution code](../../zip/chap25-solution.zip) for this chapter.


# Linearithmic Sorts



* Summary of Linearithmic Sorts.



We add another sorting strategy, the **Quicksort**, to our repertoire of linearithmic sorts.

| Name | Description | Average Case|   
| :----: | :---------- | :-------: |
| Heapsort | Build a max-heap from the sequence (bottom-up heapify). Remove the max and swap it to the end of the collection where the "leaf" was removed. Repeat the last step $n-1$ times. |  $\Omicron(n \lg n)$ |
| Merge Sort | Divide the sequence into subsequences of singletons. Successively merge the subsequences pairwise until a single sequence is reformed. |  $\Omicron(n\lg n)$ |
| Quicksort | Pick an element from the sequence (the pivot), partition the remaining elements into those greater than and less than this pivot. Repeat this for each partition (recursively). |  $\Omicron(n\lg n)$ |  

As noted [here](https://xlinux.nist.gov/dads/HTML/quicksort.html), "quicksort has running time $\Omicron(n^2)$ in the worst case, but it is typically $\Omicron(n \lg n)$. Indeed, in practical situations, a finely tuned implementation of quicksort beats most sorting algorithms, including those whose theoretical complexity is $\Omicron(n \lg n)$ in the worst case.
# Quicksort: The Big Picture!



* Explain and trace the quicksort algorithm on a particular data sequence.



In Quicksort, we **partition** the sequence into two subsequences separated by a single element $x$ that is greater than or equal to every element in the left subsequence and less than or equal to every element in the right subsequence.







After partitioning, the element $x$, called the **pivot** element, is in its final (sorted) position.



We now apply the partitioning to the two subsequences separately. This process is naturally recursive (and quick).


Demo




Resources

* Wikipedia's entry on [Quicksort](https://en.wikipedia.org/wiki/Quicksort).
* Baeldung's [An Overview of QuickSort Algorithm](https://www.baeldung.com/cs/algorithm-quicksort).
* InterviewBit's [Quicksort Tutorial](https://www.interviewbit.com/tutorial/quicksort-algorithm/) with pictorial examples and a video explanation.
* Toptal's page on [Quicksort](https://www.toptal.com/developers/sorting-algorithms/quick-sort) with animation, code, analysis, and discussion.
* [xSortLab demo](http://math.hws.edu/eck/js/sorting/xSortLab.html) on Quicksort (Along other sorts).


# Partition: Tracing



* Explain and trace the quicksort algorithm on a particular data sequence.



Exercise Trace the first pass of running quicksort (applying partitioning process) on the following sequence. Take the right-most element as pivot. 

$$
20, \space 13, \space 7, \space 71, \space 31, \space 10, \space 5, \space 50, \space 17
$$


Solution

|  20  |  13  |  7   |  71  |  31  |  10  |  5   |  50  |  17   |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: |
|  **5**   |  13  |  7   |  71  |  31  |  10  |  **20**  |  50  |  17   |
|  5   |  13  |  7   |  **10**  |  31  |  **71**  |  20  |  50  |  17   |
|  5   |  13  |  7   |  10  |  17  |  71  |  20  |  50  |  **31**   |

Items in bold indicate the elements which were swapped. 


# Partition: Implementation



* Implement the partition process.



Exercise Open the starter code, the `Demo.java` file, and complete the implementation of `partition` based on your understanding of the partition process. Take the right-most element as pivot. 


Solution

Please visit the posted solution.


# Quicksort: Recursive Implementation



* Implement the Quicksort recursively.



Exercise Open the starter code, the `Demo.java` file, and complete the implementation of `quicksort`. 

**Hint:** The quicksort is naturally recursive:
* Pick a pivot and partition the sequence.
* Recursively sort each partition.


Solution

Please visit the posted solution.


# Quicksort: Analysis



* Analyze the best and worst-case time efficiency of QuickSort.



Exercise Based on your understanding of the "partitioning" process, what is the runtime of the partition subroutine?


Solution

It is $\Omicron(n)$; each element is visited once using the _left_ and _right_ pointers. 



For the following theoretical considerations, assume the case where we select the pivot to be the rightmost element.



The quicksort runs in $\Omicron(n^{2})$ time in the **worst-case**.



Exercise Complete the following statement.  

* The worst case is when the sequence values are already sorted (in ascending or descending order). 
  
* In this case, the partition algorithm will always select the (next) smallest/largest element, resulting in the most unbalanced split possible; two subsequences with  `________` and `________` elements.

* Repeated partitioning of this type will occur `________` times before both subsequences get down to size 1 or 0 subsequences (base-case).. 

* So there will be `________` calls made to the partition algorithm, which itself runs in $\Omicron(n)$ time.

* So the total running time for the quicksort algorithm, in this case, is $\Omicron(n^{2})$.



Solution

* The worst case is when the sequence values are already sorted (in ascending or descending order). 
  
* In this case, the partition algorithm will always select the (next) smallest/largest element, resulting in the most unbalanced split possible; two subsequences with $(n-1)$ and $0$ elements.

* Repeated partitioning of this type will occur $\Omicron(n)$ times before both subsequences get down to size 1 or 0 subsequences (base-case).

* So there will be $\Omicron(n)$ calls made to the partition algorithm, which itself runs in $\Omicron(n)$ time.

* So, the total running time for the quicksort algorithm, in this case, is $\Omicron(n^{2})$.



Exercise In the worst-case, quicksort behaves like selection sort. True or false? Why?


Solution

True because each call to partition amounts to selecting the largest (or smallest) element from the subsequence passed to it.





The quicksort runs in $\Omicron(n \lg n)$ time in the **best-case**.



Exercise Complete the following statement.   

* The best case is when the sequence values are not in sorted order. Instead, each pivot selected happens to be the median of its subsequence.
   
* In this case, each recursive call to the quicksort algorithm divides the sequence into two subsequences of `_______________` length.

* Similar to binary search and merge sort, this repeated subdivision takes `________` steps to get down to size 1 or 0 subsequences (base-case).

* So there will be `________` levels in the tree of recursive calls, where each level does $\Omicron(n)$ work.

* So, the total running time for the quicksort algorithm, in this case, is $\Omicron(n \lg n)$.


Solution

* The best case is when the sequence values are not in sorted order. Instead, each pivot selected happens to be the median of its subsequence 
  
* In this case, each recursive call to the quicksort algorithm divides the sequence into two subsequences of *nearly equal* length.

* Similar to binary search and merge sort, this repeated subdivision takes $\lg n$ steps to get down to size 1 or 0 subsequences (base-case).
  
* So there will be $\Omicron(\lg n)$ levels in the tree of recursive calls, where each level does $\Omicron(n)$ work.
  
* So, the total running time for the quicksort algorithm, in this case, is $\Omicron(n \log n)$.





The quicksort runs in $\Omicron(n \lg n)$ time in the **average case**.



The proof is beyond the scope of this course! However, the interested reader is referred to [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition), section $7.2$, Performance of quicksort.



Resources

* Implementation of partition and quicksort as well as a detailed analysis of runtime can be found here: [Quicksort implementation and analysis](https://www.khanacademy.org/computing/computer-science/algorithms/quick-sort/a/overview-of-quicksort).
* [Analysis of quicksort](https://www.khanacademy.org/computing/computer-science/algorithms/quick-sort/a/analysis-of-quicksort) on Khan Academy.
* Wikipedia's entry on [Quicksort: Formal Analysis](https://en.wikipedia.org/wiki/Quicksort#Formal_analysis).



# Selecting the Pivot



* Compare the advantages and disadvantages of various **pivot** choices when implementing the general QuickSort algorithm.



So far, we have selected the rightmost element as the pivot. Our analysis of quicksort was based on this presupposition. The analysis would be the same if we were to choose the leftmost element instead. 

For the case of the sorted sequence, selecting the middle element would result in best-case performance. Generally, the ideal pivot is the _median_ of the sequence to be partitioned.


Why?

Because it results in partitions of nearly equal length.



Employing the median to sort may seem like the "[chicken or the egg](https://en.wikipedia.org/wiki/Chicken_or_the_egg)" problem since finding the median seems to require one to sort the sequence first! There are, however, celever algorithms to compute the median in linear time but those approaches are not considered practical choices.

Alternatively, one might calculate the average of all values or the mid-point value (as $\frac{max + min}{2}$). However, these strategies are $\Omicron(n)$ to compute and only good if the values in the sequence follow a symmetric distribution; e.g. a normal (Gaussian) distribution.

Another strategy is to select a random element as the pivot and hope the choice makes for partitions of *nearly equal* length. It can be shown this strategy results in *expected* $\Omicron(n \lg n)$ worst-case runtime.



A popular choice for pivot is selecting the median of the leftmost, rightmost and the middle elements in the sequence. This strategy is called **the median of the three.**



Exercise Trace the first pass of running quicksort (applying partitioning process) on the following sequence using **the median of the three** for your pivot choice.

$$
20, \space 13, \space 7, \space 71, \space 31, \space 10, \space 5, \space 50, \space 17
$$

After selecting the pivot, swap it with the rightmost element, then apply partitioning.
Use the following table to trace the process; show every swap in a different row.






Solution

Pivot is the median of $\\{20, 31, 17\\}$ which is $20$





Items in bold indicate the elements which were swapped. 




Resources

* Wikipedia's entry on [Quicksort: Choice of pivot](https://en.wikipedia.org/wiki/Quicksort#Choice_of_pivot).
* CMU's 15-451 Lecture notes on [Probabilistic Analysis and Randomized Quicksort](https://www.cs.cmu.edu/afs/cs/academic/class/15451-s07/www/lecture_notes/lect0123.pdf).



# Java Interlude: Built-in Sorts



* Describe what sorting algorithm is used in Java's API.



The Java `Collections.sort()`​ method implements (a [variant](https://en.wikipedia.org/wiki/Timsort)) of merge sort.

The Java `Arrays.sort()` method implements the quicksort, selecting the pivot as the median of the three elements (first, last and middle element) when $n \leq 40$, and the median of nine equally spaced elements when $n > 40$. It also switches to the _insertion sort_ when $n < 7$.


Why Java uses quicksort to sort a collection (array) of primitives but for collections of reference variables, it uses merge sort?

The Java API guarantees a **stable sorting**, which merge sort offers but quicksort doesn't offer. However, when sorting primitive values by their natural order, you would not notice a difference as primitive values have no _identity_. Therefore, quicksort can be used for primitive arrays and will be used when it is considered more efficient.


# Stable Sorting



* Explain what stable sorting means and determine which sorting algorithms (that we learned so far) are stable.



A stable sorting algorithm is one where it maintains the order of elements with duplicate values. This property may be necessary if the sort order is based on a subset of attributes (in an object).

Exercise Fill out the following table to determine which sorting algorithms are stable. Indicate in the third column what special conditions (or data structures) must be employed to ensure the given algorithm is stable. 

|     Sort      | Stable? | Condition |
| :-----------: | :-----: | :-------: |
|  **Bubble**   |         |           |
| **Selection** |         |           |
| **Insertion** |         |           |
|   **Heap**    |         |           |
|   **Merge**   |         |           |
|   **Quick**   |         |           |


Solution

|     Sort      | Stable? |                          Condition                           |
| :-----------: | :-----: | :----------------------------------------------------------: |
|  **Bubble**   |   Yes   |                   Don't swap equal values                    |
| **Selection** |   No    |                                                              |
| **Insertion** |   Yes   |                   Don't swap equal values                    |
|   **Heap**    |   No    |                                                              |
|   **Merge**   |   Yes   |  See below |
|   **Quick**   |   No    |                                                              |

- Bubble sort works by continuously comparing adjacent items so that the most significant items bubble over to the end of the array. The algorithm can be implemented in such a manner that if items are equal, no swapping occurs. 

- Selection sort works by repeatedly finding the minimum element in the unsorted part of a list & swapping it with the first item in the sorted part of the array. Due to the nature of this method, duplicate items would not maintain the same order. 



- Insertion sort works by continuously and sequentially comparing elements and arranging them in a particular order. This algorithm is stable because all identical elements are inserted in the sorted array after one another in the same order discovered. 

- The final sequence of the results from heapsort comes from removing items from the created heap in purely rank order (based on the key field). Therefore any information about the ordering of the items in the original sequence was lost during the heap creation stage, which came first. 

- Merge sort breaks down an array into multiple subarrays of halved size and builds back up to the original collection via comparison. The sort is stable as long as the same pattern for equivalent items is maintained (e.g., always taking left-half values). 

- Quicksort is not stable since it exchanges nonadjacent elements. 






# Dance your sort away!



* Explain and trace the quicksort algorithm on a particular data sequence.



Here quicksort is illustrated through a Hungarian folk dance (created at Sapientia University).







They also danced for [merge sort](https://youtu.be/dENca26N6V4) and [bubble sort](https://youtu.be/lyZQPjUT5B4)!
# Graph Basics

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- Identify the various components of Graph structures (node/vertex, edge, nodes adjacent to a node, edges incident with a node, etc.).
- Identify the operations of Graph ADT.
- Describe adjacency/incidence list vs. matrix-based representation of a graph.
- Analyze and compare the complexity of basic operations for adjacency list vs. matrix-based representations of a graph.

> [Starter code](../../zip/chap26-starter.zip) for this chapter.


Solution code

> This chapter does not have a starter/solution code because a homework is about implementing graphs.



# An abstraction for problem-solving



* Recognize graph as an abstraction for problem-solving.



In mathematics, graphs are a way to formally represent a network, basically a collection of connected objects. We call the objects **nodes** or **vertices**. The connections are called **edges** or **arcs**.







Graph is a super useful abstraction for problem-solving in Computer Science. 



We will look at this abstraction from the lens of Data Structures and Algorithms. 


Resources

* [Applications of Graph Theory In Computer Science: An Overview](http://www.cs.xu.edu/csci390/12s/IJEST10-02-09-124.pdf), S.G.Shirinivas et. al.




# From Definition to Abstraction



* Connect the mathematical definition of Graph to its ADT.



A graph $G = (V, E)$ consists of a nonempty set $V$ of vertices (or nodes) and a collection $E$ of edges (or arcs).







Open the starter code and look at the `Graph.java`, which defines the Graph interface:

```java
/**
 * Graph ADT.
 *
 * @param  Vertex element type.
 * @param  Edge element type.
 */
public interface Graph {
  // Operations not shown
}
```

The Graph interface declaration looks a lot like the Mathematical definition of Graph, but be careful, as `V` and `E` here are not _collection_ of vertices/edges. Instead, they are _generics_, placeholders for the datatype of what will be stored in a vertex or an edge. 
# More Abstractions: Vertex and Edge



* Describe the relationship between the Position abstraction and Vertex and Edge.



We define abstractions for `Vertex` and `Edge`.

```java
/**
 * Edge position for graph.
 * @param  Element type.
 */
public interface Edge extends Position {
}
```

```java
/**
 * Vertex position for graph.
 * @param  Element type.
 */
public interface Vertex extends Position {
}
```

In these simple abstractions, a Vertex and an Edge are just Positions.

```java
/**
 * Generic position interface.
 * @param  the element type.
 */
public interface Position {

  /**
   * Read element from this position.
   * @return element at this position.
   */
  T get();
}
```

The `Graph` interface uses `Vertex` and `Edge` abstractions similar to how the `List` interface used `Position`.







# Graph Interface: Insert Vertices



* Identify the operations of Graph ADT.



The Graph interface declares an operation that takes the data you want to store in a node. Then, it inserts it into the Graph by creating a Vertex. Finally, the operation returns the newly created Vertex to you. 

```java
/**
 * Insert a new vertex.
 *
 * @param v Element to insert.
 * @return Vertex position created to hold element.
 * @throws InsertionException If v is null or already in this Graph
 */
Vertex insert(V v) throws InsertionException;
```

There is also an operation that returns an iterable over the vertices in the Graph.

```java
/**
  * Vertices of the graph.
  *
  * @return Iterable over all graph vertices (in no specific order).
  */
Iterable> vertices();
```

Here is an example for printing the values stored in the vertices of a graph:

```java
for (Vertex v: graph.vertices()) {
    System.out.println(v.get());
}
```
# Graph: Directed vs. Undirected



* Distinguish between directed vs. undirected graph.



There are different kinds of graphs! Two broad categories are directed vs. undirected graphs. 









The difference is in the type of edge they have.









For example, $A$ and $B$ may represent two bus stops. Therefore, the directed edge shows one-way traffic from stop $A$ to stop $B$.

On the other hand, $A$ and $B$ may represent two individuals on Facebook. The undirected edge represents the friendship between them.
# Graph Interface: Directed Graph



* Notice the Graph ADT represents a directed graph.



The Graph interface is meant to be a general interface for directed graphs. 

```java
/**
 * Graph ADT to represent directed graphs.
 *
 * @param  Vertex element type.
 * @param  Edge element type.
 */
public interface Graph {
  // Operations not shown
}
```

Let's note that you can use a directed graph to represent an undirected graph. 





A directed Graph is a *generalization* of an undirected graph. If you need an undirected graph, simply insert two directed edges (presumably with the same data), one in each direction.
# Graph Interface: Insert Edges



* Identify the operations of Graph ADT.



The Graph interface has an operation to insert edges:

```java
/**
 * Insert a new edge.
 *
 * @param from Vertex position where the edge starts.
 * @param to   Vertex position where the edge ends.
 * @param e    Element to insert.
 * @return Edge position created to hold element.
 */
Edge insert(Vertex from, Vertex to, E e);
```

When we add an edge, we specify the two vertices "from" and "to" and the data we may want to store in the edge.

Notice the `insert` method is **overloaded**: we've seen the insert method that created a vertex for us. 

Like the method `vertices`, we have an `edges` method that returns an iterable over edges.

```java
/**
 * Edges of graph.
 *
 * @return Iterable over all edges of the graph 
 *         (in no specific order).
 */
Iterable> edges();
```

# Types of Edge



* Distinguish between loops and multigraph and pseudograph. 



Consider this graph:







An edge can attach a vertex to itself (like $\\{ B,B \\}$); this is called a **loop**. A graph that contains loops is called a *pseudograph*.

There can be **multiple** edges (a.k.a parallel edges) between the same end-points (like $\\{C,D \\}$, which is a double edge). Graphs that have parallel edges are called *multigraph*. 



A **simple** graph is one where there are no loops or multiple edges.



Exercise What kind of a graph is this?
 




A) Directed multigraph \
B) Directed pseudograph \
C) Simple directed graph \
D) Simple undirected graph


Solution

A simple directed graph. 

The edges between $C$ and $B$ are directed (opposite direction), so it is not multiple-edge; thus not a multigraph.



**Aside:** Not every vertex has to have an edge attached to it. 
A vertex with no edges is called an **isolated vertex**.

A graph that has an isolated vertex is not "connected." 



In a **connected graph**, it is possible to get from every vertex in the graph to every other vertex through a series of edges.


# Graph Interface: Insertion Exception



* Identify the operations of Graph ADT.



Our Graph interface is a representation for directed simple graph. Notice the exceptions thrown by the insert method:

```java
/**
 * Insert a new edge.
 *
 * @param from Vertex position where the edge starts.
 * @param to   Vertex position where the edge ends.
 * @param e    Element to insert.
 * @return Edge position created to hold element.
 * @throws PositionException  If either vertex position is invalid.
 * @throws InsertionException If insertion would create a self-loop or
 *                            duplicate edge.
 */
Edge insert(Vertex from, Vertex to, E e)
    throws PositionException, InsertionException;
```

We don't allow loops or multiple edges. On the other hand, there is no exception for when the data `e` is `null`.
# Graph Interface: Getting the Endpoints



* Identify the operations of Graph ADT.



There are two utility methods in the Graph interface where given an Edge, you can ask the abstraction to return the start or end vertex of that edge. 

```java
/**
  * Start vertex of the edge.
  *
  * @param e Edge position to explore.
  * @return Vertex position edge starts at.
  * @throws PositionException If edge position is invalid.
  */
Vertex from(Edge e) throws PositionException;
```

```java
/**
  * End vertex of the edge.
  *
  * @param e Edge position to explore.
  * @return Vertex position edge leads to.
  * @throws PositionException If edge position is invalid.
  */
Vertex to(Edge e) throws PositionException;
```

When you insert an edge, and Graph ADT gives you an Edge object, that Edge abstraction is like a receipt that you can give back to the Graph and ask for more information about the edge. However, the client is not able, on their own, to get data such as the endpoints of an Edge because the Edge abstraction is just a Position. All it provides is a getter to get the data stored in it.

# Adjacent Vertices



* Identify adjacent vertices in a graph.



Two vertices that are connected directly with an edge are called **adjacent** vertices. In the graph below, $v_1$ and $v_2$ are adjacent; $v_2$ and $v_3$ are adjacent; however, $v_1$ and $v_3$ are not adjacent.





Two adjacent vertices forming an edge are said to be **incident** to that edge. In the graph below, $v_1$ is _incident to_ the edges $e_1$ and $e_3$.





**Outgoing** edges of a vertex are _directed_ edges that the vertex is the origin.

**Incoming** edges of a vertex are _directed_ edges that the vertex is the destination.

In the graph above, $\text{out}(v_2)=\\{ e_2\\}$ and $\text{in}(v_2)=\\{e_1 \\}$. Note that $v_1$ has no incoming edges and $v_3$ has no outgoing edges.
# Graph Interface: Incident Edges



* Identify the operations of Graph ADT



We have the following operations in the Graph interface, which return an iterable over the *outgoing* and *incoming* edges of a given vertex.

```java
/**
 * Outgoing edges of a vertex.
 *
 * @param v Vertex position to explore.
 * @return Iterable over outgoing edges of the given vertex
 *         (in no specific order).
 * @throws PositionException If vertex position is invalid.
 */
Iterable> outgoing(Vertex v) throws PositionException;
```

```java
/**
 * Incoming edges of a vertex.
 *
 * @param v Vertex position to explore.
 * @return Iterable over incoming edges of the given vertex
 *         (in no specific order).
 * @throws PositionException If vertex position is invalid.
 */
Iterable> incoming(Vertex v) throws PositionException;
```
# Graph Interface: Removals



* Identify the operations of Graph ADT



There are two `remove` methods in the Graph interface: removing a vertex and removing an edge.

```java
/**
 * Remove a vertex.
 *
 * @param v Vertex position to remove.
 * @return Element that was stored in the removed vertex.
 * @throws PositionException If vertex position is invalid.
 * @throws RemovalException  If vertex still has incident edges.
 */
V remove(Vertex v) throws PositionException, RemovalException;
```

```java
/**
 * Remove an edge.
 *
 * @param e Edge position to remove.
 * @return Element that was stored in the removed edge.
 * @throws PositionException If edge position is invalid.
 */
E remove(Edge e) throws PositionException;
```

By convention, we return the data stored in a vertex or an edge when we remove it. 



Note that you may **not** remove a vertex that still has incident edges. 



In other words, before removing a vertex, you must remove the incoming and outgoing edges incident to it. 

**Aside:** The `remove` method is overloaded. The Java compiler allows overloading based on subtypes. (The compiler does not consider return type when differentiating methods; return type is not part of the method signature.) A motivation behind using Vertex and Edge interfaces as positions (instead of only Position) is that we can overload method names to keep down interface complexity. We also get some degree of static type safety: clients who confuse vertex and edge positions will notice at compile-time.
# Labeled Graph



* Define what is a labeled graph.



A graph is a "labeled graph" if labels are assigned to its vertices or edges (or both).





The label values could be of any type. When the label values are real numbers, the graph is called a **weighted graph**. 
# Graph Interface: Labeling



* Identify the operations of Graph ADT.



There are several operations in the Graph interface about labeling.

Here are four overloaded `label` methods:

```java
/**
 * Label vertex with an object.
 *
 * @param v Vertex position to label.
 * @param l Label object.
 * @throws PositionException If vertex position is invalid.
 */
void label(Vertex v, Object l) throws PositionException;
```

```java
/**
 * Label edge with an object.
 *
 * @param e Edge position to label.
 * @param l Label object.
 * @throws PositionException If edge position is invalid.
 */
void label(Edge e, Object l) throws PositionException;
```

```java
/**
 * Vertex label.
 *
 * @param v Vertex position to query.
 * @return Label object (or null if none).
 * @throws PositionException If vertex position is invalid.
 */
Object label(Vertex v) throws PositionException;
```

```java
/**
 * Edge label.
 *
 * @param e Edge position to query.
 * @return Label object (or null if none).
 * @throws PositionException If edge position is invalid.
 */
Object label(Edge e) throws PositionException;
```

And here is a method that clears all the labels:

```java
/**
 * Clear all labels.
 * All labels are null after this.
 */
void clearLabels();
```

Notice the label type is `Object`. This means there is no requirement for all labels to have the same data type. Moreover, there is no exception thrown when the label value is `null`.
# Preliminaries for Analysis



- Count the degree of a vertex.
- Describe the Handshaking lemma.
- Describe the lower and upper bounds on the number of edges in a simple and connected graph. 



For a graph $G=(V,E)$ with vertex set $V$ and edge set $E$:
* $N = \left | V \right |$ denotes the number of vertices.
* $M = \left | E \right |$ denotes the number of edges.

What can we say about $N$ and $M$?
* Theoretically, $N \in \mathbb{Z}^{+}$. (A graph with an infinite vertex set is called an *infinite* graph!) In practice, $N$ is a positive finite integer. 
* If a graph is **not** connected, $M$ could be as small as $0$.
* If a graph is **not** simple, we may have parallel edges, so $M$ could be (theoretically) as large as $+\infty$.



For a simple, connected graph, the minimum number of edges is $N-1$ (as in a [Tree](https://en.wikipedia.org/wiki/Tree_(graph_theory))). On the other hand, the maximum number of edges is in a [Complete graph](https://en.wikipedia.org/wiki/Complete_graph), where there is an edge between every pair of vertices.

$$
n \space \text{choose} \space 2 = \binom{N}{2}=\frac{N(N-1)}{2}
$$



Therefore, in a simple connected graph, $M \in \Omega(N)$ and $M \in \Omicron(N^2)$.

### Degree of a vertex

The degree of a vertex $v$ is the number of edges incident with $v$. (A loop counts as two edges.)

The degree of the vertex $v$ is denoted by $\deg(v)$.



**Degree sum formula:**

$$
\sum_{v \in V}\deg(v) = 2M
$$




Why?

Each edge contributes twice to the degree count of all vertices. 
Hence, both the left-hand and right-hand sides of this equation equal twice the number of edges.

This is also known as the [Handshaking lemma](https://en.wikipedia.org/wiki/Handshaking_lemma).



# Graph Representation: Adjacency Matrix



* Explain the adjacency matrix representation of a graph.



When it comes to implementing the Graph interface, there are several ways to represent graphs, each with advantages and disadvantages. 

The choice of representation will affect the efficiency of various operations of the Graph ADT. Therefore, depending on the problem at hand, you go for one model or another. 



Here, we'll see two ways to represent graphs: the **adjacency list** vs. the **adjacency matrix**.



An **adjacency matrix** is a $N \times N$ matrix (array), where element $(i,j)$ is 1 if and only if the edge $(v_i, v_j)$ is in $E$.





Thus an adjacency matrix takes up $\Omicron(N^2)$ storage.


Resources

* [Representing Graphs](https://www.khanacademy.org/computing/computer-science/algorithms/graph-representation/a/representing-graphs) by Khan Academy.



# Graph Representation: Adjacency List



* Explain the adjacency list representation of a graph.
* Differentiate between adjacency and incidence list representations. 



An adjacency list is a *list of lists*: each list corresponds to a vertex $u$ and contains a list of vertices adjacent to it.

Here is an example for an undirected graph:





Here is an example for directed graph:





## Incidence List

An incidence list is similar to an adjacency list except that each vertex $u$ is mapped to a list of edges $(u, v)$ incident to $u$.







In many references, the adjacency list is defined as what I have described as an incidence list!



The space requirement for adjacency/incidence list representation is $\Omicron(N+M)$. 

> You need a list of vertices $\Omicron(N)$, and each vertex has a list of its adjacent vertices (or incident edges). The size of the list will be equal to the **degree** of that vertex. The total size is the sum of the degree of all vertices, which is $2M$. (See "degree sum formula" discussed earlier.)
# Graph: Sparse vs Dense



* Distinguish between sparse vs. dense graphs.



A **sparse** graph has relatively few edges, i.e., $M$ is closer to the lower bound $\Omega(N)$.

A **dense** graph has many edges, i.e., $M$ is closer to the upper bound $\Omicron(N^2)$.



Graph representation (implementation) choice will depend on whether the problem at hand is more likely to be a sparse or dense graph!



If your graph is sparse, adjacency list representation will be more efficient (in terms of space complexity).

# Graph ADT: Efficiency of Operations



* Analyze and compare the complexity of basic operations for adjacency list vs. matrix-based representations of a graph.



Here is a summary of the core operations of Graph ADT:

| Operation   | Description |
| :---------- | :---------- |
| `insert(v)` | creates and returns a new Vertex storing element $v$ |
| `remove(V)` | removes vertex $V$ and returns the element stored in it |
| `insert(V, U, e)` | creates and returns a new Edge from vertex $V$ to $U$ (storing $e$) |
| `remove(E)` | removes edge $E$ and returns element stored in it. |
| `vertices()` | returns an iteration of all the vertices of the graph. |
| `edges()` | returns an iteration of all the edges of the graph. |
| `outgoing(V)` | returns an iteration of all outgoing edges of vertex $V$. |
| `incoming(V)` | returns an iteration of all incoming edges of vertex $V$. |

Exercise Complete the following table. Describe the efficiency of each operation for the representation in that column. 

**Assume** given a vertex, we can access it in an adjacency list or matrix (the index corresponding to it) in constant time. 

| Operation   | Adjacency List |  Adjacency Matrix | 
| :---------- | :------------: | :---------------: |
| `insert(v)` |                |                   |
| `remove(V)` |                |                   |
| `insert(V, U, e)` |          |                   |
| `remove(E)` |                |                   | 
| `vertices()` |               |                   |
| `edges()` |                  |                   |
| `outgoing(V)` |              |                   |
| `incoming(V)` |              |                   |


Solution

The answers here entirely depend on how you implement each representation. Many of the assumptions made below are not described in the notes. You must attend the lecture for a more elaborate discussion.



| Operation   | Adjacency List |  Adjacency Matrix | 
| :---------- | :------------: | :---------------: |
| `insert(v)` |     $\Omicron(1)$     |     $\Omicron(N^2)$      |
| `remove(V)` |     $\Omicron(1)$     |     $\Omicron(N^2)$      |
| `insert(V, U, e)` |  $\Omicron(1)$  |      $\Omicron(1)$       |
| `remove(E)` | $\Omicron(\max(\deg(V),\deg(U)))^*$ | $\Omicron(1)$ | 
| `vertices()`  |   $\Omicron(1)$     |     $\Omicron(1)^*$      |
| `edges()`     |   $\Omicron(M)$     |     $\Omicron(N^2)$      |
| `outgoing(V)` | $\Omicron(\deg^{+}(V))$ |   $\Omicron(N)$      |
| `incoming(V)` | $\Omicron(\deg^{-}(V))$ |   $\Omicron(N)$      |

* `remove(E)` in adjacency list: if $E$ has the endpoints $V$ and $U$, we must find (linear search) and remove $E$ from the list of edges associated with $V$ and with $U$.

* `vertices()` in the adjacency matrix is $\Omicron(1)$ by keeping an explicit list in addition to the adjacency matrix representation.



In most cases, "Adjacency List" is preferred for efficient operations: especially if the operation involves "exploring" the graph.




# Leonhard Euler



* Recognize Leonhard Euler as the father of Graph Theory. 



The great mathematician Leonhard Euler (1707–1783) is the father of Graph Theory. He became interested in [a problem](https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg) around 1735 and published a solution in 1741.





[Leonhard Euler](https://en.wikipedia.org/wiki/Leonhard_Euler)




Euler's solution opened up an entirely new branch of mathematics, now known as Graph Theory.

Euler's great intuition in modeling the problem he was trying to solve was that he recognized the physical arrangement of the objects does not matter in modeling his problem, only the _relationships_ (connections) mattered. 
And that’s what a graph is! It does not matter where the vertices are located or what the edges are shaped like. All that matters is the connections made by the edges, not the particular geometry depicted.  





These five figures (above) all represent the same graph!

I encourage you to read more on the story of how Euler discovered Graphs: [Königsberg: Seven Small Bridges, One Giant Graph Problem](https://medium.com/basecs/k%C3%B6nigsberg-seven-small-bridges-one-giant-graph-problem-2275d1670a12).


Resources

* [A Gentle Introduction To Graph Theory](https://medium.com/basecs/a-gentle-introduction-to-graph-theory-77969829ead8).
* Wikipedia's entry on [Graph Theory](https://en.wikipedia.org/wiki/Graph_theory).
* Discrete Mathematics: An Open Introduction, 3rd edition. [Chapter 4 Graph Theory](http://discrete.openmathbooks.org/dmoi3/ch_graphtheory.html). 


# Graph Search

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

* Define the **neighborhood** of a vertex.
* Identify a **path** from a source vertex to a destination vertex.
* State the **General Graph Search** problem.
* Identify **reachable** vertices in a graph from a source vertex.
* Identify a general solution to the general graph search problem.
* Generate and trace Graph search types by specifying how to explore the neighbor in the general search algorithm.
* Describe, trace, and implement the **Breadth-First Search** algorithm.
* Describe, trace, and implement the **Depth-First Search** algorithm.
* Identify which data structure supports each traversal/search type: breadth-first, depth-first
* Analyze the time complexity of BFS and DFS for each Graph implementation (list vs. matrix)

> This chapter does not have a starter/solution code.
# Neighborhood of a Vertex



* Define the neighborhood of a vertex.



Consider the following graph:





The set of all vertices in $G$ adjacent to a vertex $v$ is called the **neighborhood** of $v$ and denoted by $N(v)$.

$$
N_{\text{outgoing}}(A) = \\{B, C\\} 
$$

$$
N_{\text{incoming}}(C) = \\{A, D\\} 
$$

Exercise Name all the vertices *adjacent* to vertex $D$ in two groups – incoming neighbors and outgoing neighbors:


Solution

$$
N_{\text{outgoing}}(D) = \\{C, E\\} 
$$

$$
N_{\text{incoming}}(D) = \\{B\\} 
$$


# Path



* Identify a path from a source vertex to a destination vertex.





A **path** is a sequence of consecutive edges in a graph. 



Alternatively, we can define "path" as a sequence of vertices where each vertex in the sequence is adjacent to the vertex next to it. 

Consider the following graph:





Here are two paths from $A$ to $C$: $(A, C)$ and $(A, B, D, C)$.



A **simple path** is a path that does not repeat any nodes or edges.



In this class, when I say "path," I mean "simple path."

**Aside:** In some references, what I defined as "path" is described as "walk," and instead "simple path" is called, simply, "path."

Exercise List the edges on a **directed path** from $B$ to $E$ and from $C$ to $E$.


Solution

* Directed path from $B$ to $E$: $((B, D), (D, E))$.
* There is no directed path from $C$ to $E$.



# Graph Search: Definition



* State the general graph search problem.



The **Graph Search** problem, in a nutshell, is figuring out if a graph contains a path from one vertex to another.

Many fundamental algorithms on graphs (e.g., finding shortest path, cycles, connected components, $\dots$) are applications of the graph search problem.



**General Graph Search Problem**

**Input**: Graph $G=(V, E)$, and a starting vertex $s \in V$.\
**Goal:** Identify the vertices in $V$ reachable from $s$ in $G$.



For example, consider the following graph: (It's one graph with multiple connected components!)





The set of vertices reachable from $s$ is $\\{s, u, v, w\\}$.


# Graph Search: Exercise 



* Identify reachable vertices in a graph from a source vertex.



Exercise Identify the set of vertices reachable from $s$ and from $x$, in the following graph:






Solution

* The set of vertices reachable from $s$ is $\\{s, u, v\\}$.
* The set of vertices reachable from $x$ is $\\{x\\}$.





# Graph Search: General Solution



* Identify a general solution to the general graph search problem.



Recall:



**General Graph Search Problem**

**Input**: Graph $G=(V, E)$, and a starting vertex $s \in V$.\
**Goal:** Identify the vertices in $V$ reachable from $s$ in $G$.



The following is a solution to this problem:

```js
// Post: a vertex is reachable from s iff it is marked as explored.
mark s as "explored"; all other vertices as "unexplored"
while there is an edge (v, w) in E with v explored and w unexplored do 
    choose some such edge (v, w) // underspecified 
    mark w as explored
```

Notice the instruction marked as "underspecified"; depending on how we choose the edge, the search will be called:
* **BFS** (Breadth-First Search), or 
* **DFS** (Depth-First Search).
# Breadth-First Search



* Describe the Breadth-First Search algorithm.



According to the [Dictionary of Algorithms and Data Structures](https://xlinux.nist.gov/dads/HTML/breadthfirst.html):



Breadth-First Search, or BFS, is any search algorithm that considers neighbors of a vertex, that is, outgoing edges of the vertex's predecessor in the search, before any outgoing edges of the vertex. *Extremes are searched last*.



Given a "source" vertex to initiate the search, BFS starts by visiting its adjacent nodes. All nodes can be reached by a path from the start node containing two edges, three edges, etc. 



The BFS algorithm visits all vertices in a graph $G$ that are $k$ edges away from the source vertex $s$ before visiting any vertex $k+1$ edges away. You have seen this behavior in **level-order** *tree traversal*. 



The process is further elaborated using a demo:


Demo



The following animated visualization of the BFS algorithm (made by Gerry Jenkins) does a good job of illustrating its behavior:






Resources

* Wikipedia's entry on [Breadth-First Search](https://en.wikipedia.org/wiki/Breadth-first_search).
* Brilliant's Wiki entry on [Breadth-First Search](https://brilliant.org/wiki/breadth-first-search-bfs/).
* [Interactive visualization of BFS](https://www.cs.usfca.edu/~galles/visualization/BFS.html).
* [Video Lecture](https://youtu.be/s-CYnVz-uh4) on YouTube from MIT OpenCourseWare, Introduction to Algorithms, Fall 2011.





# BFS Exercise



* Trace the Breadth-First Search algorithm.



Consider the following graph:





Exercise Write the vertices of the above graph in the order in which they would be visited in a breadth-first traversal starting at node $0$. Assume neighbors are visited in numerical order.


Solution

| Queue   | Edges    | Explored |
| :-----: | :------: | :------: |
| 0       |  -       | 0        |
| 1       | (0, 1)   | 1        |
| 1, 2    | (0, 2)   | 2        |
| 1, 2, 5 | (0, 5)   | 5        |
| 2, 5, 3 | (1, 3)   | 3        |
| 5, 3, 4 | (2, 4)   | 4        |
| 3, 4, 7 | (5, 7)   | 7        |
| 4, 7, 6 | (3, 6)   | 6        |
| 7, 6    |  -       | -        |
| 6       |  -       | -        |
| 8       | (6, 8)   | 8        |
| -       |  -       | -        |

The answer is $0, 1, 2, 5, 3, 4, 7, 6, 8$.



# BFS Pseudocode



* Implement the Breadth-First Search algorithm.



Exercise Based on your understanding of the BFS process, complete the pseudocode of BFS!

```text
mark s as explored;all other vertices as unexplored
______________ data structure, initialized with s 
while____is not empty do
  remove the vertex from ____________, call it v 		
  for edge (v, w) in v's neighborhood do
    if ____________ then
      _________________________
      _________________________
```


Solution

```text
mark s as explored, all other vertices as unexplored
Q := a queue data structure, initialized with s 
while Q is not empty do
  remove the vertex from the front of Q, call it v 
  for edge (v, w) in v's neighborhood do
    if w is unexplored then
      mark w as explored 
      add w to the end of Q
```



# Depth-First Search



* Describe the Depth-First Search algorithm.



According to the [Dictionary of Algorithms and Data Structures](https://xlinux.nist.gov/dads/HTML/depthfirst.html):



Depth-First Search, or DFS, is any search algorithm that considers outgoing edges (children) of a vertex before its siblings, that is, outgoing edges of the vertex's predecessor in the search. *Extremes are searched first*.



The main idea behind DFS is to explore deeper into the graph whenever possible. Starting at a vertex, DFS will take a path and explore it as far as it goes. It then _backtracks_ until it reaches an unexplored neighbor (a branch on the path it has not explored yet). This process continues until every vertex that is reachable from the original source vertex has been discovered. 

> You have seen this behavior in pre-order and post-order tree traversal (and in-order binary tree traversal).

The process is further elaborated using a demo:


Demo



The following animated visualization of DFS algorithm (made by Gerry Jenkins) does a good job of illustrating its behavior:






Resources

* Wikipedia's entry on [Depth-First Search](https://en.wikipedia.org/wiki/Depth-first_search).
* Brilliant's Wiki entry on [Breadth-First Search](https://brilliant.org/wiki/depth-first-search-dfs/).
* [Interactive visualization of DFS](https://www.cs.usfca.edu/~galles/visualization/DFS.html).
* [Video Lecture](https://youtu.be/AfSk24UTFS8) on YouTube from MIT OpenCourseWare, Introduction to Algorithms, Fall 2011.


# DFS Exercise



* Trace the Depth-First Search algorithm.



Consider the following graph:





Exercise Write the vertices of the above graph in the order in which they would be visited in a depth-first traversal starting at node $0$. Assume neighbors are visited in numerical order.


Solution

| Stack      | Edges  | Explored |
| :--------: | :----: | :------: |
| 0          |  -     | 0 |
| 1          | (0, 1) | 1 |
| 2, 1       | (0, 2) | 2 |
| 5, 2, 1    | (0, 5) | 5 |
| 7, 2, 1    | (5, 7) | 7 |
| 4, 2, 1    | (7, 4) | 4 |
| 3, 2, 1    | (4, 3) | 3 |
| 6, 3, 2, 1 | (4, 6) | 6 |
| 8, 3, 2, 1 | (6, 8) | 8 |
| 3, 2, 1    |  -     | - |
| 2, 1       |  -     | - |
| 1          |  -     | - |
| -          |  -     | - |

The answer is $0, 1, 2, 5, 7, 4, 3, 6, 8$.



# DFS Pseudocode 



* Implement the Depth-First Search algorithm.



Exercise Based on your understanding of the DFS process, complete the pseudocode of DFS!

```text
mark s as explored;all other vertices as unexplored
______________ data structure, initialized with s 
while____is not empty do
  remove the vertex from ____________, call it v 		
  for edge (v, w) in v's neighborhood do
    if ____________ then
      _________________________
      _________________________
```


Solution

```text
mark s as explored, all other vertices as unexplored
S := a stack data structure, initialized with s 
while S is not empty do
  pop the vertex from the top of S, call it v 
  for each edge (v, w) in v's neighborhood do
    if w is unexplored then
      mark w as explored 
      push w to the top of S
```



# Graph Search: Analysis



* Analyze the time complexity of BFS and DFS for each Graph implementation (list vs. matrix).



Here is a (more elaborate) pseudocode for solving the General Graph Search problem:

```text
mark s as explored, all other vertices as unexplored
D := a queue or stack data structure, initialized with s 
while D is not empty do
  remove the vertex from the front/top of D, call it v 
  for edge (v, w) in v's neighborhood do
    if w is unexplored then
      mark w as explored 
      add w to the end/top of D
```



Notice the difference between BFS and DFS is that DFS uses **stack** but BFS uses **queue**.



Exercise Analyze the complexity of BFS algorithm (use Big-Oh notation).


Solution

```text
mark s as explored, all other vertices as unexplored // O(1), O(N)
D := a queue or stack data structure, initialized with s // O(1)
while D is not empty do                              // total O(N)     
  remove the vertex from the front/top of D, call it v  // O(1)
  for edge (v, w) in v’s neighborhood do                // O(neighbors(v))
    if w is unexplored then                             // O(1)
      mark w as explored                                // O(1)
      add w to the end/top of D                         // O(1)
```

* Both search explore each edge at most once (for directed graphs), or twice (undirected graphs — once
when exploring each endpoint).
* After edge $(v, u)$ is encountered, both $v$ & $u$ are marked as explored.
* We can implement the search in linear time if we can find eligible $(v, u)$ quickly (for each $v$)
* This is where adjacency (incidence) list will provide fast access.
* $\Omicron(\text{neighbors}(v))$ is $\Omicron(\deg(v))$ in incidence list (but it is $\Omicron(N)$ in adjacency matrix).
* $N \times O(\deg(v))$ is $\Omicron(M)$ because Handshaking lemma says $\sum_{v \in V} \deg(v) = 2M$.
* So in adjacency list, finding (unexplored) neighbors of each vertex takes total of $\Omicron(M)$ time.
* (In adjacency matrix, this total would be $\Omicron(N^2)$ : $N$ for `neighbors(v)` $\times$ $N$ vertices).
* Note that we can check $u$ is unexplored in $\Omicron(1)$ if we store this information in the vertex node (or HashTable of explored vertices where keys are the nodes). 



The total running time of BFS & DFS is $\Omicron(M+N)$ if we use adjacency list representation.



The space complexity of a DFS, in practice, is usually lower than that of BFS. This is because, during BFS, all the nodes at one level must be stored, whereas in DFS, all the nodes in one path need to be stored. Thus, for instance, in a tree, the number of nodes per level usually exceeds the depth of the tree.



# Graph Search: Summary



* Summarize graph search algorithms.



| BFS | DFS |
| --- | --- |
| Starts the search from the source node and visits nodes in a level-by-level manner (i.e., visiting the ones closest to the source first). | Starts the search from the source node and visits nodes as far as possible from the source node (i.e., depth-wise). |
| Usually implemented using a queue data structure.            | Usually implemented using a stack data structure.            |
| Used for finding the shortest path between two nodes, testing if a graph is bipartite, finding all connected components in a graph, etc. | Used for topological sorting, solving problems that require graph backtracking, detecting cycles in a graph, scheduling problems, etc. |



Both BFS & DFS run in $\Omicron(M+N)$ if we use the adjacency list. That's just a constant factor larger than the amount of time required to read the input!



**Note:** It is common to modify the BFS/DFS algorithm to keep track of the edges instead of (or in addition to) the vertices (where each edge describes the nodes at each end). This is useful, e.g., for reconstructing the traversed path after processing each node.

**Aside:** Both BFS & DFS can be implemented **recursively**. In particular, DFS easily lends itself to a recursive implementation. Most resources describe DFS recursively! It is left as an exercise to you to develop the recursive implementation of these algorithms.


# Shortest Path Problem

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- Describe a general Graph Search problem variant that finds a path between a source and every reachable vertex.
- Modify BFS/DFS to find a path between a source and every reachable vertex.
-  Describe a general Graph Search problem variant that finds a distance between a source and every reachable vertex. Distance is defined as the length of a path from the source to that vertex.
- Modify BFS/DFS to find the distance between a source and every reachable vertex where distance is the length of a path from the source to that vertex.
- Trace **shortest path algorithm** in unweighted graph by specifying the values in auxiliary data structures. 
- Analyze the running time of the (unweighted) shortest path algorithm, assuming an incidence/adjacency list Graph implementation.
- Recognize that (unweighted) shortest path algorithm is a modified BFS.
- Explain why the modified BFS will not find the shortest path in weighted graphs.
- Trace **Dijkstra's algorithm** (shortest path in weighted graph) by specifying the values in auxiliary data structures. 
- Analyze the running time of Dijkstra's algorithm, assuming an incidence/adjacency list Graph implementation.
- Describe the role of support data structures in reducing the running time of Dijkstra's algorithm from quadratic to log-linear.
- Implement Dijkstra's algorithm efficiently for application to a specific problem.

> This chapter does not have a starter/solution code.
# Graph Search Problem: Modify the Goal



* Describe a variant of the general Graph Search problem that finds a path between a source and every reachable vertex.



Recall:



**General Graph Search Problem**

**Input**: Graph $G=(V, E)$, and a starting vertex $s \in V$.\
**Goal:** Identify the vertices in $V$ reachable from $s$ in $G$.



We slightly modify the objective as follows:



**Goal:** Identify a path from $s$ to each (reachable) vertex in $G$. 



We will see how we can modify the solution to the General Graph Search Problem to solve the modified version of it.  
# Modified Search Problem: Find Path



* Modify BFS/DFS to find a path between a source and every reachable vertex.



**Idea:** To produce a path for each vertex, keep track of the vertex from which it was explored during the BFS/DFS process.

The demo here uses BFS, but we could do the same with DFS!


Demo



The following pseudocode describes the BFS algorithm. 

```js
// Pre: "s" is the source vertex
explored = {}
explored[s] = true 
queue.enqueue(s)
while (!queue.empty())
    v = queue.dequeue()
    for (w in adjacency[v])
        if (!explored[w])
            explored[w] = true    
            queue.enqueue(w)
```

Exercise Modify it to include and update the `previous` collection according to the demo.


Solution

```js
previous = {}
explored = {}
explored[s] = true 
queue.enqueue(s)
while (!queue.empty())
    v = queue.dequeue()
    for (w in adjacency[v])
        if (!explored[w])
            explored[w] = true    
            queue.enqueue(w)
            previous[w] = v
```



Exercise Assuming the modified BFS produced the `previous` collection. Use `previous` to print out a path from the source to any given vertex.


Solution

```js
// Pre: target is reachable from source
node = target
stack.push(node)
while (node != source)
   node = previous[node]
   stack.push(node)

print stack
```


# Modified Search Problem: Find Distance



- Describe a variant of the general Graph Search problem that finds the distance between a source and every reachable vertex. Distance is defined as the length of a path from the source to that vertex.
- Modify BFS/DFS to find the distance between a source and every reachable vertex, where distance is the length of a path from the source to that vertex.



Let us further modify the goal of the General Graph Search problem:



**Goal:** Find the distance of each (reachable) vertex in $G$ to $s$, where "distance" is defined as the **length** of a path from $s$ to the other vertex.



Let's make the observation that on a path from $s$ to $v$ and then to $u$ following the edge $(v, u)$, we have $d(u)=d(v)+1$ where $d(x)$ is the distance of vertex $x$ to the source $s$.





The demo here uses BFS, but we could do the same with DFS!


Demo



The following pseudocode describes the BFS algorithm. 

```js
// Pre: "s" is the source vertex
explored = {}
explored[s] = true 
queue.enqueue(s)
while (!queue.empty())
    v = queue.dequeue()
    for (w in adjacency[v])
        if (!explored[w])
            explored[w] = true    
            queue.enqueue(w)
```

Exercise Modify it to include and update the `distance` collection according to the demo.


Solution

```js
distance = {}
explored = {}
distance[s] = 0 
queue.enqueue(s)
while (!queue.empty())
    v = queue.dequeue()
    for (w in adjacency[v])
        if (!explored[w])
            explored[w] = true    
            queue.enqueue(w)
            distance[w] = distance[v] + 1
```


# Modified Search Problem: Directed Graph



- Recognize BFS/DFS can be carried on a directed graph.
- Trace the shortest path algorithm in an unweighted graph by specifying the values in auxiliary data structures. 
- Analyze the running time of the (unweighted) shortest path algorithm, assuming an incidence/adjacency list Graph implementation.



The BFS/DFS algorithm can be carried out on a directed graph. The only adjustment would be to consider each vertex's "outgoing neighbors" during "exploration."

Consider the following directed graph:





Exercise Carry out the BFS algorithm on the graph above starting at vertex $A$. Keep track of `previous` and `distance` values for each vertex. Reflect on the complexity of the algorithm.


Solution


* The algorithm's complexity is the same as the simple BFS algorithm; it runs in $\Omicron(N+M)$.
* The modified BFS requires more auxiliary space, although it is asymptotically linear (same as plain BFS).



# Shortest Path: Unweighted Graph



- Formally describe the shortest path problem.
- Recognize that (unweighted) shortest path algorithm is a modified BFS.



Let us formally define the _shortest path_ problem:



**Shortest Path Problem**

**Input**: Graph $G=(V, E)$, and a starting vertex $s \in V$.\
**Goal:** Identify a shortest path from $s$ to every vertex in $G$.



Note we are considering the **length** of a path here. In other words, "shortest path" means "shortest length." (We will soon contrast this with the case where shortest path means shortest _cost_.)



**Lemma**: Let $G$ be a directed or undirected graph. At the conclusion of the modified BFS, for every vertex $v \in V$, the value `distance[v]` equals the length $\text{dist}(s, v)$ of a **shortest path** from $s$ to $v$ in $G$ (or $+ \infty$ if no such path exists).



This is not the case for DFS!


Why does BFS find the shortest path?

Okay! Let's think about this problem for a moment. We have a source vertex $s$ and a target vertex $t$. We want a shortest path from $s$ to $t$ (if exists). What can we do? Well, we can look at all the vertices that are one edge away from $s$; if we find $t$, we are done. If not, we can look at all vertices that are two edges away from $s$, then those that are three edges away from $s$, and so on until we find $t$. This strategy is what BFS is doing! It explores all (reachable) vertices that are $k$ edges away from the source vertex $s$ before visiting any vertex $k+1$ edges away. So, if there are multiple paths to $t$, and one returned by BFS has $d$ edges, that path is the shortest. If there was a shorter path with, e.g., length of $d-1$, then $t$ would have been explored before exploring vertices that are $d$ edges away from the source. 

> For a formal proof, refer to [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition), Third Edition, Lemma 22.2 on page 598.




Resources

* Wikipedia's entry on [Shortest Path Problem](https://en.wikipedia.org/wiki/Shortest_path_problem).



# Shortest Path: Weighted Graph



* Explain why the modified BFS will not find the shortest path in weighted graphs.



**Recall:** A weighted graph is a labeled graph where the edge labels are numbers.





Notice in the graph above, there are two paths between $A$ and $C$. The shortest (in terms of length) is $(A, C)$. However, the shortest (cheapest) in terms of **total** weight (cost) is $(A, B, D, C)$.



The shortest path problem is generally defined in terms of "cost." We can set the edge weights to $1$, giving us the shortest path in terms of length.



Exercise Can BFS be used to find the shortest path when shortest means "cheapest"?


Solution

BFS will not work, as seen in the example graph. BFS will explore $B$ and $C$ at the same time (both are one edge away from $A$) and concludes the shortest path from $A$ to $C$ is the direct edge $(A,C)$. BST will not update this "shortest" path when it finds the second path (which is cheaper) from $D$ to $C$ because, at that point, $C$ is already "explored."






 

# Dijkstra's Algorithm



* Trace Dijkstra's algorithm (shortest path in weighted graphs) by specifying the values in auxiliary data structures.



[E. W. Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra) (1930-2002), the legendary Dutch Computer Scientist (and Turing Award winner), discovered an algorithm for finding the shortest path (in weighted graphs).





Dijkstra's algorithm works by exploring the (unexplored) neighbors of the next vertex with the smallest distance to the source vertex. For this reason, the algorithm is also called *Shortest Path First* (SPF) algorithm.

The intuition behind Dijkstra's algorithm is that if vertex $B$ is on the shortest path from $A$ to $D$, then the _subpath_ from $B$ to $D$ is also the shortest path between vertices $B$ and $D$.





> For a rigorous analysis and formal proof of correctness, refer to [CLRS](https://mitpress.mit.edu/books/introduction-algorithms-third-edition), Third Edition, Chapter 24, Section 3, Dijkstra's Algorithm on page 658.

Here we will use a demo to understand the steps involved in Dijkstra's Algorithm.


Demo



Exercise Complete the pseudocode for Dijkstra's Algorithm:

```js
for each vertex v
    distance[v] = Infinity
    previous[v] = null
    explored[v] = false
distance[s] = 0   // s is the source
repeat N times
    let v be unexplored vertex with smallest distance
    ________________
    for every u: unexplored neighbor(v)
        d = distance[v] + weight[v,u]  
        if ________________
            _______________
            _______________
```


Solution

```js
for each vertex v
    distance[v] = Infinity
    previous[v] = null
    explored[v] = false
distance[s] = 0   // s is the source
repeat N times
    let v be unexplored vertex with smallest distance
    explored[v] = true
    for every u: unexplored neighbor(v)
        d = distance[v] + weight[v,u]  
        if d < distance[u]
            distance[u] = d
            previous[u] = v
```





Resources

* Wikipedia's entry [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm).
* Brilliant's Wiki on Dijkstra's [Shortest Path Algorithm](https://brilliant.org/wiki/dijkstras-short-path-finder/).
* Programiz Tutorial on [Dijkstra's Algorithm](https://www.programiz.com/dsa/dijkstra-algorithm).
* [Dijkstra's Shortest Path Algorithm - A Detailed and Visual Introduction](https://www.freecodecamp.org/news/dijkstras-shortest-path-algorithm-visual-introduction/) on FreeCodeCamp.
* Computerphile YouTube Video on [Dijkstra's Algorithm](https://youtu.be/GazC3A4OQTE) - highly recommended!



# Dijkstra's Algorithm: Exercise



* Trace Dijkstra's algorithm (shortest path in weighted graphs) by specifying the values in auxiliary data structures.



Exercise Find the weighted shortest path from vertex $3$ to vertex $5$ in the _digraph_ (directed graph) below. 





The following parts will guide you through the process.  

1. Fill out the table below with each vertex and its corresponding outgoing vertices.

| Vertex | Outgoing |              
| :----: | :------: | 
| 0      |          |
| 1      |          |
| 2      |          |
| 3      |          |
| 4      |          |
| 5      |          |


Solution

| Vertex | Outgoing |              
| :----: | :------: |
| 0      | 1, 2, 4, 5 |
| 1      | 4, 5     |
| 2      | 3, 4     |
| 3      | 2        |
| 4      | 0, 1, 5  |
| 5      | none     |




2. Start with the following default values in Table 1 below (fill out the answers from part-1 in the "Outgoing" column). How would these values change after exploring vertex $3$? Next, fill out Table 2 with your response. What is now the unexplored vertex with the smallest distance from vertex $3$?

Table 1: Default values

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | INFINITY | null | no |
| 1 | | INFINITY | null | no |
| 2 | | INFINITY | null | no |
| 3 | | INFINITY | null | no |
| 4 | | INFINITY | null | no |
| 5 | | INFINITY | null | no |

Table 2: After exploring vertex 3

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Unexplored vertex with the smallest distance: 2

Table 1: Default values

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | INFINITY | null | no |
| 1 | 4, 5 | INFINITY | null | no |
| 2 | 3, 4 | INFINITY | null | no |
| 3 | 2 | INFINITY | null | no |
| 4 | 0, 1, 5 | INFINITY | null | no |
| 5 | none | INFINITY | null | no |

Table 2: After exploring vertex 3

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | INFINITY | null | no |
| 1 | 4, 5 | INFINITY | null | no |
| 2 | 3, 4 | **10** | **3** | no |
| 3 | 2 | **0** | null | **yes** |
| 4 | 0, 1, 5 | INFINITY | null | no |
| 5 | none | INFINITY | null | no |




3. How would these values change after exploring vertex $2$? Fill out Table 3 with your response. What is now the unexplored vertex with the smallest distance from vertex $3$?
    
Table 3: After exploring vertex 2

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Unexplored vertex with the smallest distance: $4$  

Table 3: After exploring vertex 2

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |           
| 0 | 1, 2, 4, 5 | INFINITY | null | no |
| 1 | 4, 5 | INFINITY | null | no |
| 2 | 3, 4 | 10 | 3 | **yes** |
| 3 | 2 | 0 | null | yes |
| 4 | 0, 1, 5 | **22** | **2** | no |
| 5 | none | INFINITY | null | no |




4. How would these values change after exploring vertex $4$? Fill out Table 4 with your response. What is now the unexplored vertex with the smallest distance from vertex $3$? 
    
Table 4: After exploring vertex 4

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Unexplored vertex with the smallest distance: $0$  

Table 4: After exploring vertex 4

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | **34** | **4** | no |
| 1 | 4, 5 | **52** | **4** | no |
| 2 | 3, 4 | 10 | 3 | yes |
| 3 | 2 | 0 | null | yes |
| 4 | 0, 1, 5 | 22 | 2 | **yes** |
| 5 | none | **50** | **4** | no |



5. How would these values change after exploring vertex $0$? Fill out Table 5 with your response. What is now the unexplored vertex with the smallest distance from vertex $3$? 

Table 5: After exploring vertex 0

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Unexplored vertex with the smallest distance: $1$  

Table 5: After exploring vertex 0

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | 34 | 4 | **yes** |
| 1 | 4, 5 | **44** | **0** | no |
| 2 | 3, 4 | 10 | 3 | yes |
| 3 | 2 | 0 | null | yes |
| 4 | 0, 1, 5 | 22 | 2 | yes |
| 5 | none | **49** | **0** | no |




6. How would these values change after exploring vertex $1$? Fill out Table 6 with your response. What is now the unexplored vertex with the smallest distance from vertex $3$? 
    
Table 6: After exploring vertex 1

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Unexplored vertex with the smallest distance: $5$  

Table 6: After exploring vertex 1

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | 34 | 4 | yes |
| 1 | 4, 5 | 44 | 0 | **yes** |
| 2 | 3, 4 | 10 | 3 | yes |
| 3 | 2 | 0 | null | yes |
| 4 | 0, 1, 5 | 22 | 2 | yes |
| 5 | none | 49 | 0 | no |




7. How would these values change after exploring vertex $5$? Fill out Table 7 with your response.  
    
Table 7: After exploring vertex 5

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | | | | |
| 1 | | | | |
| 2 | | | | |
| 3 | | | | |
| 4 | | | | |
| 5 | | | | |


Solution

Table 7: After exploring vertex 5

| Vertex | Outgoing | Distance from 3 | Previous | Explored |             
| :----: | :------: | :-------------: | :------: | :------: |
| 0 | 1, 2, 4, 5 | 34 | 4 | yes |
| 1 | 4, 5 | 44 | 0 | yes |
| 2 | 3, 4 | 10 | 3 | yes |
| 3 | 2 | 0 | null | yes |
| 4 | 0, 1, 5 | 22 | 2 | yes |
| 5 | none | 49 | 0 | **yes** |




1. What is the weighted shortest path from vertex 3 to 5? What is the total distance of this path?  


Solution

The weighted shortest path from vertex $3$ to vertex $5$ is: 

$$
3 \implies 2 \implies 4 \implies 0 \implies 5
$$

And the distance is $49$.


# Dijkstra's Algorithm: Analysis



- Analyze the running time of Dijkstra's algorithm, assuming an incidence/adjacency list Graph implementation.
- Describe the role of support data structures in reducing the running time of Dijkstra's algorithm from quadratic to log-linear.



Here is the Dijkstra's Algorithm for finding shortest path: 

```js
for each vertex v
    distance[v] = Infinity
    previous[v] = null
    explored[v] = false
distance[s] = 0   // s is the source
repeat N times
    let v be unexplored vertex with smallest distance
    explored[v] = true
    for every u: unexplored neighbor(v)
        d = distance[v] + weight[v,u]  
        if d < distance[u]
            distance[u] = d
            previous[u] = v
```

Exercise Analyze the complexity of SPF algorithm (use Big-Oh notation).


Solution

```js
for each vertex v            // O(N)
    distance[v] = Infinity      // O(1)
    previous[v] = null          // O(1)
    explored[v] = false         // O(1)
distance[s] = 0              // O(1)
repeat N times               // O(N)
    let v be unexplored vertex with smallest distance  // O(?)
    explored[v] = true          // O(1)
    for every u: unexplored neighbor(v)  // O(neighbor(v))
        d = distance[v] + weight[v,u]       // O(1)
        if d < distance[u]                  // O(1)
            distance[u] = d                 // O(1) 
            drevious[u] = v                 // O(1) 
```

Using incidence/adjacency list representation will make $\Omicron(\text{neighbor}(v))$ to be $\Omicron(\deg(v))$. Repeating this $N$ times will give runtime of $\Omicron(M)$ for this part of the algorithm.

Let's focus on $\Omicron(?)$:

* Finding (an unexplored) vertex with min distance is $\Omicron(N)$ if we store the "distances" in a linear data structure such as an array.
* Since the above will be repeated $N$ times, it pushes the running time of the algorithm to $\Omicron(N^2)$
* You can use a **priority queue** (min-heap; priority is distance) to get $\Omicron(\lg N)$ on finding (an unexplored) vertex with min distance.
* But you must also update the distances! How can you do that in Priority Queue? (This is left unanswered - you need it for HW8!)
  
If $\Omicron(?)$ is $\Omicron(\lg N)$ (using a [modified] priority queue), we get total running time of $\Omicron(M + N \lg N)$.





# Shortest Path: Summary



* Summarize shortest path algorithms.





**Shortest Path Problem**

**Input**: Graph $G=(V, E)$, and a starting vertex $s \in V$.\
**Goal:** Identify the shortest path from $s$ to every vertex in $G$.



There are many variations of the Shortest Path problem:
* weighted vs. unweighted (all weights equal)
* cyclic vs. acyclic
* positive weights only vs. negative weights allowed 
* single source vs. multi-source
* etc. 

The (modified) BFS can be used to solve the problem for the single-source unweighted graph. Dijkstra's algorithm can be used for a single-source weighted graph (when weights are non-negative). There are other algorithms for other variants of the problem. For instance, the [Bellman–Ford algorithm](https://en.wikipedia.org/wiki/Bellman–Ford_algorithm), the [Floyd–Warshall algorithm](https://en.wikipedia.org/wiki/Floyd–Warshall_algorithm), the [Johnson's algorithm](https://en.wikipedia.org/wiki/Johnson's_algorithm), the [Viterbi's algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm), etc.


# Minimum Spanning Tree

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- In the context of Graph Theory, define Cycle, Acyclic Graph, and Tree.
- Describe what a spanning tree is and why constructing minimum spanning trees is useful.
- Trace Prim's algorithm for finding a minimum spanning tree.
- Trace Kruskal's algorithm for finding a minimum spanning tree.
- Explain how to implement Prim's algorithm, comparing various approaches to finding the next min edge and the resulting time/space tradeoffs between them.
- Explain how to implement Kruskal's algorithm, comparing various approaches to checking for cycles and the resulting time/space tradeoffs between them.

> This chapter does not have a starter/solution code.
# Tree: A Connected Acyclic Graph



* In the context of Graph Theory, define Cycle, Acyclic Graph, and Tree.





A **cycle** is a path that starts and ends at the same vertex and includes at least one edge. 



Another name for a cycle is a "closed path." Having at least one edge means at least two vertices in the path: the start/end and one other.

Consider the following graph:





Here is a cycle: $(A, B, D, A)$. Here is another one: $(A, B, D, A, C, D, A)$



A **simple cycle** is a cycle that includes vertices other than the start/end at most once.



In this class, when I say "cycle," I mean "simple cycle."

**Aside:** In some references, what I defined as "cycle" is described as "circuit," and instead, "simple cycle" is simply called "cycle."



An **acyclic graph** is a graph having no cycles.







Recall: A graph is called **connected** if there is a path between every pair of vertices.



A connected acyclic graph is called a **tree**!




Aside: directed or undirected?

In Mathematics and Graph Theory, trees are assumed to be **undirected**. However, in the context of Data Structures, a tree is typically rooted (one vertex has been designated as "root") and directed (all edges point away from the root).

Why is a tree considered undirected in Graph Theory? Because a "connected" graph is generally defined as an undirected graph.

In directed graphs, edges connect one node to another, but not necessarily in the opposite direction (the edge relation between vertices is asymmetric). In lieu of this, a directed graph may be _weakly connected_, _unilaterally connected_, _semi-connected_, _strongly connected_, etc. — the point is that [connectivity](https://en.wikipedia.org/wiki/Connectivity_(graph_theory)#Connected_vertices_and_graphs) in directed graph is messy! 



Accordingly, the topic covered in this chapter, i.e., minimum spanning tree, is defined for undirected graphs. 



It's possible to define MST for directed graphs. Still, it is usually given other names like *optimum branching*, *min-cost arborescence*, etc.




# Spanning Tree



* Describe what spanning tree.





A **spanning tree** of a _connected undirected_ graph $G$ is a _subgraph_ of it (every edge in the tree belongs to $G$) that _spans_ $G$ (it includes every vertex of $G$).



Consider this graph:





Every tree below is a *spanning tree* of the graph above.





As can be seen above, a graph may have several spanning trees. 


A spanning tree can be built by doing a BFS/DFS of the graph.

Recall the demo for BFS/DFS from prior chapters:





The spanning trees consist (only) of the thick edges.


 


Resources

* Wikipedia's entry on [Spanning Tree](https://en.wikipedia.org/wiki/Spanning_tree).
* Computerphile's YouTube video on [Software Defined Networking](https://youtu.be/Nh2hXUuKXyQ) — an example of an application of spanning trees. 





# Minimum Spanning Tree



* Describe why constructing minimum spanning trees is useful.



When finding spanning trees of a graph, we may be interested in those where the total edge weight is minimal among all the possible spanning trees, a so-called minimum weight spanning tree (MST). 



A **minimum spanning tree** of a _weighted_ graph $G$ is a spanning tree of it with the minimum possible total edge weight.



Consider the following _weighted_ graph:





Every tree below is a *minimum spanning tree* of the graph above.





As can be seen, an MST is not necessarily unique. There may be several minimum spanning trees of the same total weight. However, if each edge has a distinct weight, there will be only one unique minimum spanning tree.



**Minimum Spanning Tree Problem**

Given a *connected*, *undirected weighted* graph $G = (V, E, w)$, the minimum (weight) spanning tree problem requires finding a spanning tree of minimum weight, where the weight of a tree $T$ is defined as the sum of the wight of all its edges:

$$
w(T) = \sum_{e \in E(T)} w(e)
$$



We will look at two algorithms to solve the MST problem:

* Prim's Algorithm
* Kruskal's Algorithm



Resources

* Wikipedia's entry on [Minimum Spanning Tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree).


# Prim's Algorithm



* Explain how to implement Prim's algorithm.



A [Greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) that grows an MST from a starting source vertex until it spans the entire graph:

* Start with an empty minimum spanning tree $T = \\{\\}$.
* Pick a vertex $v$ (at random) and add it to $T$.
* Choose a vertex $u$ not in $T$, such that the edge weight from a vertex in $T$ to $u$ is the least among all such edges.
* Add $u$ to $T$. 
* Repeat the last two steps until $(N – 1)$ edges were added.


Demo




Resources

* Wikipedia's entry on [Prim's Algorithm](https://en.wikipedia.org/wiki/Prim%27s_algorithm).
* [Interactive visualization of Prim's Algorithm](https://www-m9.ma.tum.de/graph-algorithms/mst-prim/index_en.html) by Reza Sefidgar.




# Prim's Algorithm: Exercise



* Trace Prim's algorithm for finding a minimum spanning tree.



Exercise Identify the edges on a minimum spanning tree for this graph following Prim's algorithm.






Solution



# Prim's Algorithm: Analysis



* Compare various approaches to finding the next min edge and the resulting time/space tradeoffs between them for Prim's algorithm.



Exercise Based on your understanding of Prim's algorithm, how can we efficiently implement the step which involves finding min-weight edge with one endpoint in $T$?


Solution

* Naive approach: Try all edges $\Omicron(M)$.

* Better approach: Keep all the edges that have one endpoint in $T$ in a (min-heap) Priority Queue and remove the best (min) at each iteration: $\Omicron(\lg M)$



Exercise Based on your answer to the previous question, analyze the asymptotic complexity of Prim's algorithm.


Solution

Runtime: $\Omicron(M \lg M)$ – with $\Omicron(M)$ auxiliary space.

| Operation       | Frequency | Cost per operation |
| :-------------: | :-------: | :----------------: |
| `pq.remove()`   | $\Omicron(M)$    | $\Omicron(\lg M)$         |
| `pq.insert()`   | $\Omicron(M)$    | $\Omicron(\lg M)$         |

Note: you might have to remove multiple edges until you find one with only one endpoint in $T$. That's why remove's frequency is $M$, not $N$.

Considering $M\in \Omicron(N^2)$, we have $\Omicron(M \lg M) \equiv \Omicron(M \lg N^2) \equiv \Omicron(M \lg N)$ for simple graphs.




# Kruskal's Algorithm



* Explain how to implement Kruskal's algorithm.



A Greedy algorithm that grows a forest of minimum spanning trees and eventually combines them into one MST:

* Sort all edges (in ascending order, based on weight)
* Start with an empty minimum spanning tree $T = \\{\\}$.
* Pick the smallest edge and add it to $T$.
* Add next smallest edge to $T$ unless it creates a cycle.
* Repeat the last step until $(N – 1)$ edges were added.


Demo




Resources

* Wikipedia's entry on [Kruskal's Algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm).
* [Interactive visualization of Kruskal's Algorithm](https://www-m9.ma.tum.de/graph-algorithms/mst-kruskal/index_en.html) by Reza Sefidgar.
  

# Kruskal's Algorithm: Exercise



* Trace Kruskal's algorithm for finding a minimum spanning tree.



Exercise Identify the edges on a minimum spanning tree for this graph following Kruskal's algorithm.






Solution


# Kruskal's Algorithm: Analysis



* Compare various approaches to checking for cycles and the resulting time/space tradeoffs between them for Kruskal's Algorithm.



Exercise Based on your understanding of Kruskal's algorithm, how can we efficiently implement the step which involves finding the next min-weight edge in $G$?


Solution

* Keep a sorted array of edges. Keep a pointer to the next position (edge).
* Keep edges in a (min-heap) priority queue.

With an optimal sorting algorithm (to sort edges of the input graph by increasing weight), both approaches are $\Omicron(M \lg M)$ runtime.

> We would spend $\Omicron(M \lg M)$ to sort the edges and then get the next edge in $\Omicron(1)$ time. Whereas, we can build the PriorityQueue in $\Omicron(M)$ time and remove the next "best" edge in $\Omicron(\lg M)$. We would have to do the "remove" $\Omicron(M)$ times because some edges may have to be disregarded (they cause cycle).



Exercise Once the next min-weight edge $(v, w)$ is found, how can we efficiently check if adding it to the MST would create a cycle?


Solution

We cannot check for a cycle by simply checking if the endpoints are already in $T$ (why?). We can run BFS/DFS on $T$, start at $v$ and check if $w$ is reachable.




Exercise Based on your answers to the previous questions, analyze the asymptotic complexity of Kruskal's algorithm.


Solution

| Operation   | Frequency | Cost per operation |
| :---------  | :-------: | :----------------: |
| build PQ    | $1$       | $\Omicron(M)$             |
| extract min | $\Omicron(M)$       | $\Omicron(\lg M)$         |
| run BFS/DFS | $\Omicron(M)$       | $\Omicron(N+M)$           |

From the table, it can be seen that Kruskal's algorithm is _quadratic_. However, we can improve the performance by using another data structure called **Union-Find** for efficiently checking/preventing cycles. We will explore Union-Find in the next chapter!


# Union-Find Data Structure

After reading this chapter and engaging in the embedded activities and reflections, you should be able to:

- Identify the operations of the **Union-Find data structure**.
- Trace the **quick find** implementation strategy for union-find.
- Trace the **quick union** implementation strategy for union-find.
- Differentiate the advantages/disadvantages of quick find vs. quick union.
- Trace the "quick union" implementation with **union-by-size** and **path compression** heuristics.
- Explain the runtime improvements gained by using the heuristics for union-find operations.
- Define the **iterated logarithm** (log-star) function.
- Identify the *amortized* runtime of union-find operations.
- Explain how to implement Kruskal's algorithm efficiently using a union-find structure to detect cycles. Identify the resulting time complexity.

> This chapter does not have a starter/solution code.
# Dynamic Connectivity



* Identify the operations of the Dynamic Connectivity structure.



Kruskal's algorithm needs a data structure that dynamically (and efficiently) maintains information about the connected components of a graph. 


Why does Kruskal's algorithm need such a data structure?

Every edge selected by Kruskal's algorithm must be checked to ensure adding it to the MST would not create a cycle. If the two endpoints of the edge are already "connected" (i.e. there is a path between them), adding the edge will create a cycle.



Such a data structure is called **Dynamic Connectivity** structure. According to [Wikipedia](https://en.wikipedia.org/wiki/Dynamic_connectivity):



In a dynamic connectivity structure, the set $V$ of vertices of the graph is fixed, but the set $E$ of edges can change:

* Edges may only be added to the graph (incremental connectivity);
* Edges may only be deleted from the graph (decremental connectivity);
* Edges can be either added or deleted (fully dynamic connectivity).

After each addition/deletion of an edge, the dynamic connectivity structure should adapt itself to answer questions such as "is there a path between $x$ and $y$? or equivalently: "do vertices $x$ and $y$ belong to the same connected component?".



For example, consider the following graph:





We may ask if there is a path between vertices $A$ and $G$? Or if the vertices $H$ and $J$ belong to the same connected component?

```js
connected(A,G)  // false
connected(H,J)  // true
```


Notice "is connected to" is an equivalence relation

* Reflexive: $p$ is connected to $p$.
* Symmetric: if $p$ is connected to $q$, then $q$ is connected to $p$. 
* Transitive: if $p$ is connected to $q$ and $q$ is connected to $r$, then $p$ is connected to $r$.



If edges can only be added, then the dynamic connectivity problem can be solved by a **Union-Find** data structure. 


Resources

* Wikipedia Entry on [Dynamic Connectivity](https://en.wikipedia.org/wiki/Dynamic_connectivity).


# Union-Find Data Structure



* Identify the operations of the Union-Find data structure.



A **Union-Find** data structure (a.k.a. *Disjoint-set* or *Merge-find set* data structure) categorizes objects into _disjoint_ (non-overlapping) sets. It facilitates checking if two objects belong to the same set. 



It provides near-constant-time operations to add new sets, merge existing sets, and determine whether elements are in the same set.



The most popular application of this data structure is to check whether one node in a graph can be reached from another, e.g., in  Kruskal's algorithm, to avoid forming cycles.


Resources

* Wikipedia Entry on [Disjoint-set data structure](https://en.wikipedia.org/wiki/Disjoint-set_data_structure).


# Core Operations



* Identify the operations of the Union-Find data structure.



The core operations of Union-Find data structure includes:

```js
makeSet(x)   // create and return singleton set with x as element.
find(x)      // return some representation of the set to which x belongs.
union(x,y)   // merge the sets containing x and y.
```

The Union-Find typically includes the following operations too:

```js
connected(x,y)  // return true if x & y are in the same set.
count()         // return number of (disjoint) sets.
```

There are two general approaches to implement Union-Find data structure:

* Quick Find
* Quick Union


# Quick Find



- Trace the Quick Find implementation strategy for Union-Find.
- Identify the runtime of Union-Find operations under Quick Find implementation.



The main idea behind this approach is to assign an ID to each vertex (object) to record its "membership"; $p$ and $q$ are connected if and only if they have the same ID.

* `connected(p,q)`: check if $p$ and $q$ have the same ID.
* `union(p,q)`: to merge components containing $p$ and $q$, change all entries whose ID equals `ID[q]` to `ID[p]`.



It is common to store vertices (or their references) in an array and use array indices to refer to each vertex.




Demo



Exercise What is the complexity of core operations under "Quick Find" implementation?


Solution

* `find`/`connected` involves checking `ID[p]==ID[q]` so it is $\Omicron(1)$.
* `union` is expensive, in the worst-case, it is $\Omicron(N)$ where $N$ is the number of vertices (objects).

If we start with a $N$ singleton set of objects, to build the MST, it takes at least $(N-1)$ union commands, leading to $\Omicron(N^2)$ runtime.


# Quick Union



- Trace the Quick Union implementation strategy for Union-Find.
- Identify the runtime of Union-Find operations under Quick Union implementation.



This approach is similar to Quick Find in that each vertex (object) is given an ID. However, the ID links one node to another to form a "parent/child" relationship as in a tree structure.

* `connected(p,q)`: check if $p$ and $q$ have the same _root_ (i.e., following their _parents_ we reach the same root object).
* `union(p,q)`: to merge components containing $p$ and $q$, set the root of the component containing $q$ to be a direct child of the root of the component containing $p$.


Demo



Exercise What is the complexity of core operations under "Quick Union" implementation?


Solution

* Both `connected` and `union` need to find the root of the components containing their arguments.

```java
// chase parent pointers until reach root
private int root(int x) { 
  while (x != id[x]) {
    x = id[x]; 
  }
  return x; 
} 
```

The runtime of `root` corresponds to the height of the (logical) tree structure containing the $x$ object. In the worst-case, it will be $\Omicron(N)$.



If we keep the tree flat, we can expect better performance in practice.




# Improvement 1: Weighting



- Trace the Quick Union implementation with a union-by-size heuristic.
- Explain the runtime improvements gained by using the heuristic for union-find operations.





Modify quick-union to avoid tall trees:
* Keep track of the size of each component.
* Balance by linking small tree below large one.









> **Proposition:** In this scheme, the depth of any node $x$ is at most $\lg N$.

Exercise To justify the proposition, complete the following statements by filling in the blanks.

* The depth of $x$ increases at most by `______` when tree $T_1$ containing $x$ merges into another tree $T_2$.
* Since the larger tree (among $T_1$ and $T_2$) is at least `___________` the smaller tree, the resultant tree (after union) must have at least `_________` the number of elements in the smaller one.
* The size of tree containing $x$ can `_______` at most `________` times because if you start with one node and `_______________` you will get $N$, and there is only $N$ nodes in the tree.


Solution

* The depth of $x$ increases at most by one when tree $T_1$ containing $x$ merges into another tree $T_2$. 
  * (Assume the worst case where each tree has $m$ elements and a height of $m$. Then the resulting tree after union will have $2m$ elements and heigh of $m+1$.)
* Since the larger tree (among $T_1$ and $T_2$) is at least as large as the smaller tree, the resultant tree (after union) must have at least double the number of elements in the smaller one.
* The size of tree containing $x$ can double at most $\lg N$ times because if you start with one node and double it $\lg N$ times you will get $N$, and there is only $N$ nodes in the tree.



From the proposition, it follows the height of the tree is in $\Omicron(\lg N)$.



The heuristic described above is known as **union by size**.



**Aside:** An alternative strategy is _union by rank_, which always attaches the tree with a smaller "rank" to the root of the tree having a higher rank. The rank of a node is the height of its subtree; the rank of a node can only change if it is a root.



# Improvement 2: Path Compression



- Trace the Quick Union implementation with path-compression heuristic.
- Explain the runtime improvements gained by using the heuristic for union-find operations.





After computing the root of $p$, set the ID of each examined node to point to that root.



For example, consider the following tree:





Assume we perform `find(J)` operation. On our way to find the root, we would pass through $I$, $F$, $B$ until we get to $A$, the root.

We could set all these vertices to **directly** point to the root, so the tree becomes _shallower_:



 



The heuristic described above is known as **path compression**.


# Quick Union: Exercise



- Explain the runtime improvements gained by using the heuristics for union-find operations.
- Define the iterated logarithm (log-star) function.
- Identify the amortized runtime of union-find operations.



Suppose you have singleton sets with the values $0$ through $6$. Then, we apply the following operations.

```js
union(0,5) 
union(1,4) 
union(2,3) 
union(3,6) 
union(4,6) 
union(0,4) 
```

Exercise Using both tree and array forms, show the result of each of the operations listed above, applying **union-by-size** and **path compression** heuristics. 


Solution

Here is the start:





After `union(0,5)`:





After `union(1,4)`:





After `union(2,3)`:





After `union(3,6)`: notice the size of the component containing $6$ is smaller than the size of the component containing $3$. Therefore, the component containing $6$ is added to the root of the component containing $3$.





After `union(4,6)`: notice the size of the component containing $4$ is smaller than the size of the component containing $6$. Therefore, the component containing $4$ is added to the root of the component containing $6$.





After `union(0,4)`: notice as we find the root of the component containing $4$, we apply path compression.





Then, as the size of the component containing $0$ is smaller than the size of the component containing $4$, the component containing $0$ is added to the root of the component containing $4$.






# Runtime after improvements!



* Identify the *amortized* runtime of union-find operations.



If implemented with Union-by-size and Path Compression:

* practically keeps the tree almost flat
* makes the operations work in $\Omicron(\lg^* N)$



$\lg^* N$ (read: [Iterated logarithm](https://en.wikipedia.org/wiki/Iterated_logarithm) of $N$) is the number of times one needs to apply $\lg$ to $N$ to get a value less than or equal to $1$. 



In practice, one could think of it to be almost $\Omicron(1)$ since it exceeds $5$ only after it has reached $2^{65536}$.

**Aside:** The proof of the above running time is beyond the scope of this course. The Union-Find data structure was invented in 1964. The running time above was proved in 1973 (by Hopcroft and Ullman — see their [paper](https://epubs.siam.org/doi/abs/10.1137/0202024)). 

# Kruskal's Runtime



* Explain how to implement Kruskal's algorithm efficiently using a union-find structure to detect cycles. Identify the resulting time complexity.



How to get the next min-weight edge?

> Keep edged in a (min-heap) priority queue.

How to check if adding edge (v-w) creates a cycle?

> Use Union-Find to help in checking/preventing cycle

Exercise Complete the following table:

| Operation   | Frequency | Cost per operation |
| :---------- | :-------: | :----------------: |
| build PQ    |           |                    |
| extract min |           |                    |
| union       |           |                    |
| connected   |           |                    |



Solution

| Operation   | Frequency | Cost per operation |
| :---------- | :-------: | :----------------: |
| build PQ    | $1$       | $\Omicron(M)$             |
| extract min | $\Omicron(M)$       | $\Omicron(\lg M)$         |
| union       | $\Omicron(N)$       | $\Omicron(\lg^* N)$       |
| connected   | $\Omicron(M)$       | $\Omicron(\lg^* N)$       |


K IRON   D EB  ( 443 ) - 742 - 5370   |   Personal Site   |   LinkedIn   |   kirondeb02@gmail.com  E DUCATION  Johns Hopkins University   Expected   Dec   202 4  B.S.   Computer Scienc e ; GPA: 3.9 5 /4.00  Skills  Languages : JavaScript/TypeScript, Java, Python, SQL,   C/C++   HTML/CSS , Bash  Libraries :   Vue, TypeORM, Jest,   React, Redux, Express, Node.js, Cypress , Spring Boot , Cron  Tools :   Git, Docker, Kubernetes, CircleCI,   Amazon   CloudWatch,   Amazon   API Gateway , Gradle  Experience  Machine Learning Engineer   September 2023   –   Present (2 months, Part - time)  Johns Hopkins University   Baltimore, MD  ●   Working with professor   Ali Madooei   to build a   course - specific   chatbot where student prompts are   augmented with  lecture notes using retrieval - augmented generation (RAG)  ●   Converted lecture notes into OpenAI embeddings   and stored them in a Supabase vector database for retrieval  ●   Creating GitHub Actions to automatically regenerate embeddings when lecture notes are updated  Skills:   Langchain, Supabase, OpenAI, TypeScript, GitHub Actions  DevOps Developer Intern   May   2023   –   Aug   2023   (3 months , Full - time )  IBM   New York City ,   NY  ●   Saved 70 hours of manual work per year by a utomat ing   the fetching   of   warranty   information   from Apple for  160,000 +   MacBooks managed by IBM  ●   Built   a   dockerized   Java   API   which hit s   two third - party APIs to fetch data from Apple and   populate   it in Jamf  ●   Wrote shell scripts and   a Cron Job to start the workflow daily, ensuring new MacBooks   have warranty info populated  ●   Live - streamed the app’s logs to a file and set up log rotation to avoid memory overflow  Skills: Java,   Spring Boot,   Docker,   Docker Compose,   J u nit , Gradle, Bash , Cron ,   logrotate, awk  Software Engineer ing   Co - Op   Jan 2023   –   May 2023   (4 months , Full - time )  Vontive   San Francisco, CA  ●   Integrated an API to replace outsourced loan servicing with in - house servicing, accelerating a Q4 initiative by 5  months; wrote end - to - end and isolated tests to test the API and internal controllers independently  ●   Owned 4 Jira epics to support the underwriting of new categories of loans (bank statement, vacation rental)  ●   Automated Slack messages to ping relevant internal employees with select information from the database, reducing  human   time   needed   from   100   seconds   to 1 second  ●   Aggregated fields using SQL aggregate functions and JOINs to summarize data from multiple rows  ●   Automated the manual process of locking tables and adding log statements in autogenerated TypeORM migrations  ●   #1 code contributor on the backend   and   #2 contributor on the frontend   over my Co - Op in a team of 11 engineers  Skills: TypeScript, Vue, TypeORM, PostgreSQL, Jest, Vuex, AWS CloudWatch, AWS API Gateway, CircleCI , Bash  Undergraduate Researcher   Jun 2021   –   Nov 2022 (17 months , Part - time )  Johns   Hopkins University   Baltimore, MD  ●   Authored a   paper   under professor Kevin Duh which got accepted to conference   Black b oxNLP  ●   Interpreted how Transformer hyperparameters affect model accuracy using the Explainable Boosting Machine  Skills: Python,   Google Colab,   InterpretML,   scikit - learn , Matplotlib, LaTeX  Teaching Assistant ,   Data Structures   Aug 2022   –   Present   ( 12   months , Part - time )  Johns Hopkins University   Baltimore, MD  ●   Hosted   weekly office hours to help students   debug   homework assignments and answer   data structures questions  ●   Graded coding projects and exams by checking for correctness and code quality  Skills: Java, Teaching , ChatGPT  Projects  Recruiter Emailing Scripts : wrote scripts to get the first and last name of recruiters using Google’s Custom Search AP and send  personalized emails to 500+ recruiters at various companies by   based on the   companies’   email   format s   (e.g.   fLast@company.com )  Twitter Bio Generator : used   OpenAI’s chat completion API to generate   Twitter bios based on user - provided data and a style  (professional, sassy). Used Supabase for magic link authentication
